[ { "title": "Learning Go: An Idiomatic Approach to Real-World Go Programming", "url": "/posts/learning-go-book/", "categories": "books, golang", "tags": "book, go, golang", "date": "2023-04-25 14:50:00 -0400", "snippet": "Written by Jon Bodner, Learning Go: An Idiomatic Approach to Real-World Go Programmingis, in my opinion, one of the best resources there is to learn Go, particularly if you are somewherearound the intermediate stage with a couple of years under your belt. From the basic topics covered at the start allthe way to cgo, reflection, and unsafe, this book covers it all, complete with tips on writing idiomatic code.I can guarantee this book will leave you with strongly reinforced learnings and new knowledge.The book came out right before Go 1.18 was released (mayor feature being generics),but everything contained in this book still holds beautifully.I highly recommend this book for those that value Go as a professional tool and are serious about mastering it.Check out some other books I’ve read on the bookshelf.SummaryLearning Go is a complete treasure trove of learnings about everything related to Go: its syntax, standard tools,standard APIs, common idioms, common sources of bugs, great insight into the reasons for the design of some ofGo’s APIs and operators, and ending with advanced topics such as cgo and unsafe. No prior knowledge of Go is requiredfrom the reader; given prior exposure to other similar programming languages, this book will effortlessly take thereader from zero to hero.Some of the things I likedLearning Go exposes the early adopter to common sensible idioms used throughout the ecosystem.The comma-OK idiom(page 54)Simple way to differentiate between a type’s zero value and its absence, usuallyas return values from some sort of API (typically a map).v, ok := myMap[&quot;key&quot;]if !ok { // handle key not found}// handle valueNote that if your API may return an error for other reasons, then it’s better touse a sentinel error:var ErrNotFound = errors.New(&quot;my sentinel error&quot;)v, err := myAPI.Get(&quot;key&quot;)if errors.Is(ErrNotFound) { // handle key not found}if err != nil { // handle other error}// handle valueLastly, the comma-OK idiom is implemented by channels (to differentiatebetween the zero-value and a closed channel) and type assertions (toknow whether the assertion is true).Left-aligned, short if statement bodies(page 70)Avoid deeply nested structures:// BADif i%3 == 0 { if i%5 == 0 { return &quot;FizzBuzz&quot; } else { return &quot;Fizz&quot; }} else if i%5 == 0 { return &quot;Buzz&quot;} else { return fmt.Sprint(i)}// GOODif i%3 ==0 &amp;amp;&amp;amp; i%5 == 0 { return &quot;FizzBuzz&quot;}if i%3 == 0 { return &quot;Fizz&quot;}if i%5 == 0 { return &quot;Buzz&quot;}return fmt.Sprint(i)In addition, I endorse “The Happy path is left-aligned”even when using the comma-OK idiom:// not greatif v, ok := myMap[&quot;key&quot;]; ok { // handle value} else { // handle key not found}// betterv, ok := myMap[&quot;key&quot;]if !ok { // handle key not found}// handle valueThe improvement in readability due to increased line-of-sight (see linked article) is worth the slight increase innumber of lines in my opinion.Types are executable documentation(page 136)User-defined types add clarity by exposing the concept represented by a given value. Imaginefunctional options without a custom type:func DoSomething(ctx context.Context, key string, opts ...func(*Config)) error { // do something}This API’s clarity can be enhanced as follows:type Option func(*Config)func DoSomething(ctx context.Context, key string, opts ...Option) error { // do something}But user-defined types can do much more than this. Consider http.HandlerFunc:type HandlerFunc func(ResponseWriter, *Request)func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r)}In this case, the type serves as an adapter for user-provided functions that meet the signature requirements. Thisfrees the user from having to define a whole struct type just to implement the one ServeHTTP method:func main() { mux := http.NewServeMux() mux.Handle(&quot;/foo&quot;, http.HandlerFunc(handleFoo)) // just cast the function to http.HandlerFunc err := http.ListenAndServe(&quot;:8080&quot;, mux) if err != nil { log.Fatal(err) }}func handleFoo(w http.ResponseWriter, r *http.Request) { // handle foo}One other advantage of user-defined types is that they have the potential to stop being simple data structures or primitivesand start having useful behaviour. Consider the following example:func main() { percentage := 0.2 subtotal := 29.5 ApplyDiscount(subtotal, percentage) fmt.Printf(&quot;applied %.0f%%\\n&quot;, percentage*100)}func ApplyDiscount(subtotal, percentage float64) { // do something}Let’s make “percentage” more useful as a first-class concept:type Percentage intfunc (p Percentage) String() string { return fmt.Sprintf(&quot;%d%%&quot;, p)}func (p Percentage) Float() float64 { return float64(p / 100)}func main() { var percentage Percentage = 20 subtotal := 29.5 ApplyDiscount(subtotal, percentage) fmt.Printf(&quot;applied %s\\n&quot;, percentage)}func ApplyDiscount(subtotal float64, p Percentage) { // do something}Another example - an in-memory datastore useful for tests:type Store[K comparable, V any] interface { Get(k K) (V, error) Put(k K, v V) error}type mockStore[K comparable, V any] map[K]Vfunc (m mockStore[K, V]) Get(k K) (V, error) { return m[k], nil}func (m mockStore[K, V]) Put(k K, v V) error { m[k] = v return nil}sync.Map Is Not The Map You Are Looking For(page 240)I generally find Go’s community and (in some cases) its documentation rather dogmatic and prescriptive when comparedto others. The practical effect of this in the real world is it tends to lead the novice/mid-level engineer to theincorrect conclusion that a certain API or pattern is the best fit for their use case. sync.Mapis one of those cases where the documentation generally leads engineers to suboptimal solutions in terms of performance1: Map is like a Go map[interface{}]interface{} but is safe for concurrent use by multiple goroutines without additionallocking or coordination. Loads, stores, and deletes run in amortized constant time. The Map type is specialized. Most code should use a plain Go map instead, with separate locking or coordination, forbetter type safety and to make it easier to maintain other invariants along with the map content. The Map type is optimized for two common use cases: (1) when the entry for a given key is only ever written once butread many times, as in caches that only grow, or (2) when multiple goroutines read, write, and overwrite entries fordisjoint sets of keys. In these two cases, use of a Map may significantly reduce lock contention compared to a Gomap paired with a separate Mutex or RWMutex.The first paragraph informs us that this map is safe for concurrent reads and writes. This map has had type parametersever since generics were introduced in Go 1.18, so this line is outdated.The second paragraph recommends use of the plain map with more common synchronization primitives (locks, channels).One of the reasons for this recommendation - better type safety - is now outdated. It’s a rather short paragraph.The third paragraph is longer and is the one most likely to mislead the novice/mid-level engineer: as an authoritativesource, it gives the impression that if your use case matches the two enumerated there then you should use sync.Mapwithout further consideration. The second paragraph’s recommendation is usually brushed away after reading this one.A good engineer should have further considerations before deciding on whether to use sync.Map: Are stampedes a concern? Are external services impacted when populating the cache? How large is the cache expected to grow? How frequently are cache entries added? Are cache entries ever updated after being added? How expensive is it to create entries for the cache and how does it stack against the40ns it takes to transfer L2 caches between CPUs as per the original author ofsync.Map? How many cores do your nodes have?And I’m sure there are more.In my experience, caches are usually held in memory somewhere and implemented because the cached data is “expensive”to create. Given this, scenario (1) is always served more optimally with judicious use of sync.RWMutex and a plain mapto protect against stampedes (a frequent concern), or with a plain map and atomic.Pointer to implement aread-copy-update scheme if the cache is updated infrequently.I have yet to come across scenario (2), but some of those questions would still apply.In conclusion, I think sync.Map is overused and I also think Go’s API documentation should limit its prescriptivelanguage and just state the facts of how its APIs operate.Avoid APIs that depend on exposed package-level state(page 251)“Avoid APIs that depend on exposed package-level state” is my key takeaway from the book: There are package-level functions, http.Handle, http.HandleFunc, […] Don’t use them outisde of trivial testprograms. […] Furthermore, third-party libraries could have registered their own handlers with the http.DefaultServeMuxand there’s no way to know without scanning through all of your dependencies (both direct and indirect). Keep yourapplication under control by avoiding shared state.The following example illustrates the point.Your code:import ( &quot;net/http&quot; &quot;scratchpad/global/helper&quot;)func main() { http.Handle(&quot;/myAPI&quot;, http.HandlerFunc(myHandler))}func myHandler(w http.ResponseWriter, r *http.Request) { // read request // do awesome stuff result := &quot;success &quot; + helper.DoSomethingHelpful() _, _ = w.Write([]byte(result))}What your “awesome” helper is doing:package helperimport &quot;net/http&quot;func init() { http.DefaultServeMux.Handle(&quot;/secrets&quot;, http.HandlerFunc(exposeSecrets))}func DoSomethingHelpful() string { return &quot;great work done here&quot;}func exposeSecrets(w http.ResponseWriter, r *http.Request) { // expose all your secrets from env vars, local filesystem, etc.}Some of the things I learnedA selection of some of the most interesting or surprising things I learned about Go.Complex numbers(page 24)Go supports complex numbers. You can perform arithmetic operations on them,and they are comparable (eg. can be used as keys in a map). The following example prints (4+6i) and a:func main() { a := complex(1, 2) b := complex(3, 4) c := a + b fmt.Println(c) m := map[complex128]string{ a: &quot;a&quot;, b: &quot;b&quot;, } fmt.Println(m[a])}Struct conversion(page 59)Anonymous structs are interchangeable if their fields align perfectly and are comparable:func main() { alice := struct { Name string Age int }{ Name: &quot;Alice&quot;, Age: 30, } garfield := struct { Name string Age int }{ Name: &quot;Garfield&quot;, Age: 2, } garfield = alice fmt.Printf(&quot;%+v\\n&quot;, garfield)}// Output:// {Name:Alice Age:30}This is particularly useful when populating third-party structs with fields that are anonymous structs.Instead of having to do this:type Config struct { Simple1 string Simple2 string Composite1 struct { Name string Date time.Time Value int } Composite2 struct { Name string Date time.Time Value int }}func main() { conf := Config{ Simple1: &quot;one&quot;, Simple2: &quot;two&quot;, Composite1: struct { Name string Date time.Time Value int }{ Name: &quot;alice&quot;, Date: time.Now(), Value: 1, }, Composite2: struct { Name string Date time.Time Value int }{ Name: &quot;bob&quot;, Date: time.Now(), Value: 4, }, } fmt.Printf(&quot;%+v\\n&quot;, conf)}You can save a few lines by declaring an anonymous struct that matches the structure of the composite fields:type Config struct { Simple1 string Simple2 string Composite1 struct { Name string Date time.Time Value int } Composite2 struct { Name string Date time.Time Value int }}func main() { type composite struct { Name string Date time.Time Value int } conf := Config{ Simple1: &quot;one&quot;, Simple2: &quot;two&quot;, Composite1: composite{ Name: &quot;alice&quot;, Date: time.Now(), Value: 1, }, Composite2: composite{ Name: &quot;bob&quot;, Date: time.Now(), Value: 4, }, } fmt.Printf(&quot;%+v\\n&quot;, conf)}The Universe Block(page 65)I was surprised to learn that what I thought were special keywords that could not be used anywhere in code are actuallypredeclared identifiers in the universe block: the block in whichall code is in scope. Because they are mere (predeclared) identifiers and not keywords, they can be shadowed just likeany other identifier:func main() { nil := 1 fmt.Println(nil) append := &quot;append&quot; fmt.Println(append)}// Output:// 1// appendPointer Receivers vs Value Receivers(page 132) Go considers both pointer and value receiver methods to be in the method set for a pointer. For a value instance, only the value receiver methods are in the method set.The practical effect of this is that one cannot assign a value type to a variable of an interface type if the formerdoes not have methods with value-type receivers that implement the interface:type Greeter interface { Greet()}type Runner interface { Run()}type Athlete struct{}func (a *Athlete) Greet() { fmt.Println(&quot;Hello there!&quot;)}func (a Athlete) Run() { fmt.Println(&quot;I am running!&quot;)}func main() { var a Greeter = &amp;amp;Athlete{} var b Greeter = Athlete{} // compile error! var c Runner = &amp;amp;Athlete{} var d Runner = Athlete{}}I had never given much though to this distinction and now got curious - why? The Golang FAQ has ananswer: This distinction arises because if an interface value contains a pointer *T, a method call can obtain a value bydereferencing the pointer, but if an interface value contains a value T, there is no safe way for a method call toobtain a pointer. (Doing so would allow a method to modify the contents of the value inside the interface, which isnot permitted by the language specification.) Even in cases where the compiler could take the address of a value to pass to the method, if the method modifies thevalue the changes will be lost in the caller. As an example, if the Write method of bytes.Buffer used a value receiverrather than a pointer, this code: var buf bytes.Bufferio.Copy(buf, os.Stdin) would copy standard input into a copy of buf, not into buf itself. This is almost never the desired behavior.The second reason is fairly easy to understand: if bytes.Buffer had a value receiverfor its Write method, then io.Copy would writethe contents of os.Stdin to a copy of buf, not the caller’s copy.The first reason is a bit more esoteric: if value types were allowed to implement interfaces then any method call thatmodifies the value itself would also modify the interface object itself. Or should they modify a copy made on the fly?What would be the ramifications of that? Does the caller’s reference to the interface object magically update to the new value?Or should the changes be effected on a different copy than the caller’s? Rather than opening up this can of worms, the Goteam decided to simplify the mental model by this rule prohibiting this edge case.Invoking a function with args of type interface will result in a heap allocation(page 147)This was not really a new concept to me, but I still had my “duh!” moment as I read this because I never thought toconsider this consequence of “Accept Interfaces, Return Structs” (see below): […] reducing heap allocations improves performance by reducing the amount of work for the garbage collector.Returning a struct avoids a heap allocation, which is good. However, when invoking a function with parameters ofinterface types, a heap allocation occurs for each of the interface parameters.The allocations occur not at the function’s invocation, but before it, wherever the implementations of the interfacetypes were instantiated2.Of course, the usual caveat applies: write readable, maintainable code first, then optimize if needed.Alias declarations(page 189)I was aware of aliases but never bothered to look closely at them since I haven’t had the need to declare them myself,although I have used some from the standard library plenty of times. Some of these may surprise you: byte, rune, and any are aliases (see code) os.PathError,os.FileInfo, os.FileMode,and os.DirEntry are also aliasesThe difference between alias declarations and type definitionsis the former does not create a new type; it merely creates a new name that can also be used to refer to the type definition.The following example defines type Person and an alias to it, Alice. Outwardly the code seems to define a new methodon Alice, but really the method is attached to Person and also invokable from a reference to Alice:type Person struct { name string}func (p *Person) Name() string { return p.name}type Alice = Personfunc (a *Alice) Greet() string { return fmt.Sprintf(&quot;Hello, my name is %s!&quot;, a.name)}func main() { a := &amp;amp;Alice{name: &quot;Alice&quot;} fmt.Println(a.Name()) p := &amp;amp;Person{name: &quot;Alice&quot;} fmt.Println(p.Name()) fmt.Println(p.Greet())}Before you get any ideas though - there is no way to get around the hard restriction on modifying the structure of typesin different packages:import &quot;os&quot;type ErrSneaky = os.PathErrorfunc (e ErrSneaky) DoEvil() { // error: Cannot define new methods on the non-local type &#39;fs.PathError&#39; // do something evil}Writing to channels in select statements(page 211)Not 100% surprising but then again - I have never come across a use case for this:Cases in a select statement can include “send statements”(ie. writing to a channel) as well as receiving operations (ie. reading from a channel); a single instance of selectcan have both.Monotonic time(page 240)When available, Go internally uses monotonic clocks to calculatedurations between two points in Time.This is a fascinated topic that really deserves an article of its own, so I won’t discuss it here. Dropping a coupleof links for those interested: GoDoc Long discussion on GitHub How and why the leap second affected Cloudflare DNSJSON Decoder(page 245)I have been using json.Decoder in my APIs since forever because of the way it collapses twoactions into one.Compare this:func myHandler(w http.ResponseWriter, r *http.Request) { payload, err := io.ReadAll(r.Body) if err != nil { // handle error } request := &amp;amp;MyRequest{} err = json.Unmarshal(payload, request) if err != nil { // handle error } // process request}to this:func myHandler(w http.ResponseWriter, r *http.Request) { request := &amp;amp;MyRequest{} err := json.NewDecoder(r.Body).Decode(request) if err != nil { // handle error } // process request}The benefits and features of json.Decoder do not stop there though:It can decode multiple valuestype Person struct { Name string `json:&quot;name&quot;` Age int `json:&quot;age&quot;`}type USD struct { Dollars int `json:&quot;dollars&quot;` Cents int `json:&quot;cents&quot;`}func main() { r := strings.NewReader(` {&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 32} {&quot;dollars&quot;: 10, &quot;cents&quot;: 2} `) decoder := json.NewDecoder(r) person := Person{} usd := USD{} err := decoder.Decode(&amp;amp;person) if err != nil { panic(err) } err = decoder.Decode(&amp;amp;usd) if err != nil { panic(err) } fmt.Printf(&quot;%v\\n&quot;, person) fmt.Printf(&quot;%v\\n&quot;, usd)}Note that this feature is typically used to decode sequences of objects of the same type in a loop usingDecoder.More() as exit condition.It can be more performantFollowup from above, json.Decoder onlyreads the next object or array from the streamand no more3.Empty struct uses no memory(page 263)It’s the reason why the “use-map-as-set” idiom is written like this:set := map[string]struct{}instead of:set := map[string]*struct{}One would think that adding an entry to the second case (set[&quot;a&quot;] = nil) would result in less memory usage than the firstcase (set[&quot;a&quot;] = struct{}). In fact, the opposite is true:func main() { var o struct{} var p *struct{} fmt.Println(unsafe.Sizeof(o)) // prints 0 fmt.Println(unsafe.Sizeof(p)) // prints 8}The reason that a pointer uses 8 bytes of memory (on a 64-bit system) is because that is the space required to storethe pointer value itself (not the thing to which it points). Aside from the extra heap allocation used up by thething the pointer points to, this is another reason whyinvoking a function with args of type interfacemay result in more memory use.The reason that an empty struct uses no memory is because it holds no data and, it being a value-type, it can be storedon the stack without taking up any space (again, because it does not hold any data).I point you to Dave Cheney’s excellent article on the topic, The empty struct,where he goes the extra mile and covers the implications across several scenarios.Make functions and structs with reflection(pages 312-313)I have hardly ever peeked at the reflect package because reflection isa bad idea.That being said - I had no idea Go’s reflect package was capable of instantiating types (I thought it was only goodfor introspection): reflect.MakeChan() reflect.MakeFunc() reflect.MakeMap() reflect.MakeSlice() reflect.New()Huh, would you look at that?Anyway - it’s a fun curiosity, but I’m 99.9% sure I’ll never use any of this4.Use unsafe.Pointer to convert external binary data(page 316)This is one slick trick.Say we want to read the following bytes off the network: [0 132 95 237 80 104 111 110 101 0 0 0 0 0 1 0].Say these bytes correspond to a message with the following structure (in order): Value: 4 bytes, an unsigned, big-endian 32-bit int Label: 10 bytes, ASCII Active: 1 byte, boolean flag Padding: 1 byte, because we want everything to fit into 16 bytesThe straightforward implementation of the parser would slice the incoming bytes and assign those to their respective fields:type Message struct { Value uint32 Label [10]byte Active bool}func main() { data := [16]byte{0, 132, 95, 237, 80, 104, 111, 110, 101, 0, 0, 0, 0, 0, 1, 0} msg := Message{ Value: binary.BigEndian.Uint32(data[:4]), // allocating memory for a pointer to the new slice (same underlying array tho) Label: *(*[10]byte)(data[4:16]), // ditto. Worst if you use copy(). Active: data[14] == 1, } fmt.Printf(&quot;Value: %d\\n&quot;, msg.Value) // 8675309 fmt.Printf(&quot;Label: %s\\n&quot;, msg.Label) // Phone fmt.Printf(&quot;Active: %t\\n&quot;, msg.Active) // true}Those allocations may add up.What you can do instead is the following:var isLE boolfunc init() { var x uint16 = 0xFF00 xb := *(*[2]byte)(unsafe.Pointer(&amp;amp;x)) // on a little-endian platform, the bytes will be stored as [00 FF] // on a big-endian platform, the bytes will be stored as [FF 00] isLE = xb[0] == 0x00}type Message struct { Value uint32 Label [10]byte Active bool}func main() { data := [16]byte{0, 132, 95, 237, 80, 104, 111, 110, 101, 0, 0, 0, 0, 0, 1, 0} msg := *(*Message)(unsafe.Pointer(&amp;amp;data)) if isLE { msg.Value = bits.ReverseBytes32(msg.Value) } fmt.Printf(&quot;Value: %d\\n&quot;, msg.Value) // 8675309 fmt.Printf(&quot;Label: %s\\n&quot;, msg.Label) // Phone fmt.Printf(&quot;Active: %t\\n&quot;, msg.Active) // true}Jon claims the “unsafe” version is twice as performant(benchmarks here), which I don’t doubt.There are a few things to unpack here:EndiannessWe use a variable type with a width of 2 bytes (uint16) to store complementary values in each of the bytes,FF and 00, in that order. If the order changes then the platform this code is running on is little-endian.unsafe.Pointerunsafe.Pointer can be converted to a pointer value of any other type. This is notpossible with other types or pointer values. BEWARE unsafe.Pointer is “unsafe” for a reason. Imagine the following scenario: func main() { data := [0]byte{} // you received a payload of an unexpected size msg := *(*Message)(unsafe.Pointer(&amp;amp;data)) if isLE { msg.Value = bits.ReverseBytes32(msg.Value) } // The following prints whatever garbage happens to be at the memory addresses // where Value, Label, and Active should be. // This is now a potential vulnerability in the implementation. fmt.Printf(&quot;Value: %d\\n&quot;, msg.Value) // 1884229632 fmt.Printf(&quot;Label: %s\\n&quot;, msg.Label) // ��U@ fmt.Printf(&quot;Active: %t\\n&quot;, msg.Active) // false} Memory layoutA continuous area of memory is allocated (whether on the heap or on the stack) to hold a struct’s datacitation needed.The fields may or may not themselves reside contiguously due to their certainalignment guarantees that need to be met. Each field naturallyhas a size (aka. “width”) that corresponds to its type.Bringing it all togetherBecause our Message struct above was designed to occupy exactly 16 bytes of memory (including alignment), an arrayof 16 bytes can be reinterpreted as an instance of Message. This reinterpretation is possible by casting the arrayto an unsafe.Pointer, and then casting the unsafe.Pointer to Message. This last step is only possible with unsafe.Pointer.Go’s memory layout is a great topic for a future article - stay tuned.Things I am on the fence aboutAccept Interfaces, Return Structs(page 146)“Accept Interfaces, Return Structs” is astructural pattern first popularized by Jack Lindamood way back in 2016, so it’s been around awhile.I think this pattern is at its strongest when working in big, fast-paced teams with engineers who may yet have not developedtheir design skills to its full potential. However, it’s just altogether easier to keep diluting a type’s “purpose” by tackingon more and more methods to its API. Not many people are capable of thinking in higher-level, more abstract terms like“Input”, “Output”, “Scalar”, “Func”, etc..So when you stop to think about it, “accept interfaces, return structs” is at its strongest when another universal best practiceis thrown by the wayside: “the bigger the interface, the weaker the abstraction”.And from where I am standing, a “best practice” that only stands strong by weakening another best practice doesn’t net youmuch at all from a philosophical standpoint.After years of programming in Go, I still actually do recommend this pattern to other team members, but I admit I am not fullyconvinced. And it seems no one has time for a nuanced conversation about this. The other one I can think of is database/sql where the docs for Conn say “Prefer running queries from DB unless there is a specific need for a continuous single database connection”. This leads some engineers to write service logic that receives a *sql.DB including its administrative methods (SetConnMaxIdleTime, SetConnMaxLifetime, etc). &amp;#8617; Recall that interfaces are composed of the interface’s type and an instance of a type that implements the interface. It’s the latter that usually requires an allocation in the heap because it is usually implemented by a pointer value. &amp;#8617; May read a little more than required because the minimum bytes to be read is 512: https://github.com/golang/go/blob/21ff6704bc8efa72abe191263aae938f3c867480/src/encoding/json/stream.go#L146-L169 &amp;#8617; Among many others, one downside is it mitigates the compiler’s ability to eliminate dead code &amp;#8617; " }, { "title": "Kubernetes&#39; Controller Manager", "url": "/posts/k8s-kube-controller-manager/", "categories": "", "tags": "kubernetes, k8s, kube-controller-manager", "date": "2023-03-17 07:10:00 -0400", "snippet": "Kubernetes is a platform that automates many of the complexities behind deployment, scaling,and management of resources, such as pods. Users can configurethese resources imperatively using kubectl, or declaratively usingconfiguration files (also deployed using kubectl). At the heart of this platform lies a control loop that works tobring the current state of those resources to the desired state.In this article we will take a brief look at the component that manages the main control loop,kube-controller-manager,as well as the cloud-controller-manager that managesthe control loops specific to cloud environments.ArchitectureImage taken from Kubernetes docs.“CM” represents kube-controller-manager and “CCM” represents cloud-controller-manager.The controller manager is part of Kubernetes’control plane1 and runs on themaster nodes, normally as a standalone pod. It manages many built-in controllers for different resources such asdeployments or namespaces. You can find the full list of managed controllershere.Kubernetes implements an event-driven architecture with many independent components reacting to events and acting inconcert to drive the system to a desired state. The controller manager registers“watches” on theAPI server that open a connection throughwhich events are constantly streamed2. Each of these events has an associated action, such as “add” or “delete”,and a target resource. Here’s an example from the docs:GET /api/v1/namespaces/test/pods?watch=1&amp;amp;resourceVersion=10245---200 OKTransfer-Encoding: chunkedContent-Type: application/json{ &quot;type&quot;: &quot;ADDED&quot;, &quot;object&quot;: {&quot;kind&quot;: &quot;Pod&quot;, &quot;apiVersion&quot;: &quot;v1&quot;, &quot;metadata&quot;: {&quot;resourceVersion&quot;: &quot;10596&quot;, ...}, ...}}{ &quot;type&quot;: &quot;MODIFIED&quot;, &quot;object&quot;: {&quot;kind&quot;: &quot;Pod&quot;, &quot;apiVersion&quot;: &quot;v1&quot;, &quot;metadata&quot;: {&quot;resourceVersion&quot;: &quot;11020&quot;, ...}, ...}}...The event actions are determined by their type, and the target resource is the object. Note how object is justa regular spec (Pod specs in this example).The controller manager dispatches these events to controllers that have registeredthemselves to act on them based on the event’s action and the type of the resource. Note that these controllersdo not realize the end result directly (ie. create containers, create IP addresses, etc.), they merely updateresources that are exposed on the API server itself. In other words, they update the desired state of those resourcesby posting the updated spec back to the API server.It is other components, such as the kubelet and thekube-proxy, perform the actual grunt workderived from the desired state.High level view of a small subset of what happens when a new deployment is added.In the cloudIn cloud environments, the cloud-controller-managerruns alongside the kube-controller-manager. This controller manager operates the same way as the built-inkube-controller-manager in principle, with its control loops tailored specifically for management of cloudinfrastructure. It is this controller manager that handles updates from the cloud provider: nodes automatically enteringor leaving the cluster, provisioning of load balancers, updating IP routes within your cluster, and so on.ConclusionKubernetes is a distributed system that serves as a platform that automates many of the complexities behind deployment,scaling, and management of applications. The kube-controller-manager implements many critical control loops thatensure the cluster’s current state matches the user’s desired state. These control loops function independently of eachother, listening for events from the API server and modifying resources there as well. Other components, such as thekube-proxy and the kubelet perform the actual runtime configurations on the cluster’s nodes. In cloud environments,the cloud-controller-manager runs alongside the kube-controller-manager and implements control loops specific tocloud infrastructure.Footnotes Components on the control plane observe and adjust the network’s resources, topology, and routing tables in accordance with the desired state. See Wikipedia’s article on the control plane in the context of network routing. In contrast, the data plane is the part of a system that processes the actual traffic flowing through the network. Interestingly enough, Kubernetes does not have any components in the data plane. We already noted in a previous article how kube-proxy does not reside in this plane. Instead, it is a “Node Component” that configures IP routing rules on the local node. The local node’s network stack is in the data plane, kube-proxy is not. &amp;#8617; These “watches” are based on etcd3’s Watch API. Breadcrumbs: registerResourceHandlers -&amp;gt; restulListResource -&amp;gt; ListResource -&amp;gt; -serveWatch -&amp;gt; WatchServer.ServeHTTP. Stepping back in reverse order, we see how the etcd3 watch is wired in: ListResource -&amp;gt; restfulListResource -&amp;gt; registerResourceHandlers; implementations of rest.Watcher.Watch() include Store.Watch -&amp;gt; Store.WatchPredicate -&amp;gt; DryRunnableStorage.Watch; implementations of storage.Interface.Watch() include etcd3.store.Watch -&amp;gt; etcd3.watcher.Watch. &amp;#8617; " }, { "title": "Understanding Kubernetes&#39; Cluster Networking", "url": "/posts/k8s-cluster-network/", "categories": "", "tags": "kubernetes, k8s, iptables, networking, kube-proxy, kubelet, containerd, cni, cri", "date": "2023-03-10 08:50:00 -0500", "snippet": "Kubernetes is a system for automating deployment, scaling, and management of containerizedapplications.Networking is a central part of Kubernetes,and in this article we will explore how Kubernetes configures the cluster to handleeast-west traffic. We’ll reserve discussionon north-south traffic for a later article. This article is long and a bit heavy-handed on annotations, command-line instructions, and pointers to implementations inKubernetes and associated components. There are dozens of footnotes. We are diving into fairly deep waters here.I tried my best to keep a coherent flow going. Feel free to drop a comment below if you notice a mistake somewhereor if you’d like to offer editorial advice.ConceptsBy default, all pods in a K8s cluster can communicate with each other withoutNAT (source)1,therefore each pod is assigned a cluster-wide IP address. Containers within each pod share the pod’s network namespace,allowing them to communicate with each other on localhost via the loopback interface. From the point of view ofthe workloads running inside the containers, this IP network looks like any other and no changes are necessary.Conceptual view of inter-Pod and intra-Pod network communication.Recall from a previous article that as far as K8s components go, thekubelet and the kube-proxy are responsible for creating pods and applying network configurations on the cluster’s nodes.When the pod is being created or terminated, part of the kubelet’s jobis to set up or cleanup the pod’s sandbox on the node it is running on. The kubelet relies on theContainer Runtime Interface (CRI) implementation to handle the details of creatingand destroying sandboxes. The CRI is composed of several interfaces; the interesting ones for us are theRuntimeServiceinterface (client-side API; integration point kubelet-&amp;gt;CRI) and theRuntimeServiceServerinterface (server-side API; integration point RuntimeService-&amp;gt;CRI implementation). These APIs are both bigand fat, but for this article we are only interested in the *PodSandbox set of methods (e.g. RunPodSandbox).Underneath the CRI’s hood, however, is the Container Network Interface thatcreates and configures the pod’s network namespace2.The kube-proxy configures routing rules to proxy traffic directed atServices and performs simple load-balancing betweenthe corresponding Endpoints3.Finally, a third component, coreDNS, resolves network names by looking them up in etcd.Components involved in the network configuration for a pod. Blue circles are pods and orange rectangles are daemons.Note that etcd is shown here as a database service, but it is also deployed as a pod.In the next section we will understand how pod networking works by manually creating our own pods and have a client inone pod invoke an API in a different pod. I will be using a simple K8s cluster I set up with kind in the walkthrough below.kind creates a docker container per K8s node. You may choose a similar sandbox, machine instances in the cloud, or anyother setup that simulates at least two host machines connected to the same network. Also note that Linux hosts are usedfor this walkthrough.Create your own Pod NetworkWe will manually create pods on different hosts to gain an understanding of how Kubernetes’ networking is configuredunder the hood.Network namespacesLinux has a concept called namespaces. Namespaces are a feature thatisolate the resources that a process sees from another processes. For example, a process may see MySQL running with PID123 but a different process running in a different namespace (but on the same host) will see a different process assignedto PID 123, or none at all.There are different kinds of namespaces; we are interested in the Network (net)namespace.Each namespace has a virtual loopback interface and may have additional virtual network devices attached.Each of these virtual devices may be assigned exclusive or overlapping IP address ranges.localhostProcesses running inside the same net namespace can send messages to each other over localhost. Hands On Create a net namespace with a client and a server: On a host we’ll call “client” # create network namespaceroot@kind-control-plane:/# ip netns add clientroot@kind-control-plane:/# ip netns listclient# `loopback` is DOWN by defaultroot@kind-control-plane:/# ip netns exec client ip link list1: lo: &amp;lt;LOOPBACK&amp;gt; mtu 65536 qdisc noqueue state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00# initialize `loopback` (`lo` is shorthand for &quot;loopback&quot;)root@kind-control-plane:/# ip netns exec client ip link set lo up# start the serverroot@kind-control-plane:/# ip netns exec client nohup python3 -m http.server 8080 &amp;amp;[1] 29509root@kind-control-plane:/# nohup: ignoring input and appending output to &#39;nohup.out&#39;# invoke the serverroot@kind-control-plane:/# ip netns exec client curl -m 2 localhost:8080&amp;lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; &quot;http://www.w3.org/TR/html4/strict.dtd&quot;&amp;gt;&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot;&amp;gt;&amp;lt;title&amp;gt;Directory listing for /&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Directory listing for /&amp;lt;/h1&amp;gt;&amp;lt;hr&amp;gt;&amp;lt;ul&amp;gt;...&amp;lt;/ul&amp;gt;&amp;lt;hr&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt; Traffic from a client to a server inside a network namespace. Blue is traffic on localhost. Notice the host’s interface (eth0) is bypassed entirely for this traffic.With this we have one or more processes that can communicate over localhost. This is exactly how K8s Pods work, and these“processes” are K8s containers.Connecting network namespaces on the same hostRemember that all pods in a K8s cluster can communicate with each other without NAT. So, how would two pods on the samehost communicate with each other? Let’s give it a shot. Let’s create a “server” namespace and attempt to communicate with it. Hands On On the same “client” host # create the other pod&#39;s network namespaceroot@kind-control-plane:/# ip netns add serverroot@kind-control-plane:/# ip netns listserverclient# stop the server you had running before and restart it in the new `server` namespaceroot@kind-control-plane:/# ip netns exec server nohup python3 -m http.server 8080 &amp;amp;[1] 29538root@kind-control-plane:/# nohup: ignoring input and appending output to &#39;nohup.out&#39;# attempt to call this server from the client namespaceroot@kind-control-plane:/# ip netns exec client curl localhost:8080 curl: (7) Failed to connect to localhost port 8080 after 0 ms: Connection refused We don’t have an address for server from within the client namespace yet. These two network namespaces are completelydisconnected from each other. All client and server have is localhost (dev lo) which isalways assigned 127.0.0.1. We need another interface between these two namespaces for communication to happen.Linux has the concept of Virtual Ethernet Devices (veth) that actlike “pipes” through which network packets flow, and of which you can attach either end to a namespace or a device. The“ends” of these “pipes” act as virtual devices to which IP addresses can be assigned. It is perfectly possible to createa veth device and connect our two namespaces like this:However, consider that veth are point-to-point devices with just two ends and, remembering our requirement that allPods must communicate with each other without NAT, we would need \\(n(n-1)/2\\) veth pairs, where \\(n\\) is thenumber of namespaces. This becomes unwieldy pretty quickly. We will use abridge instead to solve this problem. A bridge lets us connect anynumber of devices to it and will happily route traffic between them, turning our architecture into a hub-and-spoke andreducing the number of veth pairs to just \\(n\\). Hands On On the “client” host # create a bridgeroot@kind-control-plane:/# ip link add bridge type bridge# create veth pairsroot@kind-control-plane:/# ip link add veth-client type veth peer name veth-clientbrroot@kind-control-plane:/# ip link add veth-server type veth peer name veth-serverbr# connect one end of the veth devices to the bridgeroot@kind-control-plane:/# ip link set veth-clientbr master bridgeroot@kind-control-plane:/# ip link set veth-serverbr master bridge# attach the other end of the veth devices to their respective namespacesroot@kind-control-plane:/# ip link set veth-client netns client root@kind-control-plane:/# ip link set veth-server netns server # assign IP addresses to the bridge and our new interfaces inside the client and server namespacesroot@kind-control-plane:/# ip netns exec client ip addr add 10.0.0.1/24 dev veth-clientroot@kind-control-plane:/# ip netns exec server ip addr add 10.0.0.2/24 dev veth-serverroot@kind-control-plane:/# ip addr add 10.0.0.0/24 dev bridge# bring our devices uproot@kind-control-plane:/# ip netns exec client ip link set veth-client uproot@kind-control-plane:/# ip netns exec server ip link set veth-server uproot@kind-control-plane:/# ip link set veth-clientbr uproot@kind-control-plane:/# ip link set veth-serverbr uproot@kind-control-plane:/# ip link set bridge up# confirm state of our interfaces:# state of client interfacesroot@kind-control-plane:/# ip netns exec client ip addr1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever16: veth-client@if15: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 5e:0e:50:4b:f5:32 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.0.0.1/24 scope global veth-client valid_lft forever preferred_lft forever inet6 fe80::5c0e:50ff:fe4b:f532/64 scope link valid_lft forever preferred_lft forever# state of server interfacesroot@kind-control-plane:/# ip netns exec server ip addr...18: veth-server@if17: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 46:d0:61:5d:7c:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.0.0.2/24 scope global veth-server valid_lft forever preferred_lft forever inet6 fe80::44d0:61ff:fe5d:7c9a/64 scope link valid_lft forever preferred_lft forever# state of host interfacesroot@kind-control-plane:/# ip addr ...11: eth0@if12: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fc00:f853:ccd:e793::2/64 scope global nodad valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe12:2/64 scope link valid_lft forever preferred_lft forever14: bridge: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether ba:21:cf:c1:62:52 brd ff:ff:ff:ff:ff:ff inet 10.0.0.0/24 scope global bridge valid_lft forever preferred_lft forever15: veth-clientbr@if16: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue master bridge state UP group default qlen 1000 link/ether ba:21:cf:c1:62:52 brd ff:ff:ff:ff:ff:ff link-netns client17: veth-serverbr@if18: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue master bridge state UP group default qlen 1000 link/ether c2:52:97:04:03:2c brd ff:ff:ff:ff:ff:ff link-netns server# test connectivityroot@kind-control-plane:/# ip netns exec client curl -v 10.0.0.2:8080&amp;lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; &quot;http://www.w3.org/TR/html4/strict.dtd&quot;&amp;gt;&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot;&amp;gt;&amp;lt;title&amp;gt;Directory listing for /&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Directory listing for /&amp;lt;/h1&amp;gt;&amp;lt;hr&amp;gt;&amp;lt;ul&amp;gt;...&amp;lt;/ul&amp;gt;&amp;lt;hr&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt; At this point the whole setup looks like this:Two linux net namespaces connected to each other via a bridge. Note that although the bridge is connected to the host’sinterface (eth0), traffic between the namespaces bypasses it entirely.We have just connected two network namespaces on the same host.Connecting network namespaces on different hostsThe only way in and out of our hosts in our example above is via their eth0 interface. For outbound traffic, the packetsfirst need to reach eth0 before being forwarded to the physical network. For inbound packets, eth0 needs to forwardthose to the bridge where they will be routed to the respective namespace interfaces. Let’s first separate our twonamespaces before going further.Moving our network namespaces onto different hostsLet’s first clean up everything we’ve done so far4: Hands On Steps # delete the namespacesroot@kind-control-plane:/# ip netns del clientroot@kind-control-plane:/# ip netns del server# delete the veth and bridge devicesroot@kind-control-plane:/# ip link del veth-clientroot@kind-control-plane:/# ip link del veth-serverroot@kind-control-plane:/# ip link del bridge Let’s now set up our namespaces in different hosts. Hands On Same steps as before except on different hosts with some minor differences: On the “client” host root@kind-control-plane:/# ip netns add clientroot@kind-control-plane:/# ip link add bridge type bridgeroot@kind-control-plane:/# ip link add veth-client type veth peer name veth-clientbrroot@kind-control-plane:/# ip link set veth-client netns clientroot@kind-control-plane:/# ip link set veth-clientbr master bridgeroot@kind-control-plane:/# ip addr add 10.0.0.0/24 dev bridgeroot@kind-control-plane:/# ip netns exec client ip addr add 10.0.0.1/24 dev veth-clientroot@kind-control-plane:/# ip netns exec client ip link set lo uproot@kind-control-plane:/# ip netns exec client ip link set veth-client uproot@kind-control-plane:/# ip link set bridge uproot@kind-control-plane:/# ip link set veth-clientbr up On the “server” host root@kind-worker:/# ip netns add serverroot@kind-worker:/# ip link add bridge type bridgeroot@kind-worker:/# ip link add veth-server type veth peer name veth-serverbrroot@kind-worker:/# ip link set veth-server netns serverroot@kind-worker:/# ip link set veth-serverbr master bridgeroot@kind-worker:/# ip addr add 10.0.0.0/24 dev bridgeroot@kind-worker:/# ip netns exec server ip addr add 10.0.0.2/24 dev veth-serverroot@kind-worker:/# ip netns exec server ip link set lo uproot@kind-worker:/# ip netns exec server ip link set veth-server uproot@kind-worker:/# ip link set bridge uproot@kind-worker:/# ip link set veth-serverbr up# run the serverroot@kind-worker:/# ip netns exec server nohup python3 -m http.server 8080 &amp;amp;[1] 1314nohup: ignoring input and appending output to &#39;nohup.out&#39; Namespaces on different hosts. The host interfaces (eth0) are on the same network.Now that everything is set up, let’s first tackle outbound traffic.From our network namespaces to the physical networkFirst let’s see if we can reach eth0 on each host:# on the client hostroot@kind-control-plane:/# ip netns exec client ping 172.18.0.2ping: connect: Network is unreachable# on the server hostroot@kind-worker:/# ip netns exec server ping 172.18.0.4ping: connect: Network is unreachableThe host isn’t reachable from the namespaces yet. We haven’t configured an IP route5 to forward packets destined toeth0 in neither host. Let’s set up a default route via the bridge in both namespaces and test:# on client hostroot@kind-control-plane:/# ip netns exec client ip route add default via 10.0.0.0root@kind-control-plane:/# ip netns exec client ping 172.18.0.2 -c 2PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data.64 bytes from 172.18.0.2: icmp_seq=1 ttl=64 time=0.076 ms64 bytes from 172.18.0.2: icmp_seq=2 ttl=64 time=0.039 ms--- 172.18.0.2 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1031msrtt min/avg/max/mdev = 0.039/0.057/0.076/0.018 ms# on server hostroot@kind-worker:/# ip netns exec server ip route add default via 10.0.0.0root@kind-worker:/# ip netns exec server ping 172.18.0.4 -c 2PING 172.18.0.4 (172.18.0.4) 56(84) bytes of data.64 bytes from 172.18.0.4: icmp_seq=1 ttl=64 time=0.036 ms64 bytes from 172.18.0.4: icmp_seq=2 ttl=64 time=0.035 ms--- 172.18.0.4 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1031msrtt min/avg/max/mdev = 0.035/0.035/0.036/0.000 msGreat, we can now reach our host interfaces. By extension, we can also reach any destination reachable from eth0:# on client hostroot@kind-control-plane:/# ip netns exec client curl https://google.com&amp;lt;HTML&amp;gt;&amp;lt;HEAD&amp;gt;&amp;lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;&amp;gt;&amp;lt;TITLE&amp;gt;301 Moved&amp;lt;/TITLE&amp;gt;&amp;lt;/HEAD&amp;gt;&amp;lt;BODY&amp;gt;&amp;lt;H1&amp;gt;301 Moved&amp;lt;/H1&amp;gt;The document has moved&amp;lt;A HREF=&quot;https://www.google.com/&quot;&amp;gt;here&amp;lt;/A&amp;gt;.&amp;lt;/BODY&amp;gt;&amp;lt;/HTML&amp;gt;# on server hostroot@kind-worker:/# ip netns exec server curl https://google.com&amp;lt;HTML&amp;gt;&amp;lt;HEAD&amp;gt;&amp;lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;&amp;gt;&amp;lt;TITLE&amp;gt;301 Moved&amp;lt;/TITLE&amp;gt;&amp;lt;/HEAD&amp;gt;&amp;lt;BODY&amp;gt;&amp;lt;H1&amp;gt;301 Moved&amp;lt;/H1&amp;gt;The document has moved&amp;lt;A HREF=&quot;https://www.google.com/&quot;&amp;gt;here&amp;lt;/A&amp;gt;.&amp;lt;/BODY&amp;gt;&amp;lt;/HTML&amp;gt;This flow looks similar to the following when viewed from the client flow (Google’s infrastructure has been vastlysimplified):Next up, let’s try to communicate to our server from the client namespace.From the physical network to our network namespacesIf we try to reach server from client we can see that it doesn’t work:root@kind-control-plane:/# ip netns exec client curl -m 2 10.0.0.2:8080curl: (28) Connection timed out after 2001 millisecondsLet’s dig in with tcpdump.Open a terminal window and, since we aren’t sure what path the packets are flowing through, run tcpdump -nn -e -l -i anyon host 172.18.0.2. Friendly warning: the output will be very verbose because tcpdump will listen on all interfaces.On the same host 172.18.0.2, try to curl the server from the client namespace again withip netns exec client curl -m 2 10.0.0.2:8080. After it times out again, stop tcpdump by pressing Ctrl+C and reviewthe output. Search for 10.0.0.2, our destination address. You should spot some lines like the following:15:05:35.754605 bridge Out ifindex 5 a6:93:c7:0c:96:b2 ethertype ARP (0x0806), length 48: Request who-has 10.0.0.2 tell 10.0.0.0, length 2815:05:35.754608 veth-clientbr Out ifindex 6 a6:93:c7:0c:96:b2 ethertype ARP (0x0806), length 48: Request who-has 10.0.0.2 tell 10.0.0.0, length 28You may see several of these requests with no corresponding reply6.These are ARP requests, and the reason they’re being fired offis that there is no [IP (layer 3)] route between the client and servernamespaces. It is possible tomanually configure ARP entries and implement“proxy-ARP” to connect client and serverat Layer 2, but we are not doing that today. Kubernetes’ networking modelis built on Layer 3 and up, and so must our solution.We will configure IP routing5 rules to route client traffic to server. Let’s first configure a manual route for 10.0.0.2on the client host:# on client hostroot@kind-control-plane:/# ip route add 10.0.0.2 via 172.18.0.4# validateroot@kind-control-plane:/# curl 10.0.0.2:8080&amp;lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; &quot;http://www.w3.org/TR/html4/strict.dtd&quot;&amp;gt;&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot;&amp;gt;&amp;lt;title&amp;gt;Directory listing for /&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Directory listing for /&amp;lt;/h1&amp;gt;&amp;lt;hr&amp;gt;&amp;lt;ul&amp;gt;...&amp;lt;/ul&amp;gt;&amp;lt;hr&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;As you can see, curl‘ing our server API in the server namespace from the client host now works7.Let’s try curl‘ing the server from the client namespace again:root@kind-control-plane:/# ip netns exec client curl -m 2 10.0.0.2:8080curl: (28) Connection timed out after 2001 millisecondsAnother dump with tcpdump reveals the same unanswered ARP requests as before. Why aren’t there responses to theseconsidering we’ve successfully established a connection from the client host to the server namespace? One reason isthat the connection was made at layer 3 (IP route), but ARP is a layer 2 protocol, and as per theOSI model’s semantics, lower-level protocols cannot depend on higher-level ones.Another reason is that ARP messages only reach devices directly connected to our network interface, in this case eth0:the latter’s ARP table does not contain an entry for 10.0.0.2 even though its namespace’s IP routing table does.The layer 3 solution for us is simple: establish another IP route for 10.0.0.2 inside the client namespace8:root@kind-control-plane:/# ip netns exec client ip route add 10.0.0.2 via 10.0.0.0You can now verify that calling server from client works:root@kind-control-plane:/# ip netns exec client curl -m 2 10.0.0.2:8080&amp;lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; &quot;http://www.w3.org/TR/html4/strict.dtd&quot;&amp;gt;&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot;&amp;gt;&amp;lt;title&amp;gt;Directory listing for /&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Directory listing for /&amp;lt;/h1&amp;gt;&amp;lt;hr&amp;gt;&amp;lt;ul&amp;gt;...&amp;lt;/ul&amp;gt;&amp;lt;hr&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;Congratulations 🎉 🎉 - we have just manually created two Pods (net namespaces) on different hosts, with one “container”(curl) in one Pod invoking an API in a container in the other Pod without NAT.A process inside a client namespace connecting to an open socket on a server namespace in another host. The clientprocess does not perform any NAT.How Kubernetes creates PodsWe now know how pods are implemented under the hood. We have learned that Kubernetes “pods” are namespaces and thatKubernetes “containers” are processes running within those namespaces. These pods are connected to each other withineach host with virtual networking devices (veth, bridge), and with simple IP routing rules for traffic to crossfrom one pod to another over the physical network.Where and how does Kubernetes do all this?The Container Runtime Interface (CRI)Back in the concepts section we said the kubelet uses theContainer Runtime Interface to create the pod “sandboxes”.The kubelet creates pod sandboxeshere.Note that runtimeService is of typeRuntimeService,belonging to the CRI API. It embeds the PodSandboxManager type, which is responsible for actually creating the sandboxes(RunPodSandbox method). Kubernetes has an internal implementation of RuntimeService inremoteRuntimeService,but this is just a thin wrapper around the CRI API’sRuntimeServiceClient(GitHub won’t automatically open the file due to its size). Look closely and you’ll notice that RuntimeServiceClientis implemented byruntimeServiceClient,which uses a gRPC connection to invoke the container runtime service. gRPC is (normally) transportedover TCP sockets (Layer 3).The kubelet runs on each node and, if it needs to create a pod on that node, why would it need to communicate withthe CRI service over TCP?Go, the lingua franca of cloud-native development (including Kubernetes), has a builtinplugin system but it has some serious drawbacks in terms of maintainability.Eli Bendersky gives a good outline of how they work with pros and cons herethat is worth a read. Towards the end of the article you’ll notice a bias towards RPC-based plugins; this is exactly what the CRI’s designerschose as their architecture. So although the kubelet and the CRI service are running on the same node, the gRPC messagescan be transported locally via localhost (for TCP) or Unix domain socketsor some other channel available on the host.So we now have Kubernetes invoking the standard CRI API that in turn invokes a “remote”, CRI-compliant gRPC service.This service is the CRI implementation that can be swapped out. Kubernetes’ docs lista few common ones: containerd CRI-O Docker Engine Mirantis Container RuntimeThe details of what happens next vary by implementation, and is all abstracted away from the Kubernetes runtime.Take containerd as an example (it’s the CRI used in kind, the K8S distributionI chose for the walkthrough above).containerd has a plugin architecture that is resolved at compile time9. containerd’simplementationof RuntimeServiceServer (part of Concepts) has itsRunPodSandboxmethod (also part of Concepts) rely on a “CNI” plugin to set up the pod’s network namespace.What is the CNI?The Container Network Interface (CNI)The CNI is used by the CRI to create and configure the network namespacesused by the pods10. CNI implementations are invoked by executing their respective binaries and providing networkconfiguration via stdin (see the spec’sexecution protocol)11.On unix hosts, containerd by default looks for a standard CNI config file inside the /etc/cni/net.d directory and for theplugin binaries it looks in /opt/cni/bin (seecode).Each node in my kind cluster has only one config file: /etc/cni/net.d/10-kindnet.conflist. Here are the contents ofthis file in my control-plane node: Click to expand { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;kindnet&quot;, &quot;plugins&quot;: [ { &quot;type&quot;: &quot;ptp&quot;, &quot;ipMasq&quot;: false, &quot;ipam&quot;: { &quot;type&quot;: &quot;host-local&quot;, &quot;dataDir&quot;: &quot;/run/cni-ipam-state&quot;, &quot;routes&quot;: [ { &quot;dst&quot;: &quot;0.0.0.0/0&quot; } ], &quot;ranges&quot;: [ [ { &quot;subnet&quot;: &quot;10.244.0.0/24&quot; } ] ] }, &quot;mtu&quot;: 1500 }, { &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: { &quot;portMappings&quot;: true } } ]} The same config file on the worker nodes have identical content except for subnet, which varies from host to host.I won’t go in depth about how the CNI spec and plugins work (that deserves its own article). You can read version 0.3.1of the spec here. What’s conceptually important for usis that there are three plugins being executed (two of them are chained) with this configuration. These plugins are: ptp: creates a point-to-point link between a container and the host by using a veth device. host-local: allocates IPv4 and IPv6 addresses out of a specified address range. portmap: will forward traffic from one or more ports on the host to the container.Do any of these concepts sound familiar to you? They should!12 These are the things we painstakingly configured step-by-stepin our walkthrough above. With this information in mind, go back to the component diagram in Conceptsand map each of these concepts to the boxes in the diagram.ServicesNo discussion of Kubernetes’ cluster network can conclude without mentioning Services.Conceptually, a Kubernetes Service is merely a Virtual IP assignedto a set of pods, and to which a stable DNS name is assigned.Kubernetes also provides simple load balancing out of the box for some types of services (ClusterIP, NodePort).Each service is mapped to a set of IPs belonging to the pods exposed by the service. These set of IPs is calledEndpointSlice and is constantly updatedto reflect the IPs currently in use by the backend pods13. Which pods? The ones matching the service’s selector. Example Service with label ‘myLabel’ set to value ‘MyApp’ apiVersion: v1 kind: Service metadata: name: my-service spec: selector: myLabel: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 When a user creates a new Service: kube-apiserver assigns it the next free IP by incrementing a counter stored in etcd14. kube-apiserver stores the service in etcd15. This event is pushed to all watches16. coreDNS: Event is caught and the service’s name, namespace, and (virtual) cluster IP is cached17. Responds to requests for A records by reading from the cache18. EndpointSlice Controller: event is caught and a new EndpointSlice is assigned to the service19. kube-proxy: event is caught and iptables is configured on worker nodes20.All steps from 4 onwards are executing concurrently by independent processes. The final state is depicted in the diagram in the Concepts section.Note that we have incidentally glossed over Kubernetes’ distributed and event-driven architecture. We’ll expand on that topicin a future article.We snuck in a new concept in step 6: iptables. Let’s expand on that next.iptables Iptables is used to set up, maintain, and inspect the tables of IP packet filter rules in the Linux kernel.Several different tables may be defined. Each table contains a number of built-in chains and may also containuser-defined chains. Each chain is a list of rules which can match a set of packets. Each rule specifies what to do with a packetthat matches. This is called a `target’, which may be a jump to a user-defined chain in the same table. – iptables manpageSystem and network administrators use iptables to configure IP routing rules on Linux hosts, and so does kube-proxy21.On Windows hosts kube-proxy uses an analogous API calledHost Compute Network service API,internally represented by theHostNetworkServiceinterface. It is because of this difference in OS-dependent implementations of the network stackthat we simply labelled them as “OS IP rules” in the Concepts section’s diagram.kube-proxy uses iptables to configure Linux hosts to distribute traffic directed at a Service’s clusterIP(ie. a virtual IP) to the backend pods selected by the service using NAT.So yes, there is definitely network address translation in a Kubernetes cluster, but it’s hidden from your workloads.kube-proxy adds a rule to the PREROUTING chain that targets a custom chain called KUBE-SERVICES22.The end result looks like this:root@kind-control-plane:/# iptables -t nat -L PREROUTING -n -vChain PREROUTING (policy ACCEPT 18999 packets, 3902K bytes) pkts bytes target prot opt in out source destination 18955 3898K KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */Initially the KUBE-SERVICES chain contains rules just for the NodePort custom chain and several built-in services:root@kind-control-plane:/# iptables -t nat -L KUBE-SERVICES -n -vChain KUBE-SERVICES (2 references) pkts bytes target prot opt in out source destination 0 0 KUBE-SVC-TCOU7JCQXEZGVUNU udp -- * * 0.0.0.0/0 10.96.0.10 /* kube-system/kube-dns:dns cluster IP */ udp dpt:53 0 0 KUBE-SVC-ERIFXISQEP7F7OF4 tcp -- * * 0.0.0.0/0 10.96.0.10 /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53 0 0 KUBE-SVC-JD5MR3NA4I4DYORP tcp -- * * 0.0.0.0/0 10.96.0.10 /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153 0 0 KUBE-SVC-NPX46M4PTMTKRN6Y tcp -- * * 0.0.0.0/0 10.96.0.1 /* default/kubernetes:https cluster IP */ tcp dpt:443 417 25020 KUBE-NODEPORTS all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCALNew rules are appended for each service by the Proxier’s syncProxyRules method and are writtenhere.For example, the following shows a rule targeting a custom chain KUBE-SVC-BM6F4AVTDKG47F3K for a service named mysvc:root@kind-control-plane:/# iptables -t nat -L KUBE-SERVICES -n -vChain KUBE-SERVICES (2 references) pkts bytes target prot opt in out source destination 0 0 KUBE-SVC-BM6F4AVTDKG47F3K tcp -- * * 0.0.0.0/0 10.96.62.22 /* default/mysvc cluster IP */ tcp dpt:8080 0 0 KUBE-SVC-TCOU7JCQXEZGVUNU udp -- * * 0.0.0.0/0 10.96.0.10 /* kube-system/kube-dns:dns cluster IP */ udp dpt:53 0 0 KUBE-SVC-ERIFXISQEP7F7OF4 tcp -- * * 0.0.0.0/0 10.96.0.10 /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53 0 0 KUBE-SVC-JD5MR3NA4I4DYORP tcp -- * * 0.0.0.0/0 10.96.0.10 /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153 0 0 KUBE-SVC-NPX46M4PTMTKRN6Y tcp -- * * 0.0.0.0/0 10.96.0.1 /* default/kubernetes:https cluster IP */ tcp dpt:443 417 25020 KUBE-NODEPORTS all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCALIf we inspect KUBE-SVC-BM6F4AVTDKG47F3K we see something interesting:root@kind-control-plane:/# iptables -t nat -L KUBE-SVC-BM6F4AVTDKG47F3K -n -vChain KUBE-SVC-BM6F4AVTDKG47F3K (1 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ tcp -- * * !10.244.0.0/16 10.96.62.22 /* default/mysvc cluster IP */ tcp dpt:8080 0 0 KUBE-SEP-CMSFOBEB7HHZOTBZ all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/mysvc -&amp;gt; 10.244.1.2:8080 */ statistic mode random probability 0.33333333349 0 0 KUBE-SEP-VVWLMARALSB3FCZF all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/mysvc -&amp;gt; 10.244.2.2:8080 */ statistic mode random probability 0.50000000000 0 0 KUBE-SEP-XGAC3VXZG7B73WCD all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/mysvc -&amp;gt; 10.244.2.3:8080 */Ignoring the masq for now, we see three rules targeting chains for service endpoints. kube-proxy adds these entriesas it handles incoming events for endpointslices(see NewProxier()).Each rule has a helpful comment indicating the target service endpoint.Note how these rules have a probability assigned to them. Rules in iptables chains are processed sequentially.In this example there are three service endpoint rules, and the first is assigned a probability of 0.33. Next, if the dice rollfailed on the first one, we roll it again for the second rule, this time with a probability of 50%. If that fails,we fall back to the third rule with a probability of 100%. In this way we have an even distribution of traffic amongstthe three endpoints. The probabilities are sethere.Note how the probability curve is fixed as a flat distribution, and also note how kube-proxy is not balancing thistraffic itself. As noted in Concepts, kube-proxy is not itself in the data plane.In our example above, mysvc is selecting three pods with endpoints 10.244.1.2:8080, 10.244.2.2:8080, and 10.244.2.3:8080.This is the service definition:apiVersion: v1kind: Servicemetadata: labels: app: test name: mysvc namespace: defaultspec: type: ClusterIP ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: testAnd these are the IPs assigned to the selected pods (take note of the nodes as well):$ k get po -l app=test -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStest-75d6d47c7f-jcdzz 1/1 Running 0 4d7h 10.244.2.2 kind-worker2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;test-75d6d47c7f-lgqcq 1/1 Running 0 4d7h 10.244.1.2 kind-worker &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;test-75d6d47c7f-pjrjp 1/1 Running 0 4d7h 10.244.2.3 kind-worker2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;If we inspect one of the service endpoint chains we see something else interesting:root@kind-control-plane:/# iptables -t nat -L KUBE-SEP-CMSFOBEB7HHZOTBZ -n -vChain KUBE-SEP-CMSFOBEB7HHZOTBZ (1 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ all -- * * 10.244.1.2 0.0.0.0/0 /* default/mysvc */ 0 0 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/mysvc */ tcp to:10.244.1.2:8080We see a DNAT (destination NAT) rule that translates the destination address to 10.244.1.2:8080.We already know that this destination is hosted on node kind-worker, so investigating on that node we see:# list devices and their assigned IP rangesroot@kind-worker:/# ip addr 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: veth4e573577@if2: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default link/ether 5a:b9:16:0d:a6:18 brd ff:ff:ff:ff:ff:ff link-netns cni-b5e04919-09af-0a9f-6945-a9929d71d789 inet 10.244.1.1/32 scope global veth4e573577 &amp;lt;------ 10.244.1.2 IS IN THIS RANGE valid_lft forever preferred_lft forever13: eth0@if14: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.18.0.3/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fc00:f853:ccd:e793::3/64 scope global nodad valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe12:3/64 scope link valid_lft forever preferred_lft forever# show deviceroot@kind-worker:/# ip link list veth4e5735772: veth4e573577@if2: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 5a:b9:16:0d:a6:18 brd ff:ff:ff:ff:ff:ff link-netns cni-b5e04919-09af-0a9f-6945-a9929d71d789 &amp;lt;------ NETWORK NAMESPACE# list network namespacesroot@kind-worker:/# ip netns listcni-b5e04919-09af-0a9f-6945-a9929d71d789# list all processes running in the target namespaceroot@kind-worker:/# ps $(ip netns pids cni-b5e04919-09af-0a9f-6945-a9929d71d789) PID TTY STAT TIME COMMAND 505179 ? Ss 0:00 /pause 505237 ? Ss 0:00 nginx: master process nginx -g daemon off; 505278 ? S 0:00 nginx: worker process 505279 ? S 0:00 nginx: worker process 505280 ? S 0:00 nginx: worker process 505281 ? S 0:00 nginx: worker process 505282 ? S 0:00 nginx: worker process 505283 ? S 0:00 nginx: worker process 505284 ? S 0:00 nginx: worker process 505285 ? S 0:00 nginx: worker process 505286 ? S 0:00 nginx: worker process 505287 ? S 0:00 nginx: worker process 505288 ? S 0:00 nginx: worker process 505289 ? S 0:00 nginx: worker process 505290 ? S 0:00 nginx: worker process 505291 ? S 0:00 nginx: worker process 505292 ? S 0:00 nginx: worker process 505293 ? S 0:00 nginx: worker processWe are back in net namespace land!In our case, we are running nginx on a simple deployment: Spec apiVersion: apps/v1kind: Deploymentmetadata: labels: app: test name: test namespace: defaultspec: replicas: 3 selector: matchLabels: app: test template: metadata: labels: app: test spec: containers: - image: nginx name: nginx Tying it all togetherKubernetes is an event-driven, distributed platform that automates the deployment and networkingaspects of your workloads. kube-apiserver is the platform’s “event hub”.Blue arrows show where configuration data for Deployments flow. Red arrows show where configuration data for Servicesflow. Note that this is just a subset of all the machinery activated when a user creates either of these two resources.kubelet runs on each node and listens for events from kube-apiserver where pods are added to the node it’s running on.When a pod is created, be it with a controller or just an orphaned pod, kubelet uses the Container Runtime Interface (CRI)to create the pod’s sandbox. The CRI in turn uses the Container Network Interfaceto configure the pod’s network namespace on the node. The pod will have an IP that is reachable by any other pod in anyother node.When a ClusterIP Service is created, kube-apiserver assigns a free Virtual IP to it and persists the Service objectto etcd. The event is caught by coreDNS which proceeds to cache the service_name -&amp;gt; cluster_ip mapping, and respondto DNS requests accordingly. The event is also caught by the EndpointSlice controller which then creates and attachesan EndpointSlice with the IPs of the selected Pods to the Service and saves the update to etcd.kube-proxy runs on each node and listens for events from kube-apiserver where Services and EndpointSlices are addedand configures the local node’s IP routing rules to point the Service’s virtual IP to the backend Pods with an evendistribution.During runtime, a client container queries coreDNS for the Service’s address and directs its request to the Service’svirtual IP. The local routing rules (iptables on Linux hosts, Host Compute Service API on Windows) randomlyselect one of the backend Pod IP addresses and forwards traffic to that Pod.Footnotes You can use the NetworkPolicy resource (+ a suitable CNI plugin) to block traffic to/from Pods. &amp;#8617; Despite the CNI featuring prominently in K8S docs, Kubernetes does not actually interface with the CNI directly as others have pointed out here. Kubernetes’ source code does not depend on the CNI API. &amp;#8617; Note that kube-proxy is itself not actually in the request path (data plane). &amp;#8617; Don’t worry too much: the changes done so far are not persistent across system restarts. &amp;#8617; Wikipedia has a very nice description of the IP routing algorithm here. &amp;#8617; &amp;#8617;2 A reply would look like this: 14:47:51.365200 bridge In ifindex 5 06:82:91:69:f0:36 ethertype ARP (0x0806), length 48: Reply 10.0.0.1 is-at 06:82:91:69:f0:36, length 28 &amp;#8617; If you capture another dump with tcpdump you’ll notice an absence of ARP requests for 10.0.0.2. This is because the route forwards the traffic to 172.18.0.4, and the MAC address for the latter is already cached in the host’s ARP table. &amp;#8617; In reality, Kubernetes does this in a more efficient way by configuring IP routes for IP ranges (segments) instead of specific addresses. You can verify IP routes on a host with ip route list. In my case, I could see that Kubernetes has routed 10.244.1.0/24 via 172.18.0.4 (our “server” host) and 10.244.2.0/24 via 172.18.0.3 (a third node not relevant to our discussion). &amp;#8617; As described by Eli’s article and the opposite of the kubelet-&amp;gt;CRI integration. containerd’s CRI service is a plugin that is registered here. &amp;#8617; At the moment the CNI’s scope is limited to network-related configurations during creation and deletion of a pod. The README notes that future extensions could be possible to enable dynamic scenarios such as NetworkPolicies (cilium already supports network policies). &amp;#8617; Yet another way to implement a plugin architecture. &amp;#8617; Assuming I’ve done a decent job in this article :). &amp;#8617; Update is done by the EndpointSlice Controller. We’ll talk about this and other controllers in a future article. &amp;#8617; Breadcrumbs: (Service REST storage -&amp;gt; allocator -&amp;gt; Range allocator -&amp;gt; etcd storage) &amp;#8617; See (Store.Create). &amp;#8617; We will cover watches in more detail in a future article. &amp;#8617; Breadcrumbs: InitKubeCache -&amp;gt; dnsController.Run -&amp;gt; controller.Run -&amp;gt; Reflector.Run -&amp;gt; Reflector.ListAndWatch -&amp;gt; watchHandler. &amp;#8617; Breadcrumbs: ServeDNS -&amp;gt; A() -&amp;gt; checkForApex -&amp;gt; Services() -&amp;gt; Records() -&amp;gt; findServices -&amp;gt; SvcIndex -&amp;gt; ByIndex (client-go). &amp;#8617; See Controller.syncService. &amp;#8617; Breadcrumbs: ProxyServer.Run -&amp;gt; NewServiceConfig -&amp;gt; ServiceConfig.handleAddService -&amp;gt; Proxier.OnServiceAdd -&amp;gt; Proxier.OnServiceUpdate -&amp;gt; Proxier.Sync -&amp;gt; Proxier.syncProxyRules. &amp;#8617; iptables is the default. There is a newer alternative using IPVS that one can use by setting the proxy-mode appropriately (see proxy-mode in options for kube-proxy). There used to be an older third mode called userspace but support for that was removed. &amp;#8617; Breadcrumbs: kubeServicesChain -&amp;gt; iptablesJumpChains -&amp;gt; syncProxyRules). &amp;#8617; " }, { "title": "Plugins I use with kubectl", "url": "/posts/kubectl-plugins/", "categories": "", "tags": "kubernetes, k8s, kubectl", "date": "2022-11-28 15:40:00 -0500", "snippet": "kubectl is the official tool to query and run changes on aKubernetes cluster and provides a powerful and extensible CLI interface. There are many alternative tools out therethat do a similar job (some with GUIs); I deliberately stick with kubectl on my road to master Kubernetes, whichmeans I try not to hide too much of the complexity in the hopes of burning them into my mind. Sometimes though, thereare actions that are far too elaborate or complicated considering the number of times I need to execute them. Other times,I just wish the tool itself offered some slight quality of life improvements to the overall experience.Following are the plugins I personally use all the time to scratch some of my itches with daily use of kubectl. Beforewe go into those, let’s first quickly go over a few productivity hacks:Productivity HacksAliasingConsidering how many times a day I run a command with kubectl, I estimate aliasing this command probably saves mehundreds of keystrokes per day.Alias kubectl to a shorter string that is meaningful to you. I use k and honestly it’s probably the conventionat this point:$ alias k=kubectl$ k get poNAME READY STATUS RESTARTS AGEweb-0 1/1 Running 1 (94s ago) 2d15h...ShortnamesFamiliarize yourself with the short names of the resource definitions recognized by your cluster:# compare this:$ k get replicationcontrollers# to this:$ k get rcNote that not all resources have shortnames. You can use the api-resources command to see the shortnames available for the resource definitions in yourcluster: Example $ k api-resourcesNAME SHORTNAMES APIVERSION NAMESPACED KINDbindings v1 true Bindingcomponentstatuses cs v1 false ComponentStatusconfigmaps cm v1 true ConfigMapendpoints ep v1 true Endpointsevents ev v1 true Eventlimitranges limits v1 true LimitRangenamespaces ns v1 false Namespace... Dry runskubectl create and kubectl run have the options--dry-run that I find very useful for quickly sketching the basis for a resource I want to create when I setthe output to yaml format:$ k create deploy mydeploy --image=nginx --replicas=3 --dry-run=client -o yaml apiVersion: apps/v1kind: Deploymentmetadata: creationTimestamp: null labels: app: mydeploy name: mydeployspec: replicas: 3 selector: matchLabels: app: mydeploy strategy: {} template: metadata: creationTimestamp: null labels: app: mydeploy spec: containers: - image: nginx name: nginx resources: {}status: {}You can pipe that output to a file and make additional tweaks there. Bonus points if instead you patch the output directlywith kubectl patch (advanced).AutocompletionEnable autocompletion for your shell: bash zshPluginskrewPlugins can be “installed” to kubectl simply by placing the binary in your $PATH and adding the kubectl- prefix.It’s as simple as:# &#39;~/bin` is in my $PATH$ printf &#39;#!/bin/bash\\n\\necho $1&#39; &amp;gt; ~/bin/kubectl-echo$ chmod +x ~/bin/kubectl-echo$ k echo &#39;Hello World!&#39;Hello World!If you want to avoid the hassle of placing the executables in the correct path, and/or if you would like a tool thatcan list plugins from a central repository and automatically install those for you, then Krewis for you.Krew is a package manager maintained by the Kubernetes Special Interest Group (SIG) CLI and can do all of this for you:# List all plugins in the repo:$ k krew searchNAME DESCRIPTION INSTALLEDaccess-matrix Show an RBAC access matrix for server resources noaccurate Manage Accurate, a multi-tenancy controller noadvise-policy Suggests PodSecurityPolicies and OPA Policies f... noadvise-psp Suggests PodSecurityPolicies for cluster. no...# Show info for a plugin:$ k krew info advise-policyNAME: advise-policyINDEX: defaultURI: https://github.com/sysdiglabs/kube-policy-advisor/releases/download/v1.0.2/kube-policy-advisor_v1.0.2_linux_amd64.tar.gzSHA256: 2d3968fd80d6fe40976dbc86655ef8fe3e6ea4bcb0c43fafb99a39000daa549fVERSION: v1.0.2HOMEPAGE: https://github.com/sysdiglabs/kube-policy-advisorDESCRIPTION: Suggests PSPs and OPA Policies based on the required capabilities of the currently runningworkloads or a given manifest.# Install a plugin:$ k krew install advise-policyUpdated the local copy of plugin index.Installing plugin: advise-policyInstalled plugin: advise-policy\\ | Use this plugin: | kubectl advise-policy | Documentation: | https://github.com/sysdiglabs/kube-policy-advisor/WARNING: You installed plugin &quot;advise-policy&quot; from the krew-index plugin repository. These plugins are not audited for security by the Krew maintainers. Run them at your own risk. # List installed plugins:$ k krew listadvise-policyctxkrewns...ctxctx is a simple plugin to switch between contexts (clusters) with kubectl. Thisis a great timesaver when you constantly have to switch between different contexts (clusters). You can install itwith k krew install ctx.View configured contexts:The normal way:$ k config get-contextsCURRENT NAME CLUSTER AUTHINFO NAMESPACE* myclusterA myclusterA myclusterA default myclusterB myclusterB myclusterB default myclusterC myclusterC myclusterC defaultUsing ctx:$ k ctxmyclusterAmyclusterBmyclusterCSwitch context:The normal way:$ k config set-context myclusterAContext &quot;myclusterA&quot; modified.Using ctx:$ k ctx myclusterASwitched to context &quot;myclusterA&quot;.nsns is a sibling of ctx with a very similar purpose: it allows youto easily list and change between namespaces. This is another big timesaver. You can install it with k krew install ns.List namespaces:The normal way:$ k get nsNAME STATUS AGEdefault Active 116dgcp-auth Active 34distio-system Active 112distioinaction Active 112dkube-node-lease Active 116dkube-public Active 116dkube-system Active 116dUsing ns:$ k nsdefaultgcp-authistio-systemistioinactionkube-node-leasekube-publickube-systemSwitching namespaces (this is a big one):The normal way:$ k config set-context --current --namespace=kube-publicContext &quot;myclusterA&quot; modified.Using ns:$ k ns kube-publicContext &quot;myclusterA modified.Active namespace is &quot;kube-public&quot;.tailtail is a great plugin for streaming logs from pods. It extends the builtin kubectl logsfunctionality with the ability to figure out the labels of the pods for you when you point it to a Service or a controller.You can install it with k krew install tail.Example:$ k create deploy mydeploy --image=mysql:latestdeployment.apps/mydeploy created$ k tail -d mydeploykube-public/mydeploy-d8c5f59cc-xjdn2[mysql]: 2022-11-28 20:10:32+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.31-1.el8 started.kube-public/mydeploy-d8c5f59cc-xjdn2[mysql]: 2022-11-28 20:10:32+00:00 [Note] [Entrypoint]: Switching to dedicated user &#39;mysql&#39;kube-public/mydeploy-d8c5f59cc-xjdn2[mysql]: 2022-11-28 20:10:32+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.31-1.el8 started....Another awesome feature is that it does not require RUNNING pods - it just keeps listening for any pods with thematching criteria. This is unlike kubectl logs -f which fails if there are no matching RUNNING pods. This isgreat when you don’t want to waste time waiting for the deployment to be live to finally execute a command thattails the logs.deprecationsdeprecations lists all standard K8S resources found in your cluster that aredeprecated (as found in Kubernetes’OpenAPI spec).This is a handy tool that helps you keep your cluster and resources up-to-date. You can install it with k krew install deprecations.$ k deprecationsW1128 15:24:46.214313 450713 warnings.go:70] v1 ComponentStatus is deprecated in v1.19+RESULTS:Deprecated APIs:ComponentStatus found in /v1 ├─ ComponentStatus (and ComponentStatusList) holds the cluster validation info. Deprecated: This API is deprecated in v1.19+ -&amp;gt; GLOBAL: scheduler -&amp;gt; GLOBAL: controller-manager -&amp;gt; GLOBAL: etcd-0 Deleted APIs:safeDo you sometimes feel your spidey sense tingling and your palms sweating a little bit as you constantly switch betweencontexts (k ctx) and namespaces (k ns) while you query the cluster and modify resources? Have you ever thought thatmaybe you’re being a bit too cavalier with your k8s-foo and should probably slow down and double-check the command you’reabout to run before you hit Enter? safe](https://github.com/rumstead/kubectl-safe) is here to assuage your fears.You can install it with k krew install safe.# Alias to ensure you&#39;re always SAFE:$ alias k=kubectl safe# Queries work as usual:$ k get po# Changes must be acknowledged however:$ k create deploy mydeploy --image=nginxYou are running a create against context myclusterA, continue? [yY]kubecolorkubecolor replaces kubectl and colorizes the output1. Since it’s areplacement binary, you cannot install it with krew - you eitherdownload a prebuilt binary or build it yourself.Examples:Footnotes Make sure you install from this fork/branch to fix compatibility issues with ctx and ns. &amp;#8617; " }, { "title": "Kubernetes In Action", "url": "/posts/kubernetes-in-action/", "categories": "books, kubernetes", "tags": "kubernetes, k8s, devops, cloud, containers, docker, book", "date": "2022-11-26 09:00:00 -0500", "snippet": "Written by Marko Lukša,Kubernetes In Action is a fantastic book covering all operationalaspects of Kubernetes. I find it very hard to think of a better book on the subject. This is the first editionof the book, published in December 2017, and although dated around the edges and details, Marko’s in-depth dive intothe different components that make up Kubernetes and how they work is timeless. I highly recommend this book to anyonelooking for any serious learning of Kubernetes. This book’s shelf life is pretty long despite Kubernetes’ activedevelopment - I would think it can only be supplanted by thesecond edition coming out early next year1.Kubernetes In Action provided me with solid a theoretical and practical foundation on Kubernetes, enabling me to earnthe Certified Kubernetes Application Developer badge.Check out some other books I’ve read on the bookshelf.SummaryKubernetes In Action’s roadmap takes us on a journey with the end goal of developing Kubia - a contrived sample application - while exploringthe core concepts of Kubernetes in depth. Beyond the basics, some of the things this book explains are Kubernetes’ architecture,how pods communicate with each other, how to secure your K8S cluster, pod affinity and anti-affinity, tolerations, the API service,and how to extend Kubernetes with custom resources. The reader is kept engaged with practical exercises throughout byapplying configurations and testing them. These configurations and extra resources can be found in the book’sGitHub repository.Tools and runtimesKubernetes comes in several flavors. “Vanilla” Kubernetes can be installed usingkubeadm.Other flavors, such as minikube, make it very easy to install a localKubernetes cluster for development purposes.The primary way to interact with a Kubernetes cluster is withkubectl23. You can install it directly using the officialinstructions, but other installation means are available, such as withgcloud components install kubectl ifGKE is your provider.Normally kubectl is automatically configured by your Kubernetes provisioner with the necessary configuration to interactwith the cluster. For example, here is what my configuration looks like after runningminikube start4: Hands On Run kubectl config view to view your local configuration: Example $ kubectl config viewapiVersion: v1clusters:- cluster: certificate-authority: /home/llorllale/.minikube/ca.crt extensions: - extension: last-update: Wed, 24 Aug 2022 21:33:37 EDT provider: minikube.sigs.k8s.io version: v1.25.2 name: cluster_info server: https://192.168.49.2:8443 name: minikubecontexts:- context: cluster: minikube extensions: - extension: last-update: Wed, 24 Aug 2022 21:33:37 EDT provider: minikube.sigs.k8s.io version: v1.25.2 name: context_info namespace: default user: minikube name: minikubecurrent-context: minikubekind: Configpreferences: {}users:- name: minikube user: client-certificate: /home/llorllale/.minikube/profiles/minikube/client.crt client-key: /home/llorllale/.minikube/profiles/minikube/client.key kubectl supports drop-in plugins since v1.12. The community has provided many plugins, some of which I find immensely useful. I’ll write about these in a future article.System componentsFrom Chapter 1, section 1.3.3 Understanding the architecture of a Kubernetes clusterThere are two sets of components:Control Plane componentsThese components are in charge of monitoring and responding to events in the cluster. The API server (kube-apiserver)exposes a REST API and is what kubectl interacts with then you execute its commands. The other componentsalso discover the state of the cluster via the API server. The Controller Manager (kube-controller-manager)is the control loop that reconciles the cluster’s actual state with the desired state. The Scheduler (kube-scheduler)assigns Pods unto nodes for them to run. There are many reasons why a pod may not be scheduled unto nodes and someof those reasons can have side effects, such as an automatic scale up of the cluster’s node pool. You will probablyspend a lot of time figuring out the scheduler and looking at Pod event logs at some point or another. etcd is the distributed key-value store used by most Kubernetes clusters.Node componentsThese are components that run in each node and are used to realize the configurations sent out by the components in thecontrol plane. kubelet runs containers specifiedin Pod specs and monitors their health. kube-proxy is a network proxythat implements part of the Kubernetes Service concept. It takes the network rules configured by the control plane componentsand applies them locally to the node’s IP routing rules. It has different modes of operation; there is a nice explanationhere about its modes. Container Runtime are what run the containers (eg. docker, containerd).Kubernetes ResourcesKubernetes is a massive beast. Here are (almost) all the resources I am aware of as a developer5:Arrows indicate references to the target component.Kubernetes In Action covers all of these objects and more. I am only going to gloss over a handful of the mostimportant ones.NamespaceDon’t let its distance and disconnection from other nodes in the diagram above mislead you:namespace is one of the most fundamentalconcepts in Kubernetes, as it lets developers and administrators separate different resources into logical groups.For example, environments such as dev, staging, and prod, can reside in different namespaces within the same K8Scluster. Another popular use of namespaces is to group resources belonging to applications with cross-cutting concerns.Adding a connection to Namespace from every resource that references it would make the diagram unwieldy!Beyond grouping user resources into logical units, it is important to understand that some built-in resources arescoped to the namespace they are declared in and others operate across the whole cluster. For those that are namespaced,the default namespace is the default if none is specified. Hands On Declaratively set namespace with metadata.namespace: Example apiVersion: v1kind: Podmetadata: name: myapp namespace: mynamespacespec: containers: - name: myapp image: nginx Set namespace with kubectl: Example $ kubectl run -n mynamespace myapp --image nginx --dry-run=client -o yamlapiVersion: v1kind: Podmetadata: creationTimestamp: null labels: run: myapp name: myapp namespace: mynamespacespec: containers: - image: nginx name: myapp resources: {} dnsPolicy: ClusterFirst restartPolicy: Alwaysstatus: {} View all namespaces for a given cluster/context: Example $ kubectl get nsNAME STATUS AGEdefault Active 63distio-system Active 59dkube-node-lease Active 63dkube-public Active 63dkube-system Active 63d List all resources and see which are namespaced and which aren’t: Example $ kubectl api-resourcesNAME SHORTNAMES APIVERSION NAMESPACED KINDbindings v1 true Bindingcomponentstatuses cs v1 false ComponentStatusconfigmaps cm v1 true ConfigMapendpoints ep v1 true Endpointsevents ev v1 true Eventlimitranges limits v1 true LimitRangenamespaces ns v1 false Namespacenodes no v1 false Nodepersistentvolumeclaims pvc v1 true PersistentVolumeClaimpersistentvolumes pv v1 false PersistentVolume... Pod Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.Pods are composed of one or more containers with shared storage and network resources. – Kubernetes/PodsContainers are instances of pre-packaged images (or “snapshots”) of executable software that can be run on any platform,including Kubernetes. Pods are what run your application.The Pod resource is centered in the diagram above because it is the workhorseof a Kubernetes application deployment. We’ll explore most of those other objects in later sections. All roads lead to Pods. – me Hands On Use kubectl run to create and run Pods imperatively: Example # This pod defines a single container named _nginx_ with container image also _nginx_.# The pod&#39;s name also happens to be _nginx_.$ kubectl run nginx --image=nginx --dry-run=client -o yamlapiVersion: v1kind: Podmetadata: labels: run: nginx name: nginxspec: containers: - image: nginx name: nginx dnsPolicy: ClusterFirst restartPolicy: Always Pods are composed of one or more containers; these containers can be divided into three types: containers: these are your regular containers that run your application workload. initContainers: similar to regular containersexcept that kubelet runs them firstbefore regular containers. All initContainers must run successfully for regular containers to be started. Their use caseis obvious: use them to execute utilities or setup scripts not present in the regular container. Sadly, kubectl rundoes not support specifying initContainers, so we have to add them to the Pod’s spec manually. ephemeralContainers:these containers are added to the pod at runtime when debugging a Pod. They are not part of the Pod’s original manifest. Hands On kubectl run cannot add initContainers to a Pod. You must add them yourself: Example pod with initContainers apiVersion: v1kind: Podmetadata: name: my-pod labels: app: nginxspec: initContainers: - name: init image: busybox command: [&quot;echo&quot;, &quot;&amp;lt;DOCTYPE !html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Hello, World!&amp;lt;/h1&amp;lt;/body&amp;gt;&quot;, &quot;&amp;gt;&quot;, &quot;/usr/share/nginx/html/index.html&quot;] volumeMounts: - name: content mountPath: /usr/share/nginx/html containers: - name: app image: nginx ports: - containerPort: 80 volumeMounts: - name: content mountPath: /usr/share/nginx/html volumes: - name: content emptyDir: {} PersistentVolumeClaims and PersistentVolumesA PersistentVolume provisions storage for use by Pods. They can be created either statically or dynamically.A PersistentVolumeClaim is a request for a PersistentVolume. A PVC specifies the amount of storage requested and, ifa suitable PV is found then the PVC is bound to the PV, otherwise a new PV may be provisioned, depending on the PVC’sStorageClass. A Pod (or a PodTemplate) can reference a PVC and mount it in one or more of its containers.We’ll cover PVs and PVCs in depth in a later article.DeploymentDeployments manage the state of a set of one ormore pods. This is important for a number of use cases, such as scaling the number of pods or updating the applicationworkload’s version. You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desiredstate at a controlled rate. – Kubernetes/DeploymentUnder the hood a deployment uses a ReplicaSetto manage a set of pods for a given state. The latter deprecates and replaced the old ReplicationController. A ReplicaSet’s purpose is to maintain a stable set of replica Pods running at any given time. As such, it is oftenused to guarantee the availability of a specified number of identical Pods. – Kubernetes/ReplicaSet Hands On Create a deployment with kubectl create deploy: Example $ kubectl create deploy mydeploy --image=nginx --replicas=2 --dry-run=client -o yamlapiVersion: apps/v1kind: Deploymentmetadata: labels: app: mydeploy name: mydeployspec: replicas: 2 selector: matchLabels: app: mydeploytemplate: metadata: labels: app: mydeploy spec: containers: - image: nginx name: nginx StatefulSetStatefulSets are very similar to Deploymentswith a big distinction: each pod managed by a StatefulSet has a unique persistent identity which you can match to specificstorage volumes (these are described further down). This makes the StatefulSet particularly useful for distributed applicationssuch as CouchDB, Redis, Hyperledger Fabric, and many others.Note: a Headless Service is required for StatefulSets. Note: kubectl does not have a command to create statefulsets. Hands On Here’s a simple example from the Kubernetes docs: Example apiVersion: apps/v1kind: StatefulSetmetadata: name: webspec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: &quot;nginx&quot; replicas: 3 # by default is 1 minReadySeconds: 10 # by default is 0 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: registry.k8s.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ &quot;ReadWriteOnce&quot; ] storageClassName: &quot;my-storage-class&quot; resources: requests: storage: 1Gi Deploying the above in my local minikube cluster using kubectl apply -f &amp;lt;filename&amp;gt; we can see: Example $ kubectl get statefulset webNAME READY AGEweb 3/3 80s $ kubectl get poNAME READY STATUS RESTARTS AGEweb-0 1/1 Running 0 2m53sweb-1 1/1 Running 0 2m33sweb-2 1/1 Running 0 2m13s If you describe the pods you’ll realize each has an associated PersistentVolumeClaim: Example $ kubectl describe po web-2Name: web-2Namespace: defaultPriority: 0Service Account: defaultNode: minikube/192.168.49.2...Volumes: www: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: www-web-2 ReadOnly: false... And when describing that PVC you’ll see it’s associated with a unique PersistentVolume: Example $ kubectl describe pvc www-web-2Name: www-web-2Namespace: default...Volume: pvc-33f94fdb-9e17-4ee6-a2e7-0b10d88699e3...Used By: web-2... Scaling the statefulset down does not delete the PVs: Example $ kubectl scale statefulset web --replicas 1statefulset.apps/web scaled$ kubectl get poNAME READY STATUS RESTARTS AGEweb-0 1/1 Running 0 91m$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEwww-web-0 Bound pvc-bb53c1eb-de59-477f-b82e-606cdfe234ba 1Mi RWO standard 91mwww-web-1 Bound pvc-e8ea7850-58da-460f-9fb2-315627708cc3 1Mi RWO standard 91mwww-web-2 Bound pvc-33f94fdb-9e17-4ee6-a2e7-0b10d88699e3 1Mi RWO standard 91m$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-33f94fdb-9e17-4ee6-a2e7-0b10d88699e3 1Gi RWO Delete Bound default/www-web-2 standard 91mpvc-bb53c1eb-de59-477f-b82e-606cdfe234ba 1Gi RWO Delete Bound default/www-web-0 standard 92mpvc-e8ea7850-58da-460f-9fb2-315627708cc3 1Gi RWO Delete Bound default/www-web-1 standard 91m Scaling the statefulset back up does not create new PVCs; the existing PVCs are assigned to the pods in order: Example $ kubectl scale statefuleset web --replicas 3statefulset.apps/web scaled $ kubectl get poNAME READY STATUS RESTARTS AGEweb-0 1/1 Running 0 96mweb-1 1/1 Running 0 47sweb-2 1/1 Running 0 27s$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEwww-web-0 Bound pvc-bb53c1eb-de59-477f-b82e-606cdfe234ba 1Mi RWO standard 97mwww-web-1 Bound pvc-e8ea7850-58da-460f-9fb2-315627708cc3 1Mi RWO standard 97mwww-web-2 Bound pvc-33f94fdb-9e17-4ee6-a2e7-0b10d88699e3 1Mi RWO standard 96m HorizontalPodAutoscaler Horizontal scaling means that the response to increased load is to deploy more Pods. This is different from verticalscaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods that arealready running for the workload. – Kubernetes/HorizontalPodAutoscaler WalkthroughHPAs will update the .spec.replicas of Deployments and StatefulSets using metrics collected from theMetrics Server,as input. The Metrics Server is the default source of container metrics for autoscaling pipelines. Note: Kubernetes does not provide vertical pod autoscalers out of the box6. You can install theautoscaler developed within the Kubernetesproject umbrella, or you may use your K8S provider’s offering, such asGKE’s Vertical Pod autoscaling.A vertical pod autoscaler will automatically update the Pod’s resourcerequests and limits.ServiceExposing services in Kubernetes both within and without would be more cumbersome if not forService objects. Pods are not permanent resources;Services fill in the gap by providing a stable DNS name for a set of Pods. The target pods are selected by matchinglabels.There are four types of services: ClusterIP (default): the service will only be reachable within the cluster. NodePort: allocates a port number on every node (.spec.ports[*].nodePort) and forwards incoming traffic to the portexposed by the service (.spec.ports[*].port). NodePort services LoadBalancer: exposes the service to traffic originating from outside the cluster. The machinery used to do thisdepends on the platform. ExternalName: these map DNS names from within the cluster to external names. In other words, the cluster’s DNSservice will return CNAME records instead of A records for queries targeting the service’s name.There is a special kind of Service called aHeadless Service that does notperform load-balancing and does not provide a single address for the backing Pods. Instead, it serves to list IP addressesof all the Pods it selects. This type of Service are required for StatefulSets. Hands On Use kubectl create svc to create services imperatively: Example # Create a service of type `ClusterIP`$ kubectl create svc clusterip mysvc --tcp=8080:7001 --dry-run=client -o yamlapiVersion: v1kind: Servicemetadata: labels: app: mysvc name: mysvcspec: ports: - name: 8080-7001 port: 8080 protocol: TCP targetPort: 7001 selector: app: mysvc type: ClusterIP The problem with kubectl create svc is that you can’t specify a selector.Services without selectorshave their uses, but you are more likely to want to point your service to a set of pods in your cluster.For this use case you can either write the spec manually or use kubectl expose. Example # Expose a deployment named `webapp`. Note the `selector` automatically added:$ kubectl expose deploy webapp --type ClusterIP --name mysvc --port 8080 --dry-run=client -o yamlapiVersion: v1kind: Servicemetadata: labels: app: webappname: mysvcspec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: webapp type: ClusterIP Ingress Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing iscontrolled by rules defined on the Ingress resource. – Kubernetes/What is Ingress?An Ingress is an L7 proxy tailored for HTTP(S) services, allowing request routing based on simple rules such aspath prefixes7. Note that if you want to expose your services outside your cluster with something other than HTTP, you’dhave to use Services of type NodePort or LoadBalancer. Hands On Use kubectl create ing to create an Ingress imperatively: Example # Create an Ingress that directs incoming traffic on `www.example.com` to a backend service `webapp` on port 8080:$ kubectl create ing myingress --rule=&quot;www.example.com/webapp*=webapp:8080&quot; --dry-run=client -o yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: myingressspec: rules: - host: www.example.com http: paths: - backend: service: name: webapp port: number: 8080 path: /webapp pathType: Prefix Footnotes It appears you can use code au35luk to get a 35% discount . &amp;#8617; Fondly pronounced by many as “cube cuddle”. &amp;#8617; Other ways include a console offered by your cloud provider in cases where Kubernetes is available as a service. &amp;#8617; We will explore the configuration in depth in a future article - stay tuned. &amp;#8617; As a developer I am not including objects that I’m not likely to encounter in may day-to-day, such as TokenReview or EndpointSlice. These will typically be objects configured by an administrator role, or perhaps are objects managed by the underlying K8S provider, such as GKE. &amp;#8617; I’ve never used a VPA. That said, they might help size your nodes adequately, or have you consider making your pods more efficient. &amp;#8617; For more advanced routing you should probably make use of a service mesh such as Istio (see VirtualService), or you can explore the Gateway API that recently graduated to beta status! &amp;#8617; " }, { "title": "Golang Guild Session: Deadlocks (and how to break out of them)", "url": "/posts/golang-deadlocks/", "categories": "talks, golang", "tags": "go, golang, goroutine, concurrency, deadlock", "date": "2022-09-21 15:00:00 -0400", "snippet": "Slide deck for my presentation on Deadlocks in Go for the Golang Guild Session @ VerticalScope.This talk extends my previous slides on Golang Concurrency Patterns. Your browser does not support iframes.Click on the slide deck then press F to expand to full screen.Topics What deadlocks are When deadlocks typically happen How Go&#39;s runtime detects deadlocks Go&#39;s stacktraces ToolsReferences The Go Memory Model Scalable Go Scheduler Design Doc" }, { "title": "Golang Guild Session: Concurrency Patterns", "url": "/posts/golang-concurrency-patterns/", "categories": "talks, golang", "tags": "go, golang, goroutine, mutexes, channels, concurrency, pattern", "date": "2022-06-29 15:30:00 -0400", "snippet": "Slide deck for my presentation on Golang Concurrency Patterns for the Golang Guild Session @ VerticalScope.This talk builds upon my previous slides on Golang Concurrency Primitives. Your browser does not support iframes.Click on the slide deck then press F to expand to full screen.Topics The Done channel pattern The Fan-In pattern The Fan-Out pattern Sharding Bounded Parallelism Backpressure Deadlocks (stay tuned)References Learning Go: An Idiomatic Approach to Real-World Go Programming Cloud-Native Go: Building Reliable Services in Unreliable Environments The Go Blog: Go Concurrency Patterns: Pipelines and cancellation" }, { "title": "Certified Kubernetes Application Developer: My Experience", "url": "/posts/ckad/", "categories": "", "tags": "k8s, kubernetes, cloud, containers, certification", "date": "2022-05-08 20:00:00 -0400", "snippet": "I recently passed the Certified Kubernetes Application Developerexam1 and thought I’d share my experience leading up to and during the exam.Curriculum This is the newest exam curriculum, effective since September 28, 2021 (see blog post ).There are many study guides and practice exams out there - most haven’t been updated to account for these changes.The exam’s curriculum are listed in the Domains &amp;amp; Competencies section in the link above. As of April 2022 the topicsare the following: Click to expand Application Design and Build Define, build and modify container images Understand Jobs and CronJobs Understand multi-container Pod design patterns (e.g. sidecar, init and others) Utilize persistent and ephemeral volumes Application Deployment Use Kubernetes primitives to implement common deployment strategies (e.g. blue/green or canary) Understand Deployments and how to perform rolling updates Use the Helm package manager to deploy existing packages Application Observability and Maintenance Understand API deprecations Implement probes and health checks Use provided tools to monitor Kubernetes applications Utilize container logs Debugging in Kubernetes Application Environment, Configuration and Security Discover and use resources that extend Kubernetes (CRD) Understand authentication, authorization and admission control Understanding and defining resource requirements, limits and quotas Understand ConfigMaps Create &amp;amp; consume Secrets Understand ServiceAccounts Understand SecurityContexts Service and Networking Demonstrate basic understanding of NetworkPolicies Provide and troubleshoot access to applications via services Use Ingress rules to expose applications A notable exclusion from the curriculum are StatefulSets.StatefulSets are an important primitive when deploying stateful, distributed workloads, such as databases or blockchains.A notable inclusion with the recent update is Ingress.I cannot fathom why Ingress was not included before.Another notable inclusion with the recent update is Helm.I would have expected Kustomizeinstead since it is built into kubectl2.Books Kubernetes’ development is very active and therefore the tooling, API, and resources are still (2022) a moving target.Books tend to go out of date fast, either in whole or in part.I will be adding these two books to my bookshelf in the next couple of articles.Kubernetes in ActionAuthor: Marko Luksa Date of publication: Jan 2018 No. of pages: 559The quintessential book on Kubernetes; this is the gold standard. Though significantly outdated3, this bookcovers everything there is to know about k8s. I expect this book’s solid foundation to power a candidate through the CKA and CKSexams as well since it covers topics far and beyond what is required for the CKAD. Read my brief summary of the book in Kubernetes In Action.Certified Kubernetes Application Developer (CKAD) Study GuideAuthor: Benjamin Muschko Date of publication: Feb 2021 No. of pages: 165This book is a highly focused and condensed walk-through of the topics required to pass the exam. As such, it doesn’t gointo much depth on any of the topics. You can think of this book as a collection of summaries of the exam’s curriculum withpointers to external in-depth documentation. It also includes practice questions at the end of every chapter, which is verynice.Courses / Practice Questions / Mock Exams Despite the curriculum including Ingress resources, none of these resources included practice questions on them forsome reason.KodeKloud: Certified Kubernetes Application Developer (CKAD):One of the more popular courses out there. An excellent resource that I highly recommend. Plus, all their other courses areincluded in the price. Please be aware that their Game Of Pods lab is incompleteas of May 2022, which is a shame since it provides a decent challenge. KodeKloud is also available on Slack: kodekloud.slack.com.dgkanatsios/CKAD-exercises:A commonly cited resource for practice.OReilly’s KataKoda:Seemed pretty cool at first but then I realized: has not been updated with the curriculum’s latest changes does not work on Firefox env really slow env buggy: blocked from completing one section due to an error4 one question marked as incorrect even though my solution and their solution was identical (Services&amp;amp;Networking/question 2): exposing a deployment on an arbitrary node port one solution doesn’t work (Services&amp;amp;Networking/question 3) specifically, the verification step curl localhost:31888 is incorrect as curl needs to be pointed to the node’s internal IP. Hence the question is marked as incorrect when in fact it works as designed. note: I had to map the node’s IP to localhost in /etc/hosts!! This was not part of the solution. The manual test with curl still didn’t work though. killer.sh:Likely the best exam emulator out there. At least it should be considering the CKAD exam fee includes two free sessions,making this pretty much the “official” exam simulator. There were strange omissions though - none of the following topicswere included in the mock questions: pod affinity, CRDs, ResourceQuotas, HPAs.Article on Medium: Kubernetes CKAD Exam Example Questions Practical Challenge Series:Written by Kim Wuestkamp (killercoda.com, killer.sh), this is a great set of challenges to testyour knowledge on Kubernetes resources and the kubectl CLI. A warning though: it hasn’t been updated with the latest curriculum.Article on Medium: 150 Practice Questions for the CKAD Exam:Somewhat of a hidden gem; this article turned out to be pretty good practice given the sheer number of questions and the differenttwists that each introduces. The only slight problems are a) the solutions aren’t tucked away or hidden somewhere, and b)it hasn’t been updated with the latest curriculum (it was published in Nov 2019).Before the ExamThe CKAD exam is a proctored exam. I took it online. One very annoying thing was the security check the proctor goes through.You’ve probably read about it elsewhere: you are required to keep your desk clean and tidy, you will have to pan your webcam across your desk and around your room.What I found annoying was the excessive panning I had to do around the room - I must have literally spent 5-10 minutesjust spinning the webcam around the room.Another annoying factor was during ID verification: the proctor asked me to bring my ID close to the camera to focus on myname details. The problem was my webcam cannot focus at that length. We lost at least 10 minutes while we attempted thiswith both of my IDs.Overall the proctor’s checklist took around 20 minutes to complete; I was worried it was eating into the time allotted forthe exam and I asked the proctor about it. Thankfully, the 2-hour timer starts after this ritual.Tips for the ExamA few tips and tricks that can help you optimize your time during the exam; I was able to do a first pass with dozens of minutes to spare: know your concepts (duh) breathe kubectl mind the question’s weighted score; flag it and move on if you cannot resolve it “fast enough”. I would say “fast enough” &amp;lt;= 60 seconds. many suggest you collect and optimize your bookmarks to k8s documentation and others. I propose you don’t waste muchtime on that and instead get very comfortable with kubectl explain [--recursive] &amp;lt;topic&amp;gt;. Not only is it fast and tailoredto the environment’s actual k8s version, but also there is no chance of you opening a link you shouldn’t be. This commandshould definitely suffice if you are comfortable with the exam’s concepts. like everyone says, make sure you execute the kubectl context stanza provided for you at the top of each question without fail. like everyone says, avoid writing specifications by hand and instead rely on --dry-run=client -o yaml whenever you can. some people suggest defining variables that stand in for things like --dry-run=client -o yaml. I am decent with the keyboard;this was unnecessary for me. alias k=kubectl, although like the prior point: if you are good with the keyboard then this may not be necessary. If I recall correctly, there are several text editors available and documented. Make sure you are extremely familiarwith at least one of those. vim is the only one I care about.Notes about my Exam Somewhere I read I should expect 20-22 questions. My exam had just 16, so each was worth more. No questions on Ingress resources. Some questions were of familiar topics covered by several of the resources listed above, but formulated in a differentand confusing way. I was interrupted a handful of times by the proctor, who asked me to raise my hands and forearms to the camera. This wasannoying.Footnotes Verify my credential . &amp;#8617; Of course, kustomize and helm are not equivalent. helm has a richer template syntax, and also acts as a package manager (called “helm charts”). &amp;#8617; Second edition is in the works and is to be published in the fall of 2022: Kubernetes in Action, Second Edition . &amp;#8617; Error: Warning FailedCreatePodSandBox 10s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container &quot;a0c7e8b0ba7e7006e70dee1e648949f35682e680827775f8a00c55f8237b11a2&quot; network for pod &quot;myredis&quot;: networkPlugin cni failed to set up pod &quot;myredis_default&quot; network: unable to allocate IP address: Post &quot;http://127.0.0.1:6784/ip/a0c7e8b0ba7e7006e70dee1e648949f35682e680827775f8a00c55f8237b11a2&quot;: dial tcp 127.0.0.1:6784: connect: connection refused, failed to clean up sandbox container &quot;a0c7e8b0ba7e7006e70dee1e648949f35682e680827775f8a00c55f8237b11a2&quot; network for pod &quot;myredis&quot;: networkPlugin cni failed to teardown pod &quot;myredis_default&quot; network: Delete &quot;http://127.0.0.1:6784/ip/a0c7e8b0ba7e7006e70dee1e648949f35682e680827775f8a00c55f8237b11a2&quot;: dial tcp 127.0.0.1:6784: connect: connection refused] &amp;#8617; " }, { "title": "Golang Guild Session: Concurrency Primitives", "url": "/posts/golang-concurrency-primitives/", "categories": "talks, golang", "tags": "go, golang, scheduler, goroutine, mutexes, channels, concurrency", "date": "2022-05-04 10:00:00 -0400", "snippet": "Slide deck for my presentation on Golang Concurrency Primitives for the Golang Guild Session @ VerticalScope. Your browser does not support iframes.Click on the slide deck then press F to expand to full screen.Topics goroutines channels sync.WaitGroup sync.Once mutexes extra: sync/atomic sync.Map errgroup.Group References The Go Programming Language Specification Learning Go: An Idiomatic Approach to Real-World Go Programming YouTube: The Scheduler Saga" }, { "title": "Test-Driven Development By Example", "url": "/posts/test-driven-development-by-example/", "categories": "books, testing", "tags": "programming, testing, unit-test, kent-beck, tdd, extreme-programming, xp, book", "date": "2022-04-16 10:30:00 -0400", "snippet": "Written by Kent Beck,Test-Driven Development By Exampleexplains how TDD works by way of 2 examples: first building pieceby piece a toy Money class, then the initial scaffolds of xUnit. The third and finalpart of the book walks the reader through several TDD patterns as well as refactoring patterns.Kent Beck built SUnit for Smalltalk, and ported it over to Java as JUnit.Kent developed Extreme Programming (XP) of which TDD is a central component.Being driven by examples and principles derived from experience, Test-Driven Development By Example needs to be understoodwithin the context of Extreme Programming1.Following are a brief summary and some notes taken from interesting sections of the book.Check out some other books I’ve read on the bookshelf.SummaryTest-Driven Development By Example defines the TDD methodology and demonstrates its use by working through two examples: A Money class written in Java that requires a unified API to handle simple arithmetic between amounts expressed in different currencies. The beginnings of a test suite framework (xUnit) written in Python.Several tips and techniques are provided for writing tests. A brief walkthroughof some simple design patterns is found as well (see chapter 30).In Chapter 31 Refactoring, Kent makes the effort of codifying into words the different kind of things software engineersdo when refactoring code every day: Reconcile differences Isolate change Migrate Data Extract Method Extract Interface Inline Method Move Method Method Object Add Parameter Method Parameter to Constructor ParameterThe amount of verbiage the book invests in the refactoring tips above might sound somewhat scary and daunting, but itreally is nothing more than the bread and butter of every software engineer’s day-to-day.The MethodThe Test-Driven Development method. Write one test. Always work on one test at a time. Append to a TODO list other test scenarios that you may think of while you workon the current one. How or when to write a test? See tips and patterns. Make the test pass by either: Obvious Implementation: if the implementation is obvious and will take you a short time (seconds? minutes?), thenjust type that in. Faking It: if the implementation is not obvious or will take you a long time, return a constant that makes the test pass Refactor: Primarily by removing duplication. However, your own judgement and knowledge can drive refactors as well (Kent at one point refactored Money to makeit immutable, something that was not strictly required. (Optional) Added by me: if you relied on hard-coded constants in your implementation to keep the bar GREEN, make sureyou change the values of any constants in your tests in order to snuff out constants you may have accidentally forgotten about in the implementation. Repeat these steps until all tests (see step 1.1) are implemented and are GREEN.Testing TipsThese are covered in Chapter 26 Red Bar Patterns and are about when you write tests, where you write tests, and when youstop writing tests.Write Explanatory TestsParticularly useful when reporting bugs. Conversely…Write Regression TestsFirst thing to do when a bug is reported: write a regression test that will be GREEN once the bug is fixed.Write “Learning Tests”Learn how 3rd party code works by writing quick demonstrations in the form of tests. I also use this technique whenlearning how std lib components work.Test TechniquesChild TestIf the test case is turning out to be too big, write a smaller test case for a portion of the bigger test case. This couldinfluence the design by breaking the desired implementation into several methods or objects as opposed to one monolithicfunction.Mock ObjectIf your object-under-test relies on an expensive or complicated resource then create a fake version of the resource withfixed responses:My notes: can influence design by abstracting the complicated dependency out into an interface should add caveats for when unit tests do not provide value, such as with non-trivial SQL queries see www.mockobjects.com Kent already considers databases (p. 144). Didn’t mention in-memory or lite production-like databases like: H2 (Java) sqlite mattn/go-sqlite3 (Go, requires Cgo for installation) sqlite JDBC driver (Java) go-mysql-server Have run into DATA RACE errors, see dolthub/go-mysql-server#562 Self ShuntTest that an object communicates correctly with another by having the test object communicate with the test case insteadof an instance of that object.My notes: Should we use this in place of Mock Object? Seems to only be useful when implementing observer/listener pattern?Log StringTest for the correct sequence of “messages” (aka. method invocations) by logging them to a string and compare the expectation.For some reason this one seems a little too prescriptive for me.Crash Test DummySometimes you have a test case for critical edge conditions that would be hard or unreliably reproduced in practice,like testing code that handles a full filesystem error. In such an example, instead of making your test fixture fillup your filesystem (bad idea) to replicate the error condition, have the fake implementation of your filesystem abstractionthrow a suitable error that your application must handle.Green Bar PatternsDifferent approaches to getting your tests GREEN. This is probably one of the most contentious parts of TDD.Fake It (‘Til You Make It)First implementation for a broken test should just return a constant that matches the test’s expectations. Seems highlyimportant for some people to keep the bar green no matter how fake it is. Kent even qualifies such acts as “sins”, althoughpresumably justified.I highly recommend others do the following if they’re faking behaviour in TDD: once you think the test case and implementationare “done”, alter the test’s expected values slightly to sniff out forgotten constants in your implementation. This is especiallycrucial if the test asserts the object-under-test sends correct messages to other objects.Triangulate Abstract only when you have two or more examples.This is one of the trickier aspects of TDD, mostly due to proponents strongly pushing for faking it.If you have a function Add that takes two int args and must return their sum, what are the proper TDD steps that takeyou from A to Z?Step A:func TestAdd(t *testing.T) { expected := 4 actual := Add(2, 2) assert.Equal(t, expected, actual)}func Add(a, b int) int { return 4}What would be the next step? In these scenarios I would personally opt for randomizing the values of a and b and assertingAdd returns their sum, but some people argue that unit tests should be entirely deterministic, presumably to avoid flakytests. It’s difficult for me to see how randomizing the inputs in TestAdd would make the test flaky but anyway.Kent recommends adding another sample set of inputs to the test and triangulate the real implementation from those.Step Z:func TestAdd(t *testing.T) { expected := 4 actual := Add(2, 2) assert.Equal(t, expected, actual) expected = 7 actual = Add(3, 4) assert.Equal(t, expected, actual)}func Add(a, b int) int { return a + b}The problem I have with this is how to do we mechanically reach step Z and not fall astray? Couldn’t the implementationhave theoretically been entirely faked?func Add(a, _ int) int { values := map[int]int { 2: 4, 3: 7, } return values[a]}Eventually I came to realise that during the refactor phase (recall the method) our main goal (accordingto TDD) is to remove duplication: duplication within the implementation itself, and duplication between the implementationand the tests. In the example above, 2, 3, 4, and 7 are duplicated between the implementation and the test code.We could write constants for each of those values, but then we would just be duplicating the names of the constants inseveral places. It is then that I realised that return a + b leads to the least duplication overall in the codebase.Having gotten there, what’s to stop us from deleting one of the tests? And now we’re back at step A.… and that is a level of nuance absent in most conversations about TDD that I’ve seen.Obvious ImplementationThe answer to the conundrum above: if the implementation is sufficiently obvious to you (eg. Add) then just implementit. There is no need for fake implementations or runarounds with triangulation.… or so they say. Things get tricky while pair-programming: is the implementation also obvious to your pairing partner?How large is the gap between skill and experience levels?One to Many How do you implement an operation that works with collections of objects? Implement it without collections first, thenmake it work with collections.A little too prescriptive if you ask me.QuestionsQuestions selected from chapter 32 Mastering TDD.How large should my steps be?Chapter 32 Mastering TDD provides some guidance. The book stops short of making strong claims, but the general recommendationseems to be to go small. Ultimately the size of the steps is up to you and your tolerance level. You get hints of thisflexibility in the choice between Obvious Implementation and Fake It in The Method above. When I use TDD in practice, I commonly shift between these two modes […]. When everything is going smoothly and Iknow what to type, I put in Obvious Implementation after Obvious Implementation (running the tests each time to ensurethat what’s obvious to me is still obvious to the computer). As soon as I get an unexpected red bar, I back up,shift to faking implementations, and refactor to the right code. When my confidence returns, I go back to ObviousImplementation. Chapter 2: Degenerate Objects.What don’t you have to test?The gist is: Write tests until fear is transformed into boredom.Simple list: Conditionals Loops Operations PolymorphismHow do you know if you have good tests?Attributes of tests that suggest a design is in trouble: Long setup code Setup duplication Long running tests Fragile tests Note: different from flaky tests. These are tests with hidden or unexpected coupling to other parts of the system. How does TDD lead to frameworks?TDD focuses on the realities of today, discarding everything else (YAGNI). Code for tomorrow, design for today.Presumably what happens with TDD in practice is that removal of duplication along with respect for SOLID principles(“Open/Closed” was the only one mentioned here) leads to a natural evolution of the system’s design where changes areeffected only in specific spots as needed. There is an amusing statement here: At the limit, where you introduce the variations very quickly, TDD is indistinguishable from designing ahead.This is just YAGNI stated in a different way.How much feedback do you need?In other words, how many test cases should I test for?The answer is, it depends. Here are some questions to guide you: What are the chances of some weird edge cases occurring? What would be the impact of these edge cases? What is our target Mean Time Between Failure rate?Here is a tantalizing paragraph (emphasis mine): TDD’s view of testing is pragmatic. In TDD, the tests are a means to an end - the end being code in which we havegreat confidence. If our knowledge of the implementation gives us confidence even without a test, then we will notwrite that test. Black box testing, where we deliberately choose to ignore the implementation, has some advantages.By ignoring the code, it demonstrates a different value system - the tests are valuable alone.In this quote, if we take in everything up to and including the part in bold, one might conclude that Kent/TDD is OK withprogrammers choosing not to test certain arbitrary parts of the code. This turns on its head everything we know abouttesting so far, including the confidence that good tests and test coverage bring when refactoring, making sure everythingworks, etc. However, the last bit on black box testing makes me think this paragraph is really just talking about the choiceof not testing internal implementation details, which I completely understand and generally agree with.Can you drive development with application-level tests?Interesting conundrum.The risk with driving development with unit tests is these are only indirectly connected to the actual user story, so werun the risk of implementing a piece that may not actually be needed.The obstacle with driving development with application-level tests2 is setting up the fixtures, beingslower and more difficult to troubleshoot, as well as organizational issues surrounding resource allocation at specifictimes during the development lifecycle (devs+users) writing tests far in advance of actual development. The dreaded REDbar will stay red for quite a while.Footnotes See Extreme Programming Explained: Embrace Change, also by Kent Beck. &amp;#8617; Interesting that Application Test-Driven Development (ATDD) does not seem to come up in search results. I think it’s another term for Acceptance Test Driven Development. The link says Kent mentioned ATDD in this book. &amp;#8617; " }, { "title": "Golang Guild Session: Panics", "url": "/posts/golang-panics/", "categories": "talks, golang", "tags": "", "date": "2022-04-06 17:00:00 -0400", "snippet": "Slide deck for my presentation on Golang panics on the Golang Guild Session @ VerticalScope. Your browser does not support iframes.Click on the slide deck then press F to expand to full screen." }, { "title": "Getting to Yes: Negotiating Agreement Without Giving In", "url": "/posts/getting-to-yes/", "categories": "books, social", "tags": "book, negotiation", "date": "2022-02-16 22:43:00 -0500", "snippet": "Written by Roger Fisher and William Ury,Getting to Yes: Negotiating Agreement Without Giving Inis a fantastic guide to the art of principled negotiation. That’s right - the goal is to engage in principled negotiation.Do not let the title mislead you into thinking it’s a box of tricks for you to unilaterally “get your way”.Roger and William are both professors at Harvard and founders of the Harvard Negotiation Project.The book is a short and accessible read, suitable for everyone, including software engineers such as myself. I highlyrecommend this book to anyone looking to level up beyond their soft skills as a professional.Check out some other books I’ve read on the bookshelf.SummaryGetting to Yes exhorts parties in a negotiation to pivot away from the typical positional bargaining tactics, and intonegotiation based on principles, standards and interests independent of the will of any single party.The game of positional bargaining involves locking the “terrain” upfront by staking initial (and often extreme) positions,yielding as little as possible in a pre-established vector of negotiation while making the other side yield as much as possiblealong the same line.In contrast, principled negotiation leaves the field open for collaborative exploration of the underlying interests andthe universe of options that can address those interests, based on objective criteria, all while separating the peoplefrom the problem. The goal is to produce wiser agreements more efficiently and in a way that preserves relationships.The book is laden with great examples ranging from a fictional tenant negotiating the rent with their landlord to real life exampleswhere countries avoid war due to clever negotiation tactics. Here’s one of my favorites that showcases the advantagesof focusing on interests and not positions: […] two men are quarreling in a library. One wants the window open and the other wants it closed. They bicker backand forth about how much to leave it open: a crack, halfway, three-quarters of the way. No solution satisfies them both. Enter the librarian. She asks one why he wants the window open: “To get some fresh air.” She asks the other why hewants it closed: “To avoid the draft.” After thinking a minute, she opens wide a window in the next room, bringing infresh air without a draft.The book includes sobering chapters on what to do if the other side won’t play the game of principled negotiation and/oris more powerful. These sections really serve to round off the excellent treatise on principled negotiation by acknowledgingcircumstances where the method may not work and presents the reader with options in those cases.Lastly, the book acknowledges that readers are probably already aware of most of what it teaches at some level of their experience.The goal in writing the book was to organize these common ideas and experiences into a usable framework.ThoughtsI am a software engineer with experience ranging from individual contributions in big hierarchical organizations tomore collaborative team work in flat “startups” doing pair-programming.Despite the romanticized view of a software engineer with their hoody pulled over their headand earphones plugged into the sides of their heads, software engineering is a social endeavour. There is always some levelof collaboration needed with other software engineers, operations, DBAs, helpdesk, product managers, your manager, etc.Software engineers like to debate a lot. Too many pointless arguments about details that don’t really matter in the longrun. I’ve been guilty of this and have just bruises and scars to show for it. The world keeps turning; the business keepsoperating.Sometimes you really do have to sit down and negotiate on something worthwhile, but these instances are fewand far in between. I think that of far more value is the wisdom to distinguish between that which is worthwhileand that which isn’t1. Because one thing that Getting to YES confirms is that principled negotiation can requirea lot of effort. Brainstorming, inventing options, etc. Use wisely.Reading Getting to YES gives me comfort in recognition and validation of my system of thought, but it won’t make me anegotiator “superhero” who can win every negotiation. Sometimes the balance of BATNA (see definition below) is tiltedagainst you, the other side is aware of it, and there is nothing you can do to make up the difference. Other timesthe age-old saying applies: you cannot reason someone out of something he or she was not reasoned into. You can’t reasonagainst a working culture.Still, Getting to YES provides me a structured framework that fits my style and a checklist of things to do and thingsto watch out for. I intend to keep this book on my shelf for a long, long time.The Problems with Positional BargainingUnwise outcomesPositional bargainers remain locked in their initial positions at the expense of creativity and the discovery of theunderlying interests.InefficientParties hold extreme positions, conceding as little as necessary to keep the negotiation going. This is even worse whenthere are many parties at the negotiating table.Endangers an ongoing relationshipPositional bargaining becomes a contest of will as each party digs deeper and deeper into their position and attempts tomake the other side bend to their rigid will, resulting in anger and resentment. One side may decide to switch to a softstyle by being generous in order to preserve the relationship on good terms. This makes reaching an agreement likely andfaster, but not necessarily a wise agreement. Anger and resentment may still brew under the surface, and the softbargainer becomes vulnerable to the hard bargainer. As the book says, “if your response to hard positional bargaining issoft positional bargaining, you will probably lose your shirt”.Getting to YES without giving in: The Method1. Separate the people from the problem Disentangle the relationship from the substance; deal directly with the people problem Perception: Put yourself in their shoes. Conflict lies not in objective reality, but in people’s heads. Truth is simply onemore argument. The difference itself exists because it exists in their thinking. Fears, even if ill-founded, are realfears and need to be dealt with. They may well believe that their views are “right” as strongly as you believe yours are. When you talk about the problem, distinguish the symptoms from the person with whom you are talking. My Experience: Them: on several occasions I have engaged in a debate and after making my point on objective groundsthe other side then turns around and says “Oh I see, so what YOU want is ____”. It’s not about what _I want -it’s about reaching a satisfactory agreement grounded in reason and facts. Me: find it hard to separate the person from the problem when said person has a history of being a charlatan(ie. they lost my respect). Discuss each other’s perceptions explicitly. Don’t blame them for your problem. Give them a stake in the outcome by making sure they participate in the process. My Experience: sometimes important. There are people that have a visceral reaction to solutions of whichthey were not personally a part of. They must simply be involved in everything. Make your proposals consistent with their values. This allows them to save face. Emotion: Pay attention to core concerns: autonomy: the desire to make one’s own choices appreciation: the desire to be recognized and valued affiliation: the desire to belong as an accepted member of some peer group role: the desire to have meaningful purpose status: the desire to feel fairly seen and acknowledged Consider the role of identity: a surefire driver of strong negative emotion is a perceived threat to identity - one’sself-image or self-respect. My Experience: as software engineers, we pride and delude ourselves into thinking we are smarter than theaverage bloke. For us, a mere whiff or insinuation otherwise is an instant trigger. Make emotions explicit and acknowledge them as legitimate My Experience: I don’t really have experience here. But I do recognize it as a legitimate point and will workto bring up awareness to it on my next heated debate. Don’t react to emotional outbursts Use symbolic gestures My Experience: use with care. Use wisely. Very wisely. And keep a watchful eye on their reaction (do they reciprocate?). Communication: Listen actively and acknowledge what is being said My Experience: I have been told I don’t listen to others/others don’t feel like I listen to them. I do. I swearI do. What I plan on doing moving forward is to confirm my understanding of their points before I voice my own.I will basically replay their talking points back. Speak to be understood Speak about yourself, not about them My Experience: hard to restrain myself from pointing out unfairness. :( Speak for a purpose Prevention works best Build a working relationship Face the problem, not the people 2. Focus on interests, not positions Interests define the problem. Behind opposed positions lie shared compatible interests, as well as conflicting ones.Hard to do. How much effort can I afford to invest into this? Thinking back to the librarian in the summary’s example -could I have really come up with that solution in a minute? Perhaps. But time is precious in real life and problem-solvinglike this I usually time-box. How do you identify interests? Ask “Why?” Ask “Why not?” Talk about interests: make your interests come alive acknowledge their interests as part of the problem put the problem before your answer look forward, not back (aka “be constructive”?) be concrete but flexible A remarkable insight, obvious in hindsight: Agreement is often made possible precisely because interests differ. You and a shoe-seller may both like money and shoes.Relatively, his interest in the fifty dollars exceeds his interest in a pair of shoes. For you, the situation isreversed: you like the shoes better than the fifty dollars. Hence the deal.3. Invent options for mutual gain Avoid premature judgement of options My Experience: hard to do when I already formulated an opinion of the other side’s character. :( Don’t search for the one single perfect answer Don’t assume a “fixed pie” Don’t think that “solving their problem is their problem” My Experience: depends on whether I think the other side acts in good faith or not Separate inventing from deciding: lots and lots of brainstorming… Broaden your options: Multiply options by cycling between the specific and the general: The Circle Chart (p. 68) Look through the eyes of different experts Invent agreements of different strengths Change the scope of a proposed agreement Look for mutual gain Identify shared interests Dovetail differing interests Any difference in interests? Different beliefs? Different values placed on time? My Experience: I consider myself more a doer than a talker. I tend to place greater weight on time managementthan perfection. Different forecasts? Differences in aversions to risk? Ask for their preferences 4. Insist on using Objective Criteria deciding on the basis of will is costly principled negotiation produces wise agreements amicably and efficiently Developing objective criteria fair standards fair procedures negotiating with objective criteria frame each issue as a joint search for objective criteria ask “what is your theory?” agree on first principles reason and be open to reason never yield to pressure That all sounds good, but what if…… they are more powerful?Here is a sobering quote: No method can guarantee success if all the leverage lies on the other side. No book on gardening can teach you to growlilies in a desert or a cactus in a swamp. If you enter an antique store to buy a sterling silver George IV tea set worththousands of dollars and all you have is a one hundred-dollar bill, you should not expect skillful negotiation toovercome the difference. In any negotiation there exist realities that are hard to change. In response to power,the most any method of negotiation can do is to meet two objectives: first, to protect you against making an agreementyou should reject and second, to help you make the most of the assets you do have so that any agreement you reach willsatisfy your interests as well as possible. Chapter 6, What If They Are More Powerful?You should not expect miracles from reading this book. You should not expect to out-negotiate your boss when he has allthe power. The best you can do is develop your BATNA and use as a baseline to gauge options and protect your interestsas best you can. BATNA Best Alternative To a Negotiated AgreementAvoid setting a bottom line2. You don’t want to develop a blindspot and shut down creative solutions, nor do you wantto run the risk of setting your bottom line too high. Rather, develop your BATNA. Instead of asking yourself what yourbottom line is, ask yourself what will you do if an agreement is not reached at all. Doing so opens up more options foryou if you think about it. You are no longer locked into having to make an agreement with the other side - you can simplywalk away if your BATNA is sufficiently attractive. This is tremendously liberating. Another benefit of knowing your BATNAis that it focuses your mind away from fixed criteria (price, time, etc.) and opens is up to more creative options. Thesolution space is enlarged by magnitudes. Not knowing your BATNA means you are negotiating blind. […] the greater danger is that you are too committed to reach an agreement. The better your BATNA, the greater your power […] the relative negotiating power of two parties depends primarilyupon how attractive to each is the option of not reaching agreement. Chapter 6, What If They Are More Powerful?Knowing your own BATNA is great, but you should consider knowing the other side’s BATNA. Their BATNA may be betterfor them than any fair solution you can imagine. You may have to work hard to change their BATNA.Formulate a tripwire: this is not a bottom line; this is an intermediary point better than your BATNA but far fromyour ideal agreement. When this wire is tripped, you should sit back and assess the situation.When the other side is more powerful then your best assets are the merits of your argument. The larger a role you canestablish for principle the better off you are.… they won’t play?Use negotiation jujitsu: Recast an attack on you as an attack on the problem. Don’t attack their positions, look behind it. Don’t defend your ideas, invite criticism and advice. “What concerns of yours would this proposal fail to take into account?”. Ask them for advice. Ask them what they would do if they were in your position. Tactics to use: Use questions instead of statements. Statements generate resistance, whereas questions generate answers. Silence is one of your best weapons. Ask questions and pause. It often creates the impression of a stalemate that theother side will feel impelled to break by answering your question or coming up with a new suggestion. Use the one-text procedure: Defer to an “architect” (expert, third party, your boss; whatever fits) The architect seeks clarifications of each side’s interests as opposed to concessions The architect seeks continuous feedback Finally, there comes a point when the architect feels they can improve it no further and thus boil the decision downto a simple Yes or No. My Experience: there is no architect, only a dictator The book showcases a very interesting real-life example of a negotiation between a landlord and a tenant: The case ofJones Realty and Frank Turnbull.… they use dirty tricks?Recognize the tactic, raise the issue explicitly, and question the tactic’s legitimacy and desirability: After recognizing the tactic, consider bringing it up with the other side Execute The Method: Separate the people from the problem Focus on interests, not positions Invent options for mutual gain Insist on using objective criteria Often just recognizing a tactic will neutralize it.Common tricky tactics Deliberate deception Phony facts My Experience: typically used in high-level domains and backed up by “experience”. Hardly ever done with hard,easily falsifiable facts. Ambiguous authority Dubious intentions Less than full disclosure is not the same as deception Psychological warfare Stressful situations My Experience: one that comes to mind is being pulled into a surprise meeting having to explain myselfto a panel that has clearly discussed and prepared for the issue beforehand. Personal attacks The good-guy/bad-guy routine Threats Positional pressure tactics Refusal to negotiate Extreme demands Escalating demands Lock-in tactics Hardhearted partner A calculated delay My Experience: used when someone wants to try to stay in control of something and they sense someone else‘encroaching’ on their territory. “Take it or leave it” Don’t be a victimFootnotes Epictetus, Discourses, 2.5.4-5: “The chief task in life is simply this: to identify and separate matters so that I can say clearly to myself which are externals not under my control, and which have to do with the choices I actually control. Where then do I look for good and evil? Not to uncontrollable externals, but within myself to the choices that are my own…” &amp;#8617; A bottom line is a non-negotiable position by definition. &amp;#8617; " }, { "title": "Escaping the Build Trap: How Effective Product Management Creates Real Value", "url": "/posts/build-trap/", "categories": "books, product-management", "tags": "book, product-management", "date": "2022-01-01 14:50:00 -0500", "snippet": "Written by Melissa Perri,Escaping the Build Trap: How Effective Product Management Creates Real Valueessentially re-architects the software development lifecycle by anchoring it on actionable feedback from the customer.Actually, we probably shouldn’t write a single line of code before we are sure the customer needs the feature. Even then,you provide a minimal feature set (MVP) in alpha state and validate their effectiveness. Collecting feedback at the “tail end” of the flow is an important step, as it closes the loop and orients development of the product in a focused way thatprovides confidence on its value proposition.Check out some other books I’ve read on the bookshelf.SummaryEscaping the Build Trap criticizes the tendency of many companies to measure their success by the number of featuresshipped rather than the value those features create for their customers and how that value helps further the company’sstrategic objectives. The term product-led is coined to describe an alternative process that begins with identifyingthe strategic objectives and the value proposition for the customer base that would result in furthering those objectives.This process is embodied in the Product Kata and borrows from the scientific process by redefining the organization’sapproach to problem-solving to be one of experimentation and testing of hypothesis. A hypothesis is tested in smallincremental steps as fast as possible, with bad ideas discarded while good ideas that provide value are kept.For me, as a software engineer, this book reaffirms my commitment to in general focus on the things that really matter -things that provide actual value - and defer or discard the rest.What is the Build Trap? The build trap is when organizations become stuck measuring their success by outputs rather than outcomes. It’s whenthey focus more on shipping and developing features rather than the actual value those things produce. Part I, The Build TrapWhat we know and what we don’tWhen figuring out what product or feature to build we must first figure out what we know and don’t know:   Known Unknown Known Facts Questions Unknown Intuition Discovery Chapter 5: What We Know and What We Don’t Known-knowns: facts gathered from data or critical requirements (regulations or basic needs for the job). Known-unknowns: clarified enough that you know which questions to ask Unknown-knowns: Here Be Dragons. This is where bias thrives. Known-unknowns are born out of years of experienceand are based on intuition. Unknown-unknowns: the things you don’t know that you don’t know. You don’t know enough to ask the right questions oridentify the knowledge gaps. These are moments of surprise that need to be discovered as you sift through unrelateddata or talk to customers.How does this fit into the Agile process? Agile […] promote[s] a better way of collaboration and a faster method of building software, but it largely ignoreshow to do effective product management. Agile assumed that someone was doing that front-of-funnel part, generating and validating ideas, and instead optimizedthe production of software. Yet, that piece has been lost along the way, as companies believe that Agile is all youneed to do successful software development. So, many product managers in Agile organizations still operate with thisWaterfall mindset. Chapter 6: Bad Product Manager ArchetypesThis is a fascinating distinction; it had never occurred to me that while doing “agile” we were still stuck in thiswaterfall mindset!Speaking of validating ideas…The Product Death CycleThe Product Death Cycle is likely to occur when we end up with a mindset geared towards reactive thinking as opposedto strategic thinking, when we are implementing ideas without validating them:Figure 6-1 from Chapter 6: Bad Product Manager ArchetypesThe Product ManagerWhat does a great Product Manager look like?Chapter 7 A Great Product Manager does into some depth - I’ve summarised the contents in the following paragraphs.The term “Product manager” is misleading - the role is not a manager in the traditional sense of the word. The roledoes not come with much direct authority. Product Managers are effective when working through others, recognizing theirstrengths, in order to achieve their common goal.Product managers know why the team is building what they are building. Armed with this why they can convince the teamthat this is the right direction to go in.To figure out the why or what to build takes a strategic and experimental approach. Product Managers take input fromcustomer research, expert information, market research, business direction, experiment results, and data analysis, usingthose inputs to craft a vision for the product that will solve the customer’s needs and further the company’s vision.Since the Product Manager has to wear many hats, they need to be able to speak both the business language and thetechnical language to certain degrees. They don’t need to be experts in either, but should be sufficiently versed inboth domains so that they can discuss tradeoffs with teams on both domains. A product manager must be tech literate, not tech fluent. Chapter 7: A Great Product Manager, section Tech Expert Versus Market ExpertA Great Product Manager starts with Why. These why questions are productive and purposeful: How do we determine value? How do we measure the success of our products in the market? How do we make sure we are building the right thing? How do we price and package our product? How do we bring our product to market? What makes sense to build versus buy? How can we integrate with third-party software to enter new markets?What does a bad Product Manager look like? The Mini-CEO arises out of the myth that the role has sole authority over the product. The Waiter the product manager who is at heart an order-taker. They ask stakeholders for what they want and turnall those wishes into an itemized list of features. There is no vision. There is no decision-making involved. Andbecause they have to goals in which to provide context for tradeoffs, the priority of these features becomes apopularity contest for whomever is making the request. The Former Project Manager more focused on the when instead of the why.The Product-Led Organization The product-led organization is characterized by a culture that understands and organizes around outcomes over outputs,including a company cadence that revolves around evaluating its strategy in accordance to meeting outcomes. Inproduct-led organizations, people are rewarded for learning and achieving goals. Management encourages teams to getclose to their customers, and product management is seen as a critical function that furthers the business. Part V: The Product-Led OrganizationOutcome-Focused CommunicationIn a product-led organization the communication during meetings tends to become more focused on outcomes. Examplefrom the book: At the next quarterly business review, Karen was able to speak about the accomplishments of the team. “This quarter, we were able to launch the video-editing software and onboard 150 new classes to our site, all in keyareas of interest for our users. Since the launch of those courses, we have seen an increase in acquisition rate from15% to 25%, and our retention numbers have risen to 60%. We’re well on our way to reaching our goals. With the additionalefforts from other teams around this strategic intent, we’ll hit our goal early - within a year and a half.” Chapter 20: Outcome-Focused CommunicationTo note in this example: absolute numeric quantities are provided to give a sense of the scale of adoption “key areas of interest” reminds everyone that the effort is focused on priorities relative numeric figures are provided to compare before-after, and, when combined with the timeframe (a quarter), itgives a sense of the velocity of change and sets up expectations ends with a deadline of a year and a half, meaning it has a clearly defined goal and end-dateRoadmapsDo not think “gantt chart”!Think of roadmaps as living documents, as an explanation of strategy and the current stage of your product.A few key parts: The theme The Hypothesis Goals and success metrics Stage of development Any important milestonesUseful terminology for phases of a feature: Experiment No production code is being created. The team is understanding the problem and determining whether it is worth solving. Alpha The team is determining whether the solution is desirable to the customer with a minimum feature set withproduction code for a small set of users. The users understand they are getting early access to a feature that might changeor be killed. Beta The team is determining whether the solution is technically scalable. This release is available to more customersthan the Alpha phase. Generally Available (GA) The solution is widely available to all customers and the sales teams can talk openly about GA products and features.A Good Strategic Framework A good strategy is not a plan; it is a framework that helps you make decisions. Part III: StrategyCreating a StrategyChapter 12: Creating a Good Strategic Framework, Figure 12-3Deploying a StrategyChapter 12: Creating a Good Strategic Framework, Figure 12-1The rows in green are deployed at the company level, the rows in blue are deployed at the individual product level.Rewards and IncentivesDo not tie bonuses and promotions to how many features were shipped, rather incentivize for learning and problem-solvingfor customers.Safety and Learning (culture)To properly deploy adequate and product-led rewards and incentives the company’s culture must provide enough safetyto fail and learn. Big asterisks mark big caveats here: This doesn’t mean that we should be failing in spectacular ways. With the rise of Lean Startup, we began to focuson outcomes, yes, but we also started to celebrate failure. I want to be clear here: it is not a success if you failand do not learn. Learning should be at the core of every product-led organization. It should be what drives us as anorganization. It is just better to fail in smaller ways, earlier, and to learn what will succeed, rather than spending all the timeand money failing in a publicly large way. Chapter 22: Safety and Learning" }, { "title": "Clean Architecture: A Craftsman&#39;s Guide to Software Structure and Design", "url": "/posts/clean-architecture/", "categories": "books, architecture", "tags": "clean-architecture, uncle-bob, book", "date": "2021-12-28 00:00:00 -0500", "snippet": "Written by Robert C. Martin (a.k.a. “Uncle Bob”),Clean Architecture: A Craftsman’s Guide to Software Structure and Designoffers guidance on how to engineer your software product’s architecture - composed of many components and modules, andthe relations between them - in order to “minimize the human resources required to build and maintain the requiredsystem”.Following are some notes I took after reading the book. Rather than be the end all, be all, this is intended to be a living document with notes and annotations added as my understanding grows.Check out some other books I’ve read on the bookshelf.SummaryClean Architecture lays out recommendations for the high-level structure and ordering of system components (Java jars, Go modules)in a way that isolates changes in individual modules and minimizing disruptions in the rest. This is accomplished by isolatingcomponents implementing critical business policies from lower-level details like persistence layers or UI components byhaving the latter depend on the former and not the other way around (Dependency Inversion and the Dependency Rule).Although clearly aware of “entities” and “business logic”, Clean Architecture does not delve into the details of theprocess of determining what logic should be “business logic” vs. what shouldn’t be; for that I think the reader shouldhead towards more specialized resources such as Eric Evan’sDomain Driven Design: Tackling Complexity in the Heart of Software.Nevertheless, Clean Architecture steers the reader into thinking about software features in an “inside-out” manner:figure out the core business logic, then figure out the use cases, then plug in the rest of the details like thedatabase repositories, external service calls, and view components. This is a fantastic way of thinking about softwaredesign and I wish more teams would adopt this mindset.The goal of clean architecture The goal of software architecture is to minimize the human resources required to build and maintain the required system. Chapter I: What is Design and Architecture?The goal is achieved by identifying the components that are expected to change frequently versus those that are relativelystable. The axis of stability is a range of degrees, not a binary condition.The predominant concept at work is the Dependency Rule by far.What are components? Components are units of deployment. They are the smallest entities that can be deployed as part of a system. In Java,they are .jar files. In Ruby, they are gem files. In .Net, they are DLLs. In compiled languages, they are aggregationsof binary files. In interpreted languages, they are aggregations of source files. In all languages, they are the granuleof deployment. Chapter 12: ComponentsAs per this definition, a “component” in Go would be a module.What are modules? The simplest definition is just a source file. […] a module is a cohesive set of functions and data structures. Chapter 7: The Single Responsibility PrincipleAs per this definition, a Java “module” would be a class whereas the Go analog is a package.SOLID PrinciplesPart III Design Principles is devoted to a recap of SOLID principles and howthey “arrange our functions and data structures into classes, and how those classes should be interconnected.”SOLID principles are employed at the module level.Uncle Bob wrote previously extensively about SOLID principles inAgile Software Development: Principles, Patterns, and Practices.The book is on my reading TODO list.Of the five principles, perhaps the most relevant one to Clean Architecture is the Dependency Inversion Principle(chapter 11) since it provides the single most important tool an architect can employ to isolate stable components fromchanges in unstable ones and is directly related to the Dependency Rule.What modules go in which components?From Chapter 13: Component Cohesion:flowchart LR rep((REP:\\nGroup for\\nreusers)) ccp((CCP:\\ngroup for\\nmaintenance)) crp((CRP:\\nSplit to avoid\\nunneeded\\nreleases)) rep -- Too many unneeded releases --- ccp ccp -- Hard to reuse --- crp crp -- Too many components change --- repChapter13, figure 13.1: Cohesion principles tension diagramThe edges describe the cost of abandoning the principle on the opposite vertex.The Reuse/Release Equivalence Principle (REP) The granule of reuse is the granule of release. Chapter 13: Component CohesionClearly under-defined. Uncle Bob even admits the advice is weak. Appears to say that release snapshots should be providedfor components and that their classes and modules must belong to a cohesive group. I agree and recognize tautology whenI see it.The Common Closure Principle (CCP) Gather into components those classes that change for the same reasons and at the same times. Separate into differentcomponents those classes that change at different times and for different reasons.The Common Reuse Principle (CRP) Don’t force users of a component to depend on things they don’t need.Uncle Bob argues surprisingly strong about this principle, further saying things like: Thus when we depend on a component, we want to make sure we depend on every class in that component.In the modern software development world of downloadable and reusable libraries and APIs, this seems like a very strongstatement to make. Should we make sure we use every single feature of something like spf13/cobraor should we reinvent the wheel? Then again, with the recentlog4j vulnerabilities in mind, the wisdom behindUncle Bob’s argument is apparent. It seems difficult to choose one or the other.How do we manage coupling between components?In short, use The Dependency Rule.What is the Dependency Rule? Source code dependencies must point only inward, toward higher-level policies. Chapter 22: The Clean ArchitectureChapter 22, Figure 22.1: The clean architecture. This image instance was copied from Uncle Bob’s article here.A pet peeve of mine is when people pretend this image explains itself. It doesn’t. There’s a lot to unpack in just theterms alone (“use cases”, “controllers”, “ports”, “flow of control”) that you basically have to read the book to properlyunderstand it. Here’s my take:Explaining the Clean Onion DiagramConcentric circles Entities: are where “enterprise business rules” are implemented. These are the “high-level policies” Uncle Bob keepsmentioning elsewhere. I don’t know why he chose a different term here. Enterprise business rules are those implementedby the business whether the software system exists or not. Reference: section Entities, Chapter 20, Business Rules. Use Cases: implement the application-specific business rules. These allow the automated software to implement theenterprise business rules correctly. Example: after using enterprise business rules to determine whether a customeris applicable for a loan, the banking application may then use application business rules to determine whether theloan estimation screen should be displayed or not. Reference: section Use Cases, Chapter 20, Business Rules. Controllers/Presenters/Gateways: these are generally the “outer fringes” of your system. They adapt inputs fromthe outside world into messages the use cases can understand, and in turn adapt responses from the use cases intooutputs for the world. Controllers: very similar to the controller in the MVC design patternexcept it hands control off to the use case (a policy-driven component), not a data structure. See A Typical Scenario,chapter 22, Clean Architecture. Presenters: not to be confused with the MVC View. These adapt the use case response into a view model. See A Typical Scenario,chapter 22, Clean Architecture. Gateways: these are commonly termed “clients” for external systems - think “database repository” or “web service client”.See Database Gateways, chapter 23, Presenters and Humble Objects. The outer blue layer are your web frameworks, your dependency injection frameworks, your database drivers, etc.The fact that the green and blue layers have different labels makes the diagram confusing.Eastward horizontal arrowsThe eastward horizontal arrows connecting layers of the onion represent the direction of source code dependencies.Eg. something in DB uses or implements something in Gateways. In turn, something in the latter uses or implementssomething Use Cases. Something in Use Cases uses or implements something in Entities. Source code dependencies aremade to point inward, counter to the direction of control, following the dependency inversion principle.Diagram lower-right cornerThis is a bit of an interesting diagram because it depicts a pattern hardly ever seen in the wild: a Controller capturesan incoming message from the outside world and passes it to a Use Case. The Use Case produces a response that ishanded over to a Presenter, not the Controller. I suspect this is a rare pattern because of the popularityof using the web as transport with the browser as the client, and the prevalence and ease of use of web frameworks.Typical pattern in Go:// handleCreateOrders is our Controllerfunc (h *Handler) handleCreateOrders(w http.ResponseWriter, r *http.Request) { request := parseRequest(r) // adapt incoming message to a structure defined by the use case result, err := h.ordersUseCase.Create(request) if err != nil { http.Error(w, &quot;internal error&quot;, http.StatusInternalServerError) return } response := marshal(mapResponse(result)) // adapt use case result into view model _, err = w.write(response) // send the view model if err != nil { // log error }}Pattern prescribed in Clean Architecture:// handleCreateOrders is our Controllerfunc (h *Handler) handleCreateOrders(w http.ResponseWriter, r *http.Request) { request := parseRequest(r) // adapt incoming message to a structure defined by the use case // In this case `presenter` is a function injected into `Handler` that constructs `Presenter` implementations. // The presenter maps results and errors into the appropriate view models and sends them back to the client. h.ordersUseCase.Create(request, h.presenter(w))}Here’s what Uncle Bob calls a “typical scenario”:Chapter 22, Figure 22.2: A typical scenario for a web-based Java system utilizing a databaseFoundations of the Dependency RuleThe Stable Dependencies Principle Depend in the direction of stability. Chapter 14: Component CouplingWhat is meant by “stability”?A stable component is one that does not depend on other components yet many others depend on it.Instability of a component:\\[I = \\frac{Fan^{out}}{Fan^{in} + Fan^{out}} ; I \\in [0, 1]\\]Where: \\(I\\): instability \\(Fan^{out}\\): outgoing dependencies. Number of classes inside this component that depend on classes outsidethe component. \\(Fan^{in}\\): incoming dependencies. Number of classes outside this component that depend on classes within thecomponent.Building a tool that lets you visualize instabilities of components in a code base would be a neat little project.The Stable Abstractions Principle A component should be as abstract as it is stable. Chapter 14: Component CouplingMakes sense when you think about it. Nothing is more abstract than an interface type. Everything depends on it, yet itdepends on nothing (\\(I = 1\\)).High-level policies are put into stable components. Since 100% abstract components (like interfaces) cannot implementpolicies (only declare them), the best classes would be abstract ones. Abstract classes are also extensible which impartsthe necessary flexibility when these policies change.Go does not have classes. In Go, high-level policies would be implemented by struct types that depend on interface abstractionsof more unstable components. This aligns very well with the Go convention ofdeclaring interfaces where they are used, not wherethe implementations reside1.Clean Architecture identifies abstract components as the place where high-level business policies should go, andprovides simple formulas for measuring how abstract or stable they are. It does not inform the reader of how to identifythose policies. For that you would head to another resource such as Eric Evan’sDomain Driven Design: Tackling Complexity in the Heart of Software.Uncle Bob does provide some rules of thumb though: Source code dependencies always point in the same direction across the boundary and always towards the higher-levelcomponent. Chapter 18: Boundary AnatomyFrom Chapter 19 Policy and Level we also gather that the further a policy is from both inputs and outputs of the system,the higher its level.How do components communicate with each other?Emphasis mine: […] the data that crosses the boundaries consists of simple data structures. […] We don’t want to cheat andpass entity objects or database rows. We don’t want the data structures to have any kind of dependency that violatesthe Dependency Rule.Footnotes Be careful and thoughtful with this. There is a lot of dogma in the Go community. There are plenty of exceptions to this rule. Examples: decorator pattern, first class polymorphic types. &amp;#8617; " }, { "title": "Golang - First look at generics", "url": "/posts/golang-generics-first-look/", "categories": "programming, go", "tags": "go, golang, generics, learning-go, java", "date": "2021-12-21 00:00:00 -0500", "snippet": "This post is part of a series where I do my best to organize mythoughts around Go: its paradigms and usability as a programming language. I write this as a Java programmer thatrespects the principles of Elegant Objects.Go 1.18 Beta 1 was just released. These are my initial impressions of the mainfeature to be delivered in this release: generics.SyntaxGo’s syntax possesses a very similar structure to Java’s:Go// A function.func Print[T any](t T) { fmt.Printf(&quot;printing type: %T\\n&quot;, t)}// A type.type Tree[T any] struct { left, right *Tree[T] data T}Java// A function.public static &amp;lt;T&amp;gt; void print(T t) { System.out.println(&quot;printing type: &quot; + t.getClass().getName());}// A type.class Tree&amp;lt;T&amp;gt; { private Tree&amp;lt;T&amp;gt; left, right; private T data;}One difference: Go requires the type parameter to be explicitly constrained by a type (eg.: T any) whereas Javadoes not (T on its own is implicitly inferred as a java.lang.Object). Failing to provide the constraint in Go willresult in an error similar to the following: ./prog.go:95:13: syntax error: missing type constraintI suspect the difference lies in Java’s unified type hierarchy (every thing is a java.lang.Object). Go possesses no suchmodel.Type SwitchThe following compile error surprised me:func print[T any](t T) { switch t.(type) { case string: fmt.Println(&quot;printing a string: &quot;, t) // error: cannot use type switch on type parameter value t (variable of type T constrained by any) }}… since the following is legal Go code:func print(t interface{}) { switch t.(type) { case string: fmt.Println(&quot;printing a string: &quot;, t) }}This would appear to mean that any is not simply an alias for interface{} as declared by Robert Griesemer and IanLance Taylor in this talk. This was raised inthis issue that points tothis rationalein an earlier draft of the proposal. This is especially surprising on union type parameters:func print[T int64|float64](t T) { switch t.(type) { // error: cannot use type switch on type parameter value t (variable of type T constrained by int64|float64) case int64: fmt.Println(&quot;printing an int64: &quot;, t) case float64: fmt.Println(&quot;printing a float64: &quot;, t) }}While looking at the comments in this and related issues I get impression there’s a decent chance that type-switching ontype parameters will be possible in the future. Just not in 1.18. To work around this, assign t to a variable of typeinterface{} and type-switch on that.In the meantime, here’s the same feature in Java (combining switch expressionsfrom Java 14 and pattern matching for switch expressions (preview) in Java 17):public static &amp;lt;T&amp;gt; void print(T t) { switch(t) { case String s -&amp;gt; System.out.println(&quot;you sent string: &quot; + s); default -&amp;gt; System.out.println(&quot;you sent an unknown type: &quot; + t.getClass().getName()); };}Type ConstraintsIn Go, the type parameter constraint T any indicates T is not constrained by any particular interface. In other words,T implements interface{} (not quite; see Type Switch).In Go we can further constrain the type set of T by indicating something other than any, eg.:// T is now constrained to int types.type Tree[T int] struct { left, right *Tree[T] data T}Equivalent Java:class Tree&amp;lt;T extends Integer&amp;gt; { private Tree&amp;lt;T&amp;gt; left, right; private T data;}In Go, type parameter declarations can specify concrete types (like Java) and can be declared inline or referenced:// inlinedfunc PrintInt64[T int64](t T) { fmt.Printf(&quot;%v\\n&quot;, t)}// referencedfunc PrintInt64[T Int64Type](t T) { fmt.Printf(&quot;%v\\n&quot;, t)}// reusable (like constraints.Integer)type Bit64Type interface { int64}Go’s reusable type constraintsGo’s reusable type constraints are a bit… odd.Take this simple example interface Tester:package maintype Tester interface { Test()}type myTester struct {}func (m *myTester) Test() {}func test(t Tester) { t.Test()}func main() { test(&amp;amp;myTester{})}… then add a type constraint:package maintype Tester interface { int64 Test()}type myTester struct {}func (m *myTester) Test() {}func test(t Tester) { // ERROR: interface contains type constraints t.Test()}func main() { test(&amp;amp;myTester{})}Never mind that int64 does not implement Tester - the error implies that arguments cannot be of interface types thatcontain type constraints. This can be demonstrated even when both types implement the same methods:package maintype Tester interface { *myTester1 Test()}type myTester1 struct {}func (m *myTester1) Test() {}type myTester2 struct {}func (m *myTester2) Test() {}func test(t Tester) { // ERROR: interface contains type constraints t.Test()}func main() { test(&amp;amp;myTester1{})}My surprise stems from the reuse of the interface construct when declaring type constraints. Adding type constraintsto an interface changes its nature entirely and limits its uses to generic type parameter declarations only. This willcome across as strange to veterans who are used to Go’s structural typing system.Union TypesBoth Go and Java support union types as type parameters but they do so in very different ways.Union Types in GoGo allows union types for concrete types only.// GOODfunc PrintInt64OrFloat64[T int64|float64](t T) { fmt.Printf(&quot;%v\\n&quot;, t)}type someStruct {}// GOODfunc PrintInt64OrSomeStruct[T int64|*someStruct](t T) { fmt.Printf(&quot;t: %v\\n&quot;, t)}// BADfunc handle[T io.Closer | Flusher](t T) { // error: cannot use io.Closer in union (interface contains methods) err := t.Flush() if err != nil { fmt.Println(&quot;failed to flush: &quot;, err.Error()) } err = t.Close() if err != nil { fmt.Println(&quot;failed to close: &quot;, err.Error()) }}type Flusher interface { Flush() error}It seems like the primary motivation behind Go’s union types (known as type sets in their proposal) is to enable genericoperations using operators such as &amp;lt; on primitive types that support them(source: proposal).Other examples of type sets are in theconstraints package.To my surprise, it is possible to declare a reusable type constraint that is impossible to satisfy:package maintype Tester interface { int // int does not implement method `Test()` Test()}func test[T Tester](t T) { t.Test()}func main() { test(two(2)) // ERROR: two does not implement Tester (possibly missing ~ for int in constraint Tester)}type two intfunc (t two) Test() {}The error gives us a clue - use anapproximation constraint element:package maintype Tester interface { ~int // any type alias whose underlying type is an `int` will make do Test()}func test[T Tester](t T) { t.Test()}func main() { test(two(2)) // works}type two intfunc (t two) Test() {}Approximation constraint elements is about as close to covariance as Go will get in 1.18.Union Types in JavaJava allows union types for interface types only OR between a non-interface type and an interface type.// GOODpublic static class Tree&amp;lt;T extends Closeable &amp;amp; Flushable&amp;gt; { private Tree&amp;lt;T&amp;gt; left, right; private T data;}// GOODpublic static &amp;lt;T extends Number &amp;amp; Closeable&amp;gt; void printNumberAndClose(T t) { System.out.println(t.intValue()); try { t.close(); } catch (IOException e) { System.out.println(&quot;io exception: &quot; + e.getMessage()); }}// BADpublic static &amp;lt;T extends Integer &amp;amp; Float&amp;gt; void printIntegerOrFloat(T t) { // error: interface expected here System.out.println(t.toString()); // error: ambiguous call System.out.println(t.isNaN());}As alluded to by the &amp;amp; (“and”) operator, type parameters for union types in Java must satisfy all referenced “interfaces”:public class Main { public static void main(String... args) { printNumberAndClose(new CloseableNumber()); printNumberAndClose(12); // ERROR: no instance(s) of type variable(s) exist so that Integer conforms to Closeable printNumberAndClose(new InputStreamReader(System.in)); // ERROR: no instance(s) of type variable(s) exist so that InputStreamReader conforms to Number } static class CloseableNumber extends Number implements Closeable { // implements methods from Closeable // implements abstract methods from Number } public static &amp;lt;T extends Number &amp;amp; Closeable&amp;gt; void printNumberAndClose(T t) { System.out.println(t.intValue()); try { t.close(); } catch (IOException e) { System.out.println(&quot;io exception: &quot; + e.getMessage()); } }}Similar to the Go example above, printNumberAndClose below will compile even though the conditions of the union type areimpossible to satisfy given that java.lang.Integer is a final class:public class Main { public static void main(String... args) { printNumberAndClose(new CloseableNumber(0)); printNumberAndClose(12); // ERROR: no instance(s) of type variable(s) exist so that Integer conforms to Closeable printNumberAndClose(new InputStreamReader(System.in)); // ERROR: no instance(s) of type variable(s) exist so that InputStreamReader conforms to Integer } static class CloseableNumber extends Integer implements Closeable { // ERROR: Cannot inherit from final &#39;java.lang.Integer&#39; CloseableNumber(int n) { super(n); } // implements methods from Closeable } public static &amp;lt;T extends Integer &amp;amp; Closeable&amp;gt; void printNumberAndClose(T t) { System.out.println(t.intValue()); try { t.close(); } catch (IOException e) { System.out.println(&quot;io exception: &quot; + e.getMessage()); } }}Worst that can happen is that nobody will use printNumberAndClose.VarianceGo’s proposaldoes not include covariance nor contravariance.Java supports both via the use of wildcards:// covarianceprivate static void sort(List&amp;lt;? extends Number&amp;gt; list) { // sort}// contravarianceprivate static void reverse(List&amp;lt;? super Number&amp;gt; list) { // reverse}I summarized Java’s variance in a previous blog postFinal ThoughtsAlthough implementing a subset of Java’s generics features and despite the hackiness of reusable typesets, Go’sproposal is compelling and worthy of trials in real production code once 1.18 rolls around.Generics was a sorely missed feature in Go. I look forward to significant savings in lines of code as map/reduce algorithmsand data structures are de-duplicated altogether." }, { "title": "OAuth2 Bearer Token Usage", "url": "/posts/oauth2-token-bearer-usage/", "categories": "", "tags": "oauth2, authn", "date": "2020-12-22 00:00:00 -0500", "snippet": "I have immersed myself in the digital identity space for the past few years. A good chunk of this workinvolves reading (and sometimes creating) specifications, as you can imagine. It is critical that theybe written in such a way that two independent parties can build interoperable implementationswithout relying on each other’s code. With this in mind, let’s have a brief chat aboutOAuth2 Bearer Token Usage with a focus on the token’s encoding.But first, let’s have a briefly talk about what OAuth2 is.What is OAuth 2.0?OAuth2 is an authorization framework defined by RFC6749 outliningthe overall flow of messages between three actors: a “client”, a resource owner (RO), and an authorization server (AS).You might know the first two respectively as “relying party” and “user”. Those of you familiar withOpenID Connect also know the AS as the “Identity Provider”.At its heart, OAuth2 is all about a user authorizing a relying party to access their data hosted by an APIprotected by the authorization server. Note that it does not authorize the user themselves to access theAPI. The job of the AS is to collect and record the user’s consent to authorize the relying party access.You might have noticed the emphasis on framework above. That is because RFC6749 deliberately avoidsnormative text defining many implementation details. Stepping back a bit, all RFC6749 says is that there is a clientthat requests access to a resource protected by an authorization server, and that the resource owner mustapprove this access. Once authorized, the client obtains an access token to consume the resource.OAuth2 relies on the HTTP protocol and defines the basic structure of the messagesflowing between its actors. Relevant to the topic at hand is the token_typeincluded in the response to the client. As per the RFC, this attribute “provides the client with the informationrequired to successfully utilize the access token to make a protected resource request”.OAuth 2.0 Bearer Token UsageRFC6750 is the normative specification for how to use OAuth 2.0 Bearer tokens.What are “Bearer Tokens”?Recall the token_type attribute from above. It turns out that if the access token response indicates the token’s typeis Bearer, then it is a “bearer token” as defined in RFC6750, which means: Any party in possession of the token can use it, and It must be presented in a specific way (as defined in RFC6750).This is, by far, the most common type of access token in use on the web today.Great! I want to integrate social logins into my uber-mega website and disrupt a market overnight!Let’s get started!The misdirectionYou have implemented one of the OAuth 2 grant types (aka “flows”) as a client and the AS has issued a Beareraccess_token to you. What now? How do we use this token?Luckily for us, RFC6750 tells us exactly what to do! Or does it? Let’s explore my thought process on my first attempt at an implementation: The client must format an Authorization HTTP headerwith the token in a certain way. The syntax of bearer tokens includes a b64token: `b64token = 1*( ALPHA / DIGIT / “-“ / “.” / “_” / “~” / “+” / “/” ) *”=” This strongly suggests that Base64 encoding is involved in some way But, who encodes the access_token in Base64? Recall that the access_token is usually opaque to the client. Note that HTTP headers can have almost any US-ASCII character Also recall that the access_token pretty much consists of all printable characters - a superset of Base64 If the access_token is opaque to the client (I shouldn’t attempt to parse it), and it can also consist of invalid Base64 characters, then surely the client must Base64-encode the Bearer token, right?But are we sure? Let’s double check with RFC6750: The syntax of the “Authorization” header field for this scheme follows the usage of the Basic scheme defined in Section 2 of RFC2617 Following through we find that RFC2617 defines the Basic HTTP Authentication Scheme that also uses the Authorization HTTP header and Base64 to encode the credentialsPutting it all together: RFC6750 defines how to use OAuth 2.0 Bearer Tokens Must put the access_token in the Authorization header The syntax includes a character space identified by b64token This usage follows the Basic scheme in RFC2617 RFC2617 uses Base64 encodingGreat! All I have to do is encode the access_token in Base64 before putting it in the Authorization header.I’m ready to integrate my social logins! Narrator: He was not ready for integration.The realityBearer tokens are laid bare in the Authorization header.None of the existing implementations expect the access_token to be encoded in Base64 in the Authorization header.See for example: Microsoft Identity Platform GitHub’s Authorizing OAuth Apps An issue I filed with ORY Oathkeeper (only for me to subsequently realize my own confusion)What gives? Did everyone else get it wrong? (because of course I interpreted the spec correctly!)Lessons learnedIt is important that specifications have precise normative text around how messages are constructedand processed in order to be interoperable. If there are algorithms involved, specify them step-by-step.It is important that normative text be labelled as such.It is important to identify each role and their respective responsibilities and algorithms.In my opinion, a good example showcasing the previous points is Web Authenticationwhere: The high-level architecture is clearly depicted in diagrams Non-normative sections are clearly labelled. The interfaces are clearly defined Algorithms are explained in detail. Example: Create a new credentialI’m still grappling with a real consolidation of RFC6750 with reality. If I squint just rightI can see that when RFC6750 says “The syntax for Bearer credentials is as follows” it was unnecessarily informingthe client developer what the syntax of the token is. In hindsight, this seems to be a (rather terse) messagemeant for implementers of Authorization Servers. I think an improved version of this section would have beensplit into several parts, each directed at different audiences: one for developers of clients, another for developersof authorization servers, and another for developers of resource servers. However, the text in RFC6750 remainsterse and mixes multiple implementation details that concern the different actors in a different manner.Another improvement would be to rely less on examples and to provide normative descriptions the (very simple) processing algorithmsthat construct and parse these messages. That would have cleared up most of the confusion in the section 2.1,although the language itself could have used stronger wording. Indeed, the non-normative text in section 7.1 of RFC6749has stronger wording than that in RFC6750!No matter what, as an implementer: always verify your understanding of a specification against other implementations!" }, { "title": "Golang - Optional Arguments for APIs", "url": "/posts/golang-apis-optional-params/", "categories": "", "tags": "go, golang, design-patterns, api", "date": "2019-08-18 00:00:00 -0400", "snippet": "I was recently directed towards Dave Cheney’s article Functional options for friendlyAPIs where he shares histhoughts on designs for optional parameters in APIs. Dave ends with a proposal for functionalarguments that are optionally passed to a type constructor. There is no question this design issuperior to having a single constructor with lots of arguments.However:Dave’s design is overkill for 99% of use cases and imposes an unnecessary tax on both the maintainerand the consumer of these APIs.Developers integrating with these APIs are consumers, so are readers (aka. code reviewers).My proposalA simpler alternative: two constructors, one is default, the other accepts a config struct.Here is my proposed design for Dave’s constructors in term:package term// I identified just three options after a quick scan of the README:// Baud rate, and either CBreakMode or RawMode.type Options struct { CBreakMode bool // Defaults to RawMode if false Baud int}func Default(name string) (*Term, error) {...}func Custom(name string, options Options) (*Term, error) {...}What we gain…In terms of usageDecreased verbosity: occurrences of the symbol term is decreased. The magnitude of this benefitincreases linearly with the number of optional parameters:package consumerimport &quot;github.com/pkg/term&quot;func DaveDesign() { // default term, err := term.Open(&quot;/dev/ttyUSB0&quot;) // custom term, err := term.Open( &quot;/dev/ttyUSB0&quot;, term.Speed(57600), term.CBreakMode, )}func MyDesign() { // A ctor named &#39;Default&#39; immediately conveys the possibility of // customization to a consumer term, err := term.Default(&quot;/dev/ttyUSB0&quot;) // custom term, err := term.Custom( &quot;/dev/ttyUSB0&quot;, term.Options{ Baud: 57600, CBreakMode: true, } )}In terms of maintenanceDecreased number of unit tests: reducing the set of options to a value object renders tests for them needless.What we lose…In terms of usageNothing as far as I can see.The symbol Default clearly signals the possibility of custom Terms such that a developerconsuming this API would seek out alternatives if required. This means this design has no addedconfusing aspects.In terms of maintenanceN/A. We improve maintainability by reducing the number of artifacts we need to test.Any validations and/or computations can be extracted unto their own functions (whether static ormember functions) of the constructor’s type." }, { "title": "Variance in Java", "url": "/posts/java-variance/", "categories": "", "tags": "java, variance, generics", "date": "2019-04-10 00:00:00 -0400", "snippet": "The other day I came across this post describing what the author sees as pros and cons of Go after 8 months of experience. I mostly agree after working full time with Go for a comparable duration.Despite that preamble, this is a post about Variance in Java, where my goal is to refresh my understanding of what Variance is and some of the nuances of its implementation in Java.(ProTip: You’ll need to know this for your OCJP certificate exam.)I will write down my thoughts on this subject for Go in a later post.What is Variance?The Wikipedia article on variance says: Variance refers to how subtyping between more complex types relates to subtyping between their components.“More complex types” here refers to higher level structures like containers and functions. So, variance is about the assignment compatibility between containers and functions composed of parameters that are connected via a Type Hierarchy. It allows the safe integration of parametric and subtype polymorphism1. Eg. can I assign the result of a function that returns a list of cats to a variable of type “list of animals”? Can I pass in a list of Audi cars to a method that accepts a list of cars? Can I insert a wolf in this list of animals?In Java, variance is defined at the use-site2.4 Kinds of VarianceParaphrasing the wiki article, a type constructor is: Covariant if it accepts subtypes but not supertypes Contravariant if it accepts supertypes but not subtypes Bivariant if it accepts both supertypes and subtypes Invariant if does not accept neither supertypes nor subtypes(Obviously the declared type parameter is accepted in all cases.)Invariance in JavaThe use-site must have no open bounds on the type parameter.If A is a supertype of B, then GenericType&amp;lt;A&amp;gt; is not a supertype of GenericType&amp;lt;B&amp;gt; and vice versa.This means these two types have no relation to each other and neither can be exchanged for the other under any circumstance.Invariant containersIn Java, invariants are likely the first examples of generics you’ll encounter and are the most intuitive. The methods of the type parameter are useable as one would expect. All methods of the type parameter are accessible.They cannot be exchanged:// Type hierarchy: Person :&amp;gt; Joe :&amp;gt; JoeJrList&amp;lt;Person&amp;gt; p = new ArrayList&amp;lt;Joe&amp;gt;(); // COMPILE ERROR (a bit counterintuitive, but remember List&amp;lt;Person&amp;gt; is invariant)List&amp;lt;Joe&amp;gt; j = new ArrayList&amp;lt;Person&amp;gt;(); // COMPILE ERRORYou can add objects to them:// Type hierarchy: Person :&amp;gt; Joe :&amp;gt; JoeJrList&amp;lt;Person&amp;gt; p = new ArrayList&amp;lt;&amp;gt;();p.add(new Person()); // okp.add(new Joe()); // okp.add(new JoeJr()); // okYou can read objects from them:// Type hierarchy: Person :&amp;gt; Joe :&amp;gt; JoeJrList&amp;lt;Joe&amp;gt; joes = new ArrayList&amp;lt;&amp;gt;();Joe j = joes.get(0); // okPerson p = joes.get(0); // okCovariance in JavaThe use-site must have an open lower bound on the type parameter.If B is a subtype of A, then GenericType&amp;lt;B&amp;gt; is a subtype of GenericType&amp;lt;? extends A&amp;gt;.Arrays in Java have always been covariantBefore generics were introduced in Java 1.5, arrays were the only generic containers available. They have always been covariant, eg. Integer[] is a subtype of Object[]. The compiler allows you to pass your Integer[] to a method that accepts Object[]. If the method inserts a supertype of Integer, an ArrayStoreException is thrown at runtime. Covariant generic type rules implement this check at compile time, disallowing the mistake to ever happen in the first place.public static void main(String... args) { Number[] numbers = new Number[]{1, 2, 3, 4, 5}; trick(numbers);}private static void trick(Object[] objects) { objects[0] = new Float(123); // ok objects[1] = new Object(); // ArrayStoreException thrown at runtime}Covariant containersJava allows subtyping (covariant) generic types but it places restrictions on what can “flow into and out of” these generic types in accordance with the Principle of Least Astonishment3. In other words, methods with return values of the type parameter are accessible, while methods with input arguments of the type parameter are inaccessible.You can exchange the supertype for the subtype:// Type hierarchy: Person :&amp;gt; Joe :&amp;gt; JoeJrList&amp;lt;? extends Joe&amp;gt; = new ArrayList&amp;lt;Joe&amp;gt;(); // okList&amp;lt;? extends Joe&amp;gt; = new ArrayList&amp;lt;JoeJr&amp;gt;(); // okList&amp;lt;? extends Joe&amp;gt; = new ArrayList&amp;lt;Person&amp;gt;(); // COMPILE ERRORReading from them is intuitive:// Type hierarchy: Person :&amp;gt; Joe :&amp;gt; JoeJrList&amp;lt;? extends Joe&amp;gt; joes = new ArrayList&amp;lt;&amp;gt;();Joe j = joes.get(0); // okPerson p = joes.get(0); // okJoeJr jr = joes.get(0); // compile error (you don&#39;t know what subtype of Joe is in the list)Writing to them is prohibited (counterintuitive) to guard against the pitfalls with arrays described above. Eg. in the example code below, the caller/owner of a List&amp;lt;Joe&amp;gt; would be astonished if someone else’s method with covariant arg List&amp;lt;? extends Person&amp;gt; added a Jill.// Type hierarchy: Person &amp;gt; Joe &amp;gt; JoeJrList&amp;lt;? extends Joe&amp;gt; joes = new ArrayList&amp;lt;&amp;gt;();joes.add(new Joe()); // compile error (you don&#39;t know what subtype of Joe is in the list)joes.add(new JoeJr()); // compile error (ditto)joes.add(new Person()); // compile error (intuitive)joes.add(new Object()); // compile error (intuitive)Contravariance in JavaThe use-site must have an open upper bound on the type parameter.If A is a supertype of B, then GenericType&amp;lt;A&amp;gt; is a supertype of GenericType&amp;lt;? super B&amp;gt;.Contravariant containersContravariant containers behave counterintuitively: contrary to covariant containers, access to methods with return values of the type parameter are inaccessible while methods with input arguments of the type parameter are accessible:You can exchange the subtype for the supertype:// Type hierarchy: Person &amp;gt; Joe &amp;gt; JoeJrList&amp;lt;? super Joe&amp;gt; joes = new ArrayList&amp;lt;Joe&amp;gt;(); // okList&amp;lt;? super Joe&amp;gt; joes = new ArrayList&amp;lt;Person&amp;gt;(); // okList&amp;lt;? super Joe&amp;gt; joes = new ArrayList&amp;lt;JoeJr&amp;gt;(); // COMPILE ERRORCannot capture a specific type when reading from them:// Type hierarchy: Person &amp;gt; Joe &amp;gt; JoeJrList&amp;lt;? super Joe&amp;gt; joes = new ArrayList&amp;lt;&amp;gt;();Joe j = joes.get(0); // compile error (could be Object or Person)Person p = joes.get(0); // compile error (ditto)Object o = joes.get(0); // allowed because everything IS-A Object in JavaYou can add subtypes of the “lower bound”:// Type hierarchy: Person &amp;gt; Joe &amp;gt; JoeJrList&amp;lt;? super Joe&amp;gt; joes = new ArrayList&amp;lt;&amp;gt;();joes.add(new JoeJr()); // allowedBut you cannot add supertypes:// Type hierarchy: Person &amp;gt; Joe &amp;gt; JoeJrList&amp;lt;? super Joe&amp;gt; joes = new ArrayList&amp;lt;&amp;gt;();joes.add(new Person()); // compile error (again, could be a list of Object or Person or Joe)joes.add(new Object()); // compile error (ditto)Bivariance in JavaThe use-site must declare an unbounded wildcard on the type parameter.A generic type with an unbounded wildcard is a supertype of all bounded variations of the same generic type. Eg. GenericType&amp;lt;?&amp;gt; is a supertype of GenericType&amp;lt;String&amp;gt;. Since the unbounded type is the root of the type hierarchy, it follows that of its parametric types it can only access methods inherited from java.lang.Object.Think of GenericType&amp;lt;?&amp;gt; as GenericType&amp;lt;Object&amp;gt;.Variance of structures with N type parametersWhat about more complex types such as Functions? Same principles apply, you just have more type parameters to consider:// Type hierarchy: Person &amp;gt; Joe &amp;gt; JoeJr// InvarianceFunction&amp;lt;Person, Joe&amp;gt; personToJoe = null;Function&amp;lt;Joe, JoeJr&amp;gt; joeToJoeJr = null;personToJoe = joeToJoeJr; // COMPILE ERROR (personToJoe is invariant)// CovarianceFunction&amp;lt;? extends Person, ? extends Joe&amp;gt; personToJoe = null; // covariantFunction&amp;lt;Joe, JoeJr&amp;gt; joeToJoeJr = null;personToJoe = joeToJoeJr; // ok// ContravarianceFunction&amp;lt;? super Joe, ? super JoeJr&amp;gt; joeToJoeJr = null; // contravariantFunction&amp;lt;? super Person, ? super Joe&amp;gt; personToJoe = null;joeToJoeJr = personToJoe; // okVariance and InheritanceJava allows overriding methods with covariant return types and exception types:interface Person { Person get(); void fail() throws Exception;}interface Joe extends Person { JoeJr get(); void fail() throws IOException;}class JoeImpl implements Joe { public JoeJr get() {} // overridden public void fail() throws IOException {} // overridden}But attempting to override methods with covariant arguments results in merely an overload:interface Person { void add(Person p);}interface Joe extends Person { void add(Joe j);}class JoeImpl implements Joe { public void add(Person p) {} // overloaded public void add(Joe j) {} // overloaded }Final thoughtsVariance introduces additional complexity to Java. While the typing rules around variance are easy to understand, the rules regarding accessibility of methods of the type parameter are counterintuitive. Understanding them isn’t just “obvious” - it requires pausing to think through the logical consequences.However, my daily experience has been that the nuances generally stay out of the way: I cannot recall an instance where I had to declare a contravariant argument, and I rarely encounter them (although they do exist). Covariant arguments seem slightly more common (example4), but they’re easier to reason about (fortunately).Covariance is its strongest virtue considering that subtyping is a fundamental technique of object-oriented programming (case in point: see note 4).Conclusion: variance provides moderate net benefits in my daily programming, particularly when compatibility with subtypes is required (which is a regular occurrence in OOP). Taming the Wildcards: Combining Definition- and Use-Site Variance by John Altidor, et. al. &amp;#8617; As I understand it, the difference between use-site and definition-site variance is that the latter requires the variance be encoded into the generic type itself (think of having to declare MyGenericType&amp;lt;? extends Number&amp;gt;), forcing the API developer to preempt all use cases. C# defines variance at the definition-site. On the other hand, use-site variance doesn’t have this restriction - the API developer can simply declare his API as generic and let the user determine variance for his use cases. The downside of use-site invariance are the “hidden” surprises described above, all derived from “conceptual complexity, […] anticipation of generality at allusage points” (see Taming the Wildcards paper above). &amp;#8617; Principle of least astonishment - Wikipedia. I vaguely remember a reference somewhere about the designers of Java following this principle but I can’t seem to find it now. &amp;#8617; Joined concatenates several Texts. Declaring an invariant iterable of Text would make this constructor unusable to subtypes of Text. &amp;#8617; &amp;#8617;2 " }, { "title": "Golang - The database/sql package", "url": "/posts/golang-the-sql-package/", "categories": "", "tags": "go, golang, learning-go, go-proverbs", "date": "2019-04-03 00:00:00 -0400", "snippet": "This post is part of a series where I do my best to organize my thoughts around Go: its paradigms and usability as a programming language. I write this as a Java programmer that respects the principles of Elegant Objects.I am studying the Go Code Review mantra Accept Interfaces, Return Structs and was inspired to write this post after coming across Eli Bendersky’s post Design patterns in Go’s database/sql package. This is the first instance where I feel I can endorse the mantra with confidence. Eli does a good job analysing the architecture of database/sql - I’m just here to provide a little nuance and some of my own notes.Problem statementApplication programmers need a Database abstraction layer over a variety of SQL or SQL-like datasources for the most common use cases.Designing DALs is hard for two primary reasons: The large variety of database implementations and drivers The large variety of common use cases CRUD Transactions Connection pooling Prepared Statements Mapping of data types Stored procedures and functions Any solution is bound to end up as fat APIs ala sql.DB or JDBC.database/sql.DBInterestingly, sql.DB is a concrete type, not an interface. Why?sql.DB exposes a fat interface for the reasons laid out in Problem statement. There are several strategies to limit an API’s obesity1 - both database/sql and JDBC opt for the Interface Segregation Principle. database/sql muddles the waters a bit by doing More Than One Thing, while JDBC offers a cleaner separation of concerns.Because sql.DB is necessarily fat2,3, making it an interface will only hinder code that depends on it: it’s painful and wasteful to have to implement all those methods in your production code and in your mocks when you only need 3 or 4. For these reasons, programmers in general prefer to design facades or adapters and place them in front4 of fat APIs. Both in Java and in Go this extra component can be either a concrete type or an abstract type.YAGNI: whether you use a concrete or an abstract type depends on whether you’ll actually need the extra level of abstraction.Decoupling the user interface from driver interfaceRegarding why database/sql split the user interface sql.DB from driver.Driver, Eli notes: Adding user-facing capabilities is difficult because they may require adding methods to the interface. This breaks all the interface implementations and requires multiple standalone projects to update their code. Encapsulating functionality that is common to all database backends is difficult, because there is no natural place to add it if the user interacts directly with the DB interface. It has to be implemented separately for each backend, which is wasteful and logistically complicated. If backends want to add optional capabilities, this is challenging with a single interface without resorting to type-casts for specific backends. These points are true, but I think the overarching theme behind this design decision is simplicity and ease of use. Proper interface segregation and separation of concerns5 took the back seat and it all led to a mix of several orthogonal requirements in a single interface: Execute queries Connection pooling Thread safetyThey decided to implement Connection Pooling and Thread Safety themselves while drivers need only provide connections (and statements, and everything else derived from connections).The upside of this clear violation of SRP6 is a simpler learning curve for the user (they are exposed to a single, simple interface) and a simpler driver interface for vendors to implement.The downside is that the maintainers are burdened with the maintenance of code that doesn’t necessarily meet all user’s needs and may be stifling innovation in this area for Golang7.Conclusiondatabase/sql eases the learning curve for users by providing a simple API that nevertheless breaks the single responsibility principle and incorporates orthogonal yet useful functionality into this package, potentially discouraging innovation.sql.DB is presented best as a concrete type and not an interface because its requirements necessarily inflate it into a fat API, greatly diminishing any returns an interface has in a structurally-typed language like Go.1 aka “surface area”, but hey - since we’re talking about “Fat APIs” we might as well run with it :)2 justifiably breaking the Go Proverb The bigger the interface, the weaker the abstraction3 see section 2.9 of Elegant Objects vol 1 Keep interfaces short; use smarts4 aka “wrap”, but I dislike the term because its meaning has been diluted and may refer to any one of several distinct patterns5 see design parameters for database/sqlhere6 Single Responsibility Principle7 consider the wide variety of database-connection-pooling libraries in the Java ecosystem and how they each emphasize different aspects like ease of use, performance, features, etc.8 see “slow builds” and “uncontrolled dependencies” in section 4 Pain Points of Rob Pike’s Go at Google: Language Design in the Service of Software Engineering" }, { "title": "Golang - Smarter containers", "url": "/posts/golang-smarter-containers/", "categories": "", "tags": "go, golang, learning-go, elegant-objects", "date": "2019-03-13 00:00:00 -0400", "snippet": "This post is part of a series where I do my best to organize my thoughts around Go: its paradigms and usability as a programming language. I write this as a Java programmer that respects the principles of Elegant Objects.I wish to make a correction in this post. I want to focus on this statement: Products is not a “smart” container - see point #3 in the Java proposal. You would have to manually append the newly-created Product to ProductsWhat I meant is that clients would have to use it like this:prods := make(Products, 0)p := prods.Create(10)prods = append(prods, p) // extra imperative code forced on the client to add the product to the containerLet’s pay attention to this snippet:func (p *Products) Create(price float64) Product { prod := &amp;amp;product{id: 123, price: price} tmp := append(*p, prod) // compiler would not allow p = &amp;amp;(append(*p, prod)) p = &amp;amp;tmp // the problem here is that the caller still retains the original handle to `p` return prod}I was really close to solving that riddle. The trick is to assign a new value to the pointer variable. The pointer variable itself is passed by value, so callers would also see the side effects. Here’s what I mean:func (p *Products) Create(price float64) *Product { prod := &amp;amp;product{price: Price} *p := append(*p, prod) // done in one line for brevity return prod}// A test like this would passfunc TestCreate(t *testing.T) { prods := make(Products, 0) prod := prods.Create(10) assert.Len(t, prods, 1) assert.Contains(t, prods, prod)}With this I’ve proved that Products can be made smarter: create products and dynamically append them to itself.Several problems remain: Slices don’t know how to iterate themselves - only range knows that. Since this power is taken away from developers, iteration of Products is only possible with objects in memory. You cannot implement a custom iterable - like in Java - that can dynamically fetch results from a datasource. Since iteration is only done in memory space, deferred execution is harder to pull off. You’d basically need an abstraction for a function that returns the actual slice (think type Products func() []Product) Cannot be decorated. Cannot implement Premium as a slice of products because the type will have no usable attribute for this. Unless… we go back the function abstraction idea…" }, { "title": "Golang - another go at elegant containers", "url": "/posts/golang-elegant-functional-containers/", "categories": "", "tags": "go, golang, learning-go, elegant-objects", "date": "2018-12-19 00:00:00 -0500", "snippet": "This post is part of a series where I do my best to organize my thoughts around Go: its paradigms and usability as a programming language. I write this as a Java programmer that respects the principles of Elegant Objects.In a previous post I attempted to implement Elegant container-like idioms in Go. My approach was straightforward: follow the same train of thoughts I do in Java. I failed miserably.Following is an approach I find interesting.Let’s use FunctionsLet’s ditch interfaces altogether and define our Products type as a function. I’ve managed to earn back two features of the Java counterpart: Actual decorators Deferred executionHowever, I’ve only managed to work it out for query capabilities. Our Products is still a castrated object because it lacks smart capabilities as per point #3 in the previous post.package productstype Product interface { Id() int Price() float64}type Products func() []Product// function with a function as receiver!func (p Products) Fetch(id int) Product { for _, prod := range p() { if prod.Id() == id { return prod } } return nil}// all productsfunc All() Products { // read from a database, etc. return nil}// premium products filtered by `minimum` pricefunc Premium(minimum float64, all Products) Products { return func() []Product { filtered := make([]Product, 0) for _, p := range all() { if p.Price() &amp;gt;= minimum { filtered = append(filtered, p) } } return filtered }}// USAGEfunc main() { premium := products.Premium(1000, products.All()) prod := premium.Fetch(123) // fetch one premium product fmt.Printf(&quot;%+v&quot;, prod) for _, p := range premium() { // iterate through all premium products fmt.Printf(&quot;%+v&quot;, p) }}ConclusionA bit early to actually reach a conclusion but this design further encourages me to believe that Go is a lot more oriented towards functional programming than object-oriented programming. Almost to the pointing of making me question what net value do interfaces in this language provide?" }, { "title": "Golang - are Elegant Containers possible?", "url": "/posts/golang-elegant-containers/", "categories": "", "tags": "go, golang, learning-go, elegant-objects", "date": "2018-12-19 00:00:00 -0500", "snippet": "This post is part of a series where I do my best to organize my thoughts around Go: its paradigms and usability as a programming language. I write this as a Java programmer that respects the principles of Elegant Objects.What are “Elegant Containers”?EO style containers maximize the reuse of the highest abstractions possible, do not add unnecessary attributes or “getters”, and earn our respect because they know how to do their job.ScenarioWe need to create and fetch products. We also need to segregate products into regular and premium classes. Premium products cannot be priced below $1000.Java Examplepublic interface Products extends Iterable&amp;lt;Product&amp;gt; { Optional&amp;lt;Product&amp;gt; fetch(Long id); Product create(Float price);}public final class AllProducts implements Products { ...}public final class Premium implements Products { private static final Float MINIMUM = 1000f; private final Products all; public Premium(Products all) { this.all = all; } // overridden public Optional&amp;lt;Product&amp;gt; fetch(Long id) { return this.all.fetch(id).filter(prod -&amp;gt; prod.price() &amp;gt;= MINIMUM); } // overridden public Product create(Float price) { if (price &amp;lt; MINIMUM) { throw new IllegalArgumentException(); } return this.all.create(price); } // overridden public Iterator&amp;lt;Product&amp;gt; iterator() { return new Filtered&amp;lt;&amp;gt;( // org.cactoos.iterator.Filtered prod -&amp;gt; prod.price() &amp;gt;= MINIMUM, this.all.iterator() ); }}This design has several interesting properties: Products can be iterated over in a for-each loop The semantics of “Products IS-A Iterable&amp;lt;Product&amp;gt;” just works Any Product created will be viewable in a subsequent for-each traversal High cohesion: AllProducts focuses on all products, while Premium focuses on enforcing premium pricing rules. Any Iterable&amp;lt;Product&amp;gt; can be decorated with another Iterable&amp;lt;Product&amp;gt; Iteration is lazily-evaluatedCan it be done in Go?Elephant in the room: range only works on arrays and slices (those two are the only applicable types within scope of this blog post). That’s right: unlike in Java, canonical for-each loops in Go can only be done against arrays or slices, instead of against an interface. This immediately negates several points above.Not iterating against an interface means decorators lose the ability to lazyily evaluate the decorated object. This has implications for performance.However way you slice it, any “iterable” decorators will have to preload the entire decorated array and operate on that.So, barring that, how would this all look like in Go?type Product interface { Id() int Price() float64}// Our &quot;elegant&quot; container. Notice this type doesn&#39;t implement an interface.type Products []Productfunc (p *Products) Create(price float64) Product { prod := &amp;amp;product{id: 123, price: price} tmp := append(*p, prod) // compiler would not allow p = &amp;amp;(append(*p, prod)) p = &amp;amp;tmp // the problem here is that the caller still retains the original handle to `p` return prod}func (p *Products) Fetch(id int) Product { for _, prod := range *p { if prod.Id() == id { return prod } } // idiomatic Go signals &quot;not found&quot; using `nil` return nil}// Our &quot;decorator&quot;. Notice this is a completely different type than `Products`type Premium struct { Products threshold float64}func (p *Premium) Fetch(id int) Product { prod := p.Products.Fetch(id) if prod != nil &amp;amp;&amp;amp; prod.Price() &amp;gt;= p.threshold { return prod } return nil}func (p *Premium) Create(price float64) Product { if price &amp;lt; p.threshold { panic(&quot;illegal price&quot;) } return p.Products.Create(price)}There are a couple of problems here; Products is not a “smart” container - see point #3 in the Java proposal. You would have to manually append the newly-created Product to Products Premium is NOT a Products:func Test(t *testing.T) { prods := make(Products, 0) test(prods) premium := Premium{Products: prods, threshold: 1000} test(premium) // compiler error: cannot use premium (type Premium) as type Products}" }, { "title": "Golang - methods on nil references", "url": "/posts/golang-methods-on-nil-references/", "categories": "", "tags": "go, golang, null-object-pattern, nil, learning-go", "date": "2018-12-09 15:00:00 -0500", "snippet": "This is the first in a series of posts where I do my best to organize my thoughts around Go: its paradigms and usability as a programming language. I write this as a Java programmer that respects the principles of Elegant Objects.Go has structs - which are essentially DTOs - and the ability to implement methods on these structs by specifying receivers on functions.Go allows one to call methods on nil references because, although functions and structs are both equally first-class citizens, functions are more equal than structs (hence this post’s feature image).What are methods on nil references good for?Consider this API:package peopletype Person interface { Name() string}// GetPerson returns nil indicating the person was not foundfunc GetPerson(name string) Person { return nil}type person struct { name string}func (p *person) Name() string { return p.name} Our test code will panic if nil is returned by GetPerson(): person := GetPerson() fmt.Print(person.Name()) // panic: invalid memory address or nil pointer dereferenceThere are several ways the API can be improved in order to signal that this person was not found; I’m not sure which one is more idiomatic in Go. Let’s consider implementing the Null Object Pattern by exploiting the fact that you can execute methods on nil references.Let’s modify the Name() implementation on our person struct:func (p *person) Name() string { if p == nil { return &quot;person was not found&quot; } return p.name}Our test code will now print person was not found.Now Person has a dual nature: depending on circumstances, it can be a normal person with a name, or it can be an “invalid” person. This is an additional cognitive burden when trying to understand this struct. person is now unfocused, breaking the Single Responsibility Principle.I don’t know what good methods on nil references are for." }, { "title": "Nominalized Adjectives as Names for Decorators", "url": "/posts/naming-decorators/", "categories": "", "tags": "decorators, oop", "date": "2018-07-05 10:17:00 -0400", "snippet": "There is a strong tendency among Java and C# programmers to prefix or suffix their extended types,such as naming a “smart” View as SmartView,or a Work that is “delegated” as DelegatingWork.In this post I will focus on decorators and how thiswidespread naming scheme reduces readability and adds no value to the code’s context. I think it’stime we retire this needless naming redundancy. Milton, from Office SpaceComposable decorators are small,highly cohesive objects that work off of another instance of their same type and thus are unable tofunction on their own. You can think of decorators as adjectives. final Collection&amp;lt;Product&amp;gt; products = new FilteredCollection&amp;lt;&amp;gt;( Product::active, new MappedCollection&amp;lt;&amp;gt;( Subscription::product, new JoinedCollection&amp;lt;&amp;gt;( subscriptions1, subscriptions2, ... ) ) );The problem with the traditional naming scheme is the needless repetition: we know from the outsetthat products is a Collection but the code keeps hammering this point home over and over againas we read on. This code is tedious to write, but more importantly, it is tedious to read,because of how the words are composed: ‘product’ is a filtered collection, a mapped collection, a joined collection, collectionNormal, every day speech is not encumbered like this; nouns are routinely omitted when sufficientmeaning can be extracted from the context. You don’t normally say The rich people and the poorpeople, you just say the rich and the poor. Nouns are omitted and adjectives arenominalized.Following this same principle, to make the code above read like this: ‘product’ is a filtered, mapped, joined collectionIt would have to look like this: final Collection&amp;lt;Product&amp;gt; products = new Filtered&amp;lt;&amp;gt;( Product::active, new Mapped&amp;lt;&amp;gt;( Subscription::product, new Joined&amp;lt;&amp;gt;( subscriptions1, subscriptions2, ... ) ) );I recommend we make our code terser by removing redundancy and allowing the code’s context to workin our favor for readability’s sake. For example, let’s use nominalized adjectives as names for ourdecorators." }, { "title": "On the proposal for Data Classes", "url": "/posts/on-the-proposal-for-data-classes/", "categories": "", "tags": "software-development, design-patterns, antipattern, dto, java", "date": "2017-11-05 10:40:00 -0500", "snippet": "There is a new draft proposal for Java ‘Data Classes’ being worked on in project amber - read about it here. In short I think the main points are: Design intent: clearly and unequivocally express the design intent of a class as a ‘DTO’ (even though the author never mentions DTO) Boilerplate: let the compiler take care of proper implementations of typical DTO operations like equals() and hashCode() Switch statements: the author briefly talks about enabling semantic features around these DTOs, eg. the ability to use them as targets for switch statementsThe draft also goes into some specifics like accessors not necessarily following JavaBeans conventions, opt-in mutability, and migration/compatibility concerns, among others.Let me tell you why I think this proposal is a bad idea.Design intent:This is an honorable goal: it will immediately be obvious to a reader what the design intent of a data class is. The reader wouldn’t have to wade through countless lines following the typical getters and setters, equals, hashCode, and toString implementations just to be sure that this class is just a data class. Fine.To see the problem with this line of thinking you must first step back and ask yourself: how many DTO-types have I defined in my project and will this new language construct simplify my code considerably? If your answer is “a lot” to both questions, you have a deeper problem that this new construct cannot solve: you may not be thinking in an object-oriented way. Many of the types you’ve defined are merely bags of data and you are most likely programming in an imperative style, not object-oriented as you should be with Java. That is the crux of my argument.Let’s kill the obvious retort right away: DTO IS an antipattern but pure OOP is not practical therefore there’s no point in judging and getting upset about it.Yes, DTO is unavoidable sometimes, especially when interfacing with remote APIs (eg. JAX-WS). However, a couple of points: These should exist at the periphery of your code They should be encapsulated by your domain model, where operations like equals() and hashCode() truly matterIn essence, you must avoid anemic domain models. DTOs, if present, should only exist at the fringes of your project, and not form the backbone of your whole design philosophy. They should ideally only be present when auto-generated by framework/tools like wsimport and you should never have to touch them.Boilerplate:If you accept the previous argument, then the getter/setter “boilerplate” becomes a non-issue: tools will automatically create them only where you absolutely need them. In regards to the proposal, that leaves us with just the other Object methods of interest: equals() and hashCode(). Here’s what I think:They should be taken out of Object and made into their own interfaces.Not all APIs want or even need all their DTOs to implement equals(), much less hashCode().Switch statements:This is cool but, why not switch on any object using Object#equals (or on my proposed Equality interface)? Because of performance reasons? They already reached a compromise on switching on strings. And as an SO user so eloquently put it (source):...technical obstacles shouldn’t drive language design. If there is no way to compile it to efficient code, it might still get compiled to the equivalent of if … else … clauses, and still have a win on source code brevity.The bottom lineBy promoting DTOs to first-class citizenship, novice programmers who don’t know better will feel emboldened to keep abusing the DTO pattern. The other interest group - the implementators of “interface APIs” like Hibernate or jax-ws-ri - will gain some marginal benefit by having their templates reduced by a few lines.This proposal will overall promote bad design with only marginal benefits.Why not focus on other, good patterns?" }, { "title": "Java Raw Types", "url": "/posts/java-raw-types/", "categories": "", "tags": "bugs, software-development, java, maven, generics", "date": "2017-10-08 13:03:00 -0400", "snippet": "I was recently confounded by a something unexpected in Java’s type-erasure. Consider the following snippets:public class Issue&amp;lt;T&amp;gt; { public List&amp;lt;String&amp;gt; list() { /* return a list */ }}public static void main(String... args) { final Issue issue = new Issue(); //notice the use of the raw type final String s = issue.list().get(0); //DOES NOT COMPILE}It turns out that all generic type information is erased from an instance of a raw type - including all instance methods and non-static fields that have nothing to do with the type parameter declared in the class declaration. In the example above, issue.list() returns a raw List from which you can’t extract a String.Getting around this limitation depends on your context. If the calling code has the option of defining the type parameter at runtime, then the fix is as easy as final Issue&amp;lt;?&amp;gt; issue = new Issue&amp;lt;&amp;gt;(); final String s = issue.list().get(0);. My situation was different though: due to an internal implementation detail, my interface’s generic type declaration leaked out into its public API, even though it was never my intention for the calling code to be able to provide implementations of my interface. My final solution was getting rid of the generic type declaration entirely, albeit at the cost of a less-than-perfect separation of concerns in my internal implementation.Why does Java behave this way? “Backwards-compatibility issues”. ¯\\_(ツ)_/¯" }, { "title": "Java&#39;s equals()/hashCode()", "url": "/posts/java-s-equals-hashcode-should-not-have-been-declared-in-object/", "categories": "", "tags": "java, OOP, equals, hashcode", "date": "2017-10-08 00:00:00 -0400", "snippet": "One of my pet peeves of the Object.equals() and Object.hashCode() implementations that every class inherits in Java is the fact that, in principle, these are really intimate concerns of the class implementation, and that Object, a class that can be far removed from a user-defined type, should not be dictating what equality means to a descendant.I mean, if I had to guess, I’d say that James Gosling, creator of the Java programming language, was already [righteously] thinking of collections when designing Java, but gave no thought to union types.Enter union typesIt turns out that since generics were first introduced in Java, union of types has been supported! Consider this example:public interface Intersection&amp;lt;T extends Number &amp;amp; Comparable&amp;lt;T&amp;gt;&amp;gt; { public default int doSomething(T t1, T t2) { return t1.compareTo(t2) + t1.byteValue(); }}Pay attention to the implementation of doSomething: t1 holds methods of both Comparable and Number!Having learned all this, I believe if Java had supported union types when it introduced Object.equals() and Object.hashCode(), then logically the latter two should have been introduced in their own two interfaces, perhaps Equality&amp;lt;T&amp;gt; and Hashcode respectively.But first:Reference equalityJava already provides the == operator, otherwise known as the reference equality operator. That is, if two references, X and Y, point to the same object in the heap, then X == Y will equal true.Object.hashCode()The default implementation of Object.hashCode() is a native method call that “typically … [converts] the internal address of the object into an integer”.Object.equals()The default implementation of Object.equals() is precisely a reference comparison!public boolean equals(Object other) { return this == other;}In other words, whoever needs to perform the default “equality comparison” on an object already has the == operator at his disposal - no API contract required! Moreover, this violates the aforementioned principle, elegantly exposed by Brian Goetz - current Java language architect - while responding to a related inquiry:The decision about equals/hashCode behavior is so fundamental that it should belong to the writer of the class, at the time the class is first written...This is another reason why equals() and hashCode() are best left as interface contracts to be implemented in classes, eg.://assume the declaration for Equality was legal in Javapublic interface Equality&amp;lt;T&amp;gt; { public boolean equals(T other);}public interface HashCode { public int code();}… and any code with requirements on the equality or hashCode amongst instances of the types it accepts should really declare its intent via its API, eg.:public interface Map&amp;lt;K extends Equality&amp;lt;K&amp;gt; &amp;amp; HashCode, V&amp;gt; { public V get(K key);}[EDIT 2017-11-05]Reading back on this as I prepare my post on the new ‘data class’ proposal in project amber, I realized that the above interface proposals can be improved a bit:Analyzing the equals() vs. the hashCode() contracts, it becomes obvious that despite mention of hashCode() in the equals() javadoc, the latter’s implementation does not depend on the former’s. However, hashCode() does make demands of equals(). Therefore, I amend my interface proposal to:public interface Equality&amp;lt;T&amp;gt; { public boolean equals(T other);}/** * Specifications for the requisites on Equality#equals */public interface HashCode&amp;lt;T&amp;gt; extends Equality&amp;lt;T&amp;gt; { public int code();}//and Map&#39;s declaration would clear up a bitpublic interface Map&amp;lt;K extends HashCode&amp;lt;K&amp;gt;, V&amp;gt; { public V get(K key);}It is now clear that a) equality does not require an object to be hashable, and b) hashCode is a separate concern intended to improve performance of some collections that require Equality#equals to behave a certain way in order for those collections to behave properly." }, { "title": "Migration to Github Pages", "url": "/posts/migration-to-github-pages/", "categories": "", "tags": "blog", "date": "2017-01-30 09:17:00 -0500", "snippet": "Welcome to the new blog!I’m sunsetting the old blog and migrating all posts here, closer to my public code.The move also allows version control (always welcome) and better automation (via ‘git push’ to master branch).I’m using jekyll to build this blog. Things I’ve yet to properly figure out: Preferred comment system. Apparently, minima only supports disqus out-of-the-box and I’m afraid of ads introduced through it. A decent theme for this blog. I’ve enabled jekyll’s default theme for now since the one I liked best doesn’t work on jekyll 3.3.1 (the version that github currently supports). How to properly link to my linkedin profile. The minima theme doesn’t seem to support it.What I’ll be posting aboutI’ve always had a passion for programming, and I’m a strong believer in the open-source movement, so I’ll definitely be writing related to that. Checkout my code at github.On the personal level, I’ve been passionate about powerlifting for around 18 months or so, so look out for a few posts here and there on that. And, perhaps - drum roll - if I ever get serious again about guitar playing, I’ll be posting some of that as well!" }, { "title": "JDeveloper 11g: SOA composite referencing 2-way-SSL-enabled webservice at design+deployment time", "url": "/posts/jdeveloper-11g-soa-composite/", "categories": "", "tags": "11g, jdeveloper, ssl, bad_certificate", "date": "2015-12-30 16:23:00 -0500", "snippet": "I just spent an embarrassingly long time figuring this out.Design TimeYou&#39;re designing your composite and you need to integrate it with some external web service over SSL. You attempt to add the partner link and it may fail right away (cannot read WSDL) OR it may read it BUT JDeveloper&#39;s schema validator may fail to parse the service&#39; schema. Review your HTTPS Credentials (Tools -&amp;gt; Preferences -&amp;gt; Credentials -&amp;gt; HTTPS Credential) settings and also make sure you&#39;ve set up your keystores and truststores correctly. There is plenty of documentation regarding SSL, keystores and keytool (albeit all a bit confusing, at least for me) out there.Easy.Deployment TimeThis is where you will get the dreaded &quot;received fatal_alert: bad_certificate&quot;. This part vexed me. If the above was done right, why is compilation/deployment failing?The issue is that the HTTPS Credential settings are applied only on the process that performs the schema validation, and not the one that does the final compilation/deployment.Add these entries to your jdev.conf ($MWHOME/jdeveloper/jdev/bin/jdev.conf):AddVMOption -Dweblogic.security.SSL.enableJSSE=true (this one might not be required)AddVMOption -Djavax.net.ssl.trustStore=PATH_TO_HTTPSCREDENTIAL_TRUSTSTOREAddVMOption -Djavax.net.ssl.trustStorePassword=TRUSTSTORE_PWDAddVMOption -Djavax.net.ssl.keyStore=PATH_TO_HTTPSCREDENTIAL_KEYSTOREAddVMOption -Djavax.net.ssl.keyStorePassword=KEYSTORE_PWDRestart JDeveloper. You&#39;re welcome. " }, { "title": "Maven: error due to attempt to deploy `sources` artifact twice", "url": "/posts/maven-error-due-to-attempt-to-deploy/", "categories": "", "tags": "", "date": "2015-11-25 08:37:00 -0500", "snippet": "Unable to get rid of the 400 error when running a build to deploy your artifact to a maven repository? Have you read about how the sources:jar goal is executed twice and then tried overriding that behavior? Still not working?Well, if you&#39;re like me and you are blindly running your build from Netbeans, you might want to verify that you&#39;re not executing both the install and deploy goals. Run them separately and your artifact should be uploaded to your repo without issue. :)" }, { "title": "Java Collections waste (or lack thereof)", "url": "/posts/java-collections-waste-or-lack-thereof/", "categories": "", "tags": "collections, heap, java", "date": "2014-02-13 00:00:00 -0500", "snippet": "If you are a performance freak who makes sure every collection is initialized with the “right” size then don’t be: http://java.dzone.com/articles/java-collection-waste. In all fairness, notice the caveat at the end, though I also suspect that won’t be an issue. :)Edit 18 Aug 2019: See the Java Collections Tutorial on Java Code Geeks for a nice introduction to Java’s Collections." }, { "title": "A note on Weblogic JMS Queue pending + current messages count", "url": "/posts/a-note-on-weblogic-jms-queue-pending/", "categories": "", "tags": "jms, weblogic", "date": "2013-11-02 11:03:00 -0400", "snippet": "Recently I had to perform some load tests on an application that received its input via a Weblogic JMS queue, hence making the rate of message consumption one of the key metrics to keep track of and compare to a known standard.Developing the WLST script proved a bit challenging only for the fact that all documentation of what constitutes &quot;total pending messages&quot; in the queue is a bit ambiguous (Oracle Weblogic&#39;s own documentation being the worst of all). So I did a little digging around and some experimenting and present to you here a graph which I think illustrates quite succinctly what it&#39;s all about:&quot;Messages pending&quot; seem to be the working buffer (tier 1) from where the MDBs consume the messages. &quot;Messages current&quot; seem to be a secondary buffer (tier 2, probably serialized to disk) which are fed into &quot;messages pending&quot; whenever there&#39;s available capacity in the latter.For proof, notice how &quot;messages current&quot; starts climbing only after &quot;messages pending&quot; has reached a certain level, and notice how &quot;messages current&quot; is the first to drop to zero, after which &quot;messages pending&quot; falls to zero as well.By the way, &quot;messages processed&quot; in the graph was calculated using the simple formula: msgs_received - (msgs_current + msgs_pending)." }, { "title": "The Case of the Sudden OC4J Restarts [part 2]", "url": "/posts/the-case-of-sudden-oc4j-restarts-part-2/", "categories": "", "tags": "rmi, jvm, oc4j, oracle, soa suite, java", "date": "2013-04-16 22:11:00 -0400", "snippet": " [Please read part 1 here.] TL;DR: for JDK 1.5, watch your Full GC frequency with sun.rmi.dgc.server.gcInterval/sun.rmi.dgc.client.gcInterval Read on for more. Logging basic GC activity revealed very odd behavior: 1.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Extended full garbage collection every evening at 10 pm. Looking through the logs I would reliably find a large spike in the duration of one full garbage collection cycle at the exact same hour. I still cannot ascertain the cause of this - I searched extensively and found nothing. I even went to #oracle @ freenode but got no answer. One thing I did notice while going back through the opmn.log: ping timeouts almost always occurred at this time, and, indeed, there was at least one instance when OPMN bounced the server at this hour. 2.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Full garbage collection approximately every ~64 seconds. This one was interesting. The Full GCs ran like clockwork instead of as a response to the server&#39;s load: &amp;nbsp; &amp;nbsp; At first it struck me as odd that the average interval between Full GCs would be 64 seconds as opposed to 60. Looking back now the answer is obvious. More on that later. &amp;nbsp; Memory usage was forcefully kept in check by the constant battering of the garbage collector but at the cost of processing time available to do actual work: the JVM was losing 7.53% of its time to garbage collection. Breaking this down further: &amp;nbsp; &amp;nbsp; Young GC Full GC Avg mem. usage post gc (MB) 592 539 Total memory freed (%) 98.5 1.5 GC time spent (%) 3.6 96.4 &amp;nbsp; The numbers are clear: these Full GC cycles are not being effective at all - in fact, they look outright useless and are robbing our server of precious processing time to do actual work! &amp;nbsp; So what could be triggering constant full garbage collection? This puzzled me for a while. &amp;nbsp; I went through all the parameters being passed to the JVM at startup: nothing. I felt confident about this finding yet I decided to put a final nail in the coffin and set the -XX:+DisableExplicitGC flag. This would also allow me to confirm if there exists some code somewhere in the JVM&#39;s classpath calling the System.gc() function. The results were striking: &amp;nbsp; &amp;nbsp; How much server uptime was lost to garbage collection now? 0.19%. Looking good. &amp;nbsp; Of course, average memory usage and allocation increased - as expected - and the JVM&#39;s natural garbage collection algorithm reliably kicked in and freed enough memory. &amp;nbsp; Who was calling System.gc() ? Good question. &amp;nbsp; First order of business was to review our source code. System.gc() was nowhere to be found. Next - check all dependencies explicitly declared for our projects. Decompiling them all and searching revealed 5 classes: 2 were Oracle&#39;s (weblogic, orabpel) and the other was related to Mozilla&#39;s javascript engine. A very brief attempt at figuring out all the possible paths that can lead this code to call System.gc() was very quickly aborted: this is a very difficult task, and I would be leaving out the countless other classes in the JVM&#39;s classpath. &amp;nbsp; I decided to dive into Oracle&#39;s KB to see what I can find. Answer: not much. Apparently, no one else was having this issue, or not many people have noticed it yet. I found quite a few articles detailing JVM troubleshooting methods, SOA Suite installation guides, etc., etc. - nothing of much use. Then, in passing, as a final note in an article liked to by another, I found mention of these uncommonly used startup parameters: ·&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; sun.rmi.dgc.server.gcInterval ·&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; sun.rmi.dgc.client.gcInterval &amp;nbsp; You can read more about them here. The default time of 1 hour mentioned there was not always the case - it used to be 1 minute. Our JDK was affected. &amp;nbsp; The difference between those default 60 seconds and the ~64 seconds on average from the first graph above were enough to bug me - until I looked at the second graph again: the average duration of each Full GC hovered close to 4 seconds. 60 + ~4 ~= 64. :D &amp;nbsp; Removing the -XX:+DisableExplicitGC flag and adding these two above, set to 1 hour each, proved fruitful: Full GCs were mostly limited to that hourly schedule. &amp;nbsp; Now it&#39;s just a question of figuring out the best values for our needs (or whether we need these settings at all). " }, { "title": "Vim Tip of the Day!", "url": "/posts/vim-tip-of-day/", "categories": "", "tags": "printf, vim, line number", "date": "2013-04-09 17:26:00 -0400", "snippet": "Let&#39;s say you have to upload a bunch of data to a table and let&#39;s say you need to index them with an ID key.You could just declare the column to auto-increment when creating the table in the database (or, in the case of Oracle, define a trigger/sequence combo to produce those values).... or you could just generate them beforehand in Vim using line numbers. Yes, line numbers.Suppose you already have your insert statements set up in vim like this: insert into tmp_table values (33.832,&#39;GC&#39;,131072,18844,502464,0.1015525);insert into tmp_table values (38.272,&#39;GC&#39;,149916,26475,502464,0.1751825);insert into tmp_table values (52.808,&#39;GC&#39;,157547,42326,502464,0.2372075);insert into tmp_table values (68.815,&#39;GC&#39;,173398,55185,633536,0.1455373);insert into tmp_table values (84.570,&#39;GC&#39;,317329,74330,633536,0.1732868);insert into tmp_table values (167.147,&#39;GC&#39;,307334,102686,635008,0.2086538);insert into tmp_table values (167.355,&#39;Full GC&#39;,102686,101944,635008,1.8805190);insert into tmp_table values (184.915,&#39;GC&#39;,338599,123368,642688,0.0918409);Executing %s/(/\\=printf(&#39;(%d,&#39;, line(&#39;.&#39;))/ produces the indexes that you need (if the first column is the index field): insert into tmp_table values (1,33.832,&#39;GC&#39;,131072,18844,502464,0.1015525);insert into tmp_table values (2,38.272,&#39;GC&#39;,149916,26475,502464,0.1751825);insert into tmp_table values (3,52.808,&#39;GC&#39;,157547,42326,502464,0.2372075);insert into tmp_table values (4,68.815,&#39;GC&#39;,173398,55185,633536,0.1455373);insert into tmp_table values (5,84.570,&#39;GC&#39;,317329,74330,633536,0.1732868);insert into tmp_table values (6,167.147,&#39;GC&#39;,307334,102686,635008,0.2086538);insert into tmp_table values (7,167.355,&#39;Full GC&#39;,102686,101944,635008,1.8805190);insert into tmp_table values (8,184.915,&#39;GC&#39;,338599,123368,642688,0.0918409);Just one more reason to love Vim!" }, { "title": "Embedding a CDATA section in your BPEL Response", "url": "/posts/embedding-cdata-section-in-your-bpel/", "categories": "", "tags": "soa, cdata, oracle soa suite, bpel", "date": "2013-03-08 20:49:00 -0500", "snippet": " This tip might save you a few minutes. A colleague recently wanted his BPEL process (Oracle SOA Suite) to return a string containing XML markup, and within that markup he needed to include a CDATA section. The requirement for returning this document as a string will perhaps be discussed in another article. Short answer: it&#39;s not possible. Longer answer: Those of you who have some experience with [Oracle] BPEL Process development will have probably come across the { http://schemas.oracle.com/xpath/extension}getContentAsString()function. Those of you who don&#39;t: this function enables you to treat an XML structure like a string. It&#39;s the only way (that my colleague and I know of) to do this in Oracle BPEL. What happens when have some XML structure with a CDATA section, pass it to getContentAsString and assign it to an xsd:string ? The start and end angled brackets (&amp;quot;&amp;lt;&amp;quot; and &amp;quot;&amp;gt;&amp;quot;) pertaining to the CDATA delimiters alone are replaced by their entity references (&amp;quot;&amp;amp;lt;&amp;quot; and &amp;quot;&amp;amp;gt;&amp;quot;). All other markup in the XML structure would be left intact. Why? If you only witness this happening via the console audit trails, or basically through any XML parser, you are unlikely to realize that the getContentAsString function actually embeds the XML structure in a CDATA element already! What you would see via the online console: &amp;lt;root&amp;gt;                 &amp;lt;ElementList&amp;gt;                                 &amp;lt;element1&amp;gt;data1&amp;lt;/element1&amp;gt;                                 &amp;lt;element2&amp;gt;data2&amp;lt;/element2&amp;gt;                                 &amp;lt;element3&amp;gt;&amp;amp;lt;![CDATA[ some CDATA characters ]]&amp;amp;gt;&amp;lt;/element3&amp;gt;                 &amp;lt;/ElementList&amp;gt; &amp;lt;/root&amp;gt; Where the element root is of type xsd:string and is the node to which you&#39;re assigning the output of getContentAsString. But if you look at the raw message, that is, without parsing the XML data (eg.: soapUI) then you see this: &amp;lt;root&amp;gt;&amp;lt;![CDATA[                 &amp;lt;ElementList&amp;gt;                                 &amp;lt;element1&amp;gt;data1&amp;lt;/element1&amp;gt;                                 &amp;lt;element2&amp;gt;data2&amp;lt;/element2&amp;gt;                                 &amp;lt;element3&amp;gt;&amp;amp;lt;![CDATA[ some CDATA characters ]]&amp;amp;gt;&amp;lt;/element3&amp;gt;                 &amp;lt;/ElementList&amp;gt;]]&amp;gt; &amp;lt;/root&amp;gt; The XML spec clearly states that CDATA sections cannot be nested. Hence the escaping of those angle brackets. What&#39;s more, CDATA sections are not meant to be parsed (that is the point of having CDATA sections), and that includes those escaped characters. Don&#39;t go thinking that the XML parser will see them and convert them back to angle brackets. So what can you do? Don&#39;t convert your structure to a string. Return a complex type. If you do so, then you can do something similar to the following in a transformation for example:                 &amp;lt;element3&amp;gt;                                 &amp;lt;xsl:text&amp;gt;&amp;lt;![CDATA[&amp;lt;![CDATA[ some CDATA characters ]]]]&amp;gt;&amp;lt;![CDATA[&amp;gt;]]&amp;gt;&amp;lt;/xsl:text&amp;gt;                 &amp;lt;/element3&amp;gt; By putting that right angle bracket in a separate CDATA section, I avoid &amp;quot;closing&amp;quot; the first CDATA section marked in red so that I can also include those two square brackets &amp;quot;]&amp;quot; in it. The output would be something like this: &amp;lt;root&amp;gt;                 &amp;lt;ElementList&amp;gt;                                 &amp;lt;element1&amp;gt;data1&amp;lt;/element1&amp;gt;                                 &amp;lt;element2&amp;gt;data2&amp;lt;/element2&amp;gt;                                 &amp;lt;element3&amp;gt;&amp;lt;![CDATA[ some CDATA characters ]]&amp;gt;&amp;lt;/element3&amp;gt;                 &amp;lt;/ElementList&amp;gt; &amp;lt;/root&amp;gt; However, this time the element rootis not of type xsd:string; instead, root is now a complex type defined as the parent element of ElementList, itself a complex type having element1, element2 and element3 as child nodes. Hope it helps. " }, { "title": "Focus", "url": "/posts/focus/", "categories": "", "tags": "", "date": "2013-03-05 21:05:00 -0500", "snippet": "Focus. It&#39;s what the mind needs. Meanwhile, I continue finding my way towards enlightenment." }, { "title": "The Case of Sudden OC4J Restarts [part 1]", "url": "/posts/the-case-of-sudden-oc4j-restarts/", "categories": "", "tags": "", "date": "2013-02-27 12:33:00 -0500", "snippet": " [Please read part 2 here.] Our OC4J instance would restart itself every once in a while - no culprits found looking through unix&#39; history. My next thought was to check the crontab but my user doesn&#39;t have access to that. Weighing bureaucracy (soliciting the unix admins to check it or give me access) versus pursuing other means of inquiry: of course I chose the latter! From the opmnlog file: 13/02/25 10:48:15 [ons-connect] Passive connection 16438,xxx.xxx.xxx.xxx,6233 missing connect 13/02/25 11:45:28 [ons-connect] Passive connection 1667c, xxx.xxx.xxx.xxx,6233 missing connect 13/02/25 12:06:18 [ons-connect] Passive connection 16751, xxx.xxx.xxx.xxx,6233 missing connect 13/02/25 14:22:19 [libopmnoc4j] OC4J ping attempt timed out 13/02/25 14:22:19 [libopmnoc4j] Process Ping Failed: default_group~oc4j_esbas~default_group~1 (1786977551:2786) 13/02/25 14:23:09 [libopmnoc4j] OC4J ping attempt timed out 13/02/25 14:23:09 [libopmnoc4j] Process Ping Failed: default_group~oc4j_esbas~default_group~1 (1786977551:2786) 13/02/25 14:23:59 [libopmnoc4j] OC4J ping attempt timed out 13/02/25 14:23:59 [libopmnoc4j] Process Ping Failed: default_group~oc4j_esbas~default_group~1 (1786977551:2786) 13/02/25 14:23:59 [libopmnoc4j] Process Unreachable: default_group~oc4j_esbas~default_group~1 (1786977551:2786) 13/02/25 14:23:59 [pm-process] Restarting Process: default_group~oc4j_esbas~default_group~1 (1786977551:2786) 13/02/25 14:28:59 [pm-process] Stopping Process: default_group~oc4j_esbas~default_group~1 (1786977551:2786) 13/02/25 14:28:59 [libopmnoc4j] Forcefully Terminating Process: default_group~oc4j_esbas~default_group~1 (1786977551:2786) 13/02/25 14:29:01 [pm-process] Starting Process: default_group~oc4j_esbas~default_group~1 (1786977551:0) 13/02/25 14:30:03 [pm-process] Process Alive: default_group~oc4j_esbas~default_group~1 (1786977551:9531) Classic Ping-Echo tactic to satisfy quality attributes related to availability in system architectures. Anything could be causing those ping timeouts, but at least now I know I don&#39;t have to deal with the sysadmins. Neat. In the instance&#39;s message log I see a few instances of the following occurring at around the same timeframe: &amp;lt;MESSAGE&amp;gt;   &amp;lt;HEADER&amp;gt;     &amp;lt;TSTZ_ORIGINATING&amp;gt;2013-02-25T14:21:57.735-04:00&amp;lt;/TSTZ_ORIGINATING&amp;gt;     &amp;lt;COMPONENT_ID&amp;gt;tip&amp;lt;/COMPONENT_ID&amp;gt;     &amp;lt;MSG_TYPE TYPE=&amp;quot;ERROR&amp;quot;&amp;gt;&amp;lt;/MSG_TYPE&amp;gt;     &amp;lt;MSG_LEVEL&amp;gt;1&amp;lt;/MSG_LEVEL&amp;gt;     &amp;lt;HOST_ID&amp;gt;hostid123&amp;lt;/HOST_ID&amp;gt;     &amp;lt;HOST_NWADDR&amp;gt;xxx.xxx.xxx.xxx&amp;lt;/HOST_NWADDR&amp;gt;     &amp;lt;MODULE_ID&amp;gt;esb.server.dispatch.agent&amp;lt;/MODULE_ID&amp;gt;     &amp;lt;THREAD_ID&amp;gt;90&amp;lt;/THREAD_ID&amp;gt;     &amp;lt;USER_ID&amp;gt;abcde&amp;lt;/USER_ID&amp;gt;   &amp;lt;/HEADER&amp;gt;   &amp;lt;CORRELATION_DATA&amp;gt;     &amp;lt;EXEC_CONTEXT_ID&amp;gt;&amp;lt;UNIQUE_ID&amp;gt;1697007412:56432:1360980064744:1021&amp;lt;/UNIQUE_ID&amp;gt;&amp;lt;SEQ&amp;gt;0&amp;lt;/SEQ&amp;gt;&amp;lt;/EXEC_CONTEXT_ID&amp;gt;   &amp;lt;/CORRELATION_DATA&amp;gt;   &amp;lt;PAYLOAD&amp;gt;     &amp;lt;MSG_TEXT&amp;gt;Failed to commit transaction&amp;lt;/MSG_TEXT&amp;gt;     &amp;lt;SUPPL_DETAIL&amp;gt;&amp;lt;![CDATA[oracle.tip.esb.server.common.exceptions.BusinessEventRetriableException: Failed to commit transaction; transaction status is &amp;quot;6&amp;quot;         at oracle.tip.esb.server.common.JTAHelper.commitTransaction(JTAHelper.java:216)         at oracle.tip.esb.server.dispatch.agent.ESBWork.run(ESBWork.java:155)         at oracle.j2ee.connector.work.WorkWrapper.runTargetWork(WorkWrapper.java:242)         at oracle.j2ee.connector.work.WorkWrapper.doWork(WorkWrapper.java:215)         at oracle.j2ee.connector.work.WorkWrapper.run(WorkWrapper.java:190)         at EDU.oswego.cs.dl.util.concurrent.PooledExecutor$Worker.run(PooledExecutor.java:825)         at java.lang.Thread.run(Thread.java:595) Caused by: javax.transaction.RollbackException: Timed out         at com.evermind.server.ApplicationServerTransaction.checkForRollbackOnlyWhileInCommit(ApplicationServerTransaction.java:664)         at com.evermind.server.ApplicationServerTransaction.doCommit(ApplicationServerTransaction.java:273)         at com.evermind.server.ApplicationServerTransaction.commit(ApplicationServerTransaction.java:162)         at com.evermind.server.ApplicationServerTransactionManager.commit(ApplicationServerTransactionManager.java:475)         at oracle.tip.esb.server.common.JTAHelper.commitTransaction(JTAHelper.java:214)         ... 6 more ]]&amp;gt;&amp;lt;/SUPPL_DETAIL&amp;gt;   &amp;lt;/PAYLOAD&amp;gt; &amp;lt;/MESSAGE&amp;gt; This message eventually turned out to be a dead end. My immediate hypothesis is that the server is under load and thus transactions are starting to hang and being rolled back, as well as being unable to respond to OPMN&#39;s ping requests. Threads can get stuck due to many different causes, such as waiting for some I/O to complete, or entering a loop without exit condition, or competing for shared resources with other threads, etc. To find out what exactly was happening, I first need to identify the components involved in this transaction. And this is where I stumbled on my first roadblock. How do I trace this message back to a component. It is obvious that the CORRELATION_DATAelement is somehow pointing me in some direction, but how do I interpret its UNIQUE_ID? I have yet to find a clear description of this element&#39;s contents. From Oracle: Using ODL, diagnostic messages are written to log files using XML format and each message includes a HEADER element containing information about the message, optionally a CORRELATION_DATA element containing information to assist in correlating messages across components, and a PAYLOAD element containing the message text including optional arguments and associated values. ... and that&#39;s it. My hypothesis is that the UNIQUE_IDelement&#39;s text value is a string of values delimited by colons, and each value serves as a unique identifier for every instance of any components involved in the handling of this message or operation. So far I have not been able to prove this. I haven&#39;t conclusively linked any of those values to any BPEL or ESB instance ID. If anyone out there can point to a clear description of the CORRELATION_DATAelement and what those values above would  mean, it would be very welcome! Next thought: monitor GC performance. It is possible that the garbage collecting mechanism is occasionally interfering with the OC4J instance&#39;s ability to respond to OPMN&#39;s ping requests. If, say, a full garbage collection is taking place (which can potentially last up to tens of seconds) while OPMN&#39;s ping arrives, then yes, a timeout can definitely happen. Which is why I have added these flags to the process&#39; java-optionsin opmn.xml:                 -XX:-PrintGCDetails -XX:-PrintGCTimeStamps -Xloggc:gc.log Now it&#39;s just a matter of waiting for the next event... " }, { "title": "Writing prose in two languages (or how I coded some bits in application code, other bits in database code)", "url": "/posts/writing-prose-in-two-languages-or-how-i/", "categories": "", "tags": "", "date": "2012-03-19 23:39:00 -0400", "snippet": "I didn&#39;t code it. But I&#39;m part of the team in charge of completing the project and maintaining it.Oh yeah. The project consists of an integration layer between different systems; it is implemented on top of Oracle&#39;s SOA Suite. There are many BPEL processes, ESBs, web services, etc. Since it&#39;s Oracle&#39;s SOA Suite, we use JDeveloper (dear god, don&#39;t get me started) and Oracle Database. We color-coordinated everything - we&#39;re cool like that. The only other caveat is that there are a few stored procedures here and there, as well as some triggers. Triggers. Containing business logic. Like, if this new row in table X has the &quot;status&quot; column set to 123, then update tables A, B, C, D and E with status 456.It&#39;s pretty much guaranteed that you will fail to pin point this mechanism in a reasonable amount of time the first time you come across such a scenario. It&#39;s a pretty big context switch and will surely be the last thing that you will check.This post on StackExchange reminded me of all this while I lay cool sipping on a cold beer at my home." }, { "title": "Performance comparison: Linked-List vs. Vector [Java]", "url": "/posts/performance-comparison-linked-list-vs/", "categories": "", "tags": "", "date": "2012-03-09 19:45:00 -0500", "snippet": "Kjellkod recently published an interesting article comparing the performance of linked-lists against that of a vector, as well as discussing the reason for the unexpected results. You can also follow his article at the CodeProject if you want.Like Kjellkod mentions, in theory the linked-list should deliver better performance than a vector whenever you need to insert or remove objects at or from positions not at the ends, however the reality is that in real, modern systems the hardware optimizes instruction/data reads by caching contiguous bytes of RAM in the CPU, and this optimization makes all the difference in the world when you have to insert or remove an item from the collection because you first have to seek the position you&#39;re interested in before performing the operation. Since arrays are contiguous blocks of memory by definition (and a vector is essentially an array with a cherry on top), then arrays will naturally perform better because cache misses are minimized, as opposed to a linked-list.The Test &amp;nbsp; Naturally, I had to see this for myself, so I did a test in Java (as opposed to C++11 like Kjellrod). My testbed is a laptop running Windows 7 64-bit running on an i7-263QM @ 2GHz with 6GB of DDR3 1333Mhz DRAM. I have no idea about the timings of the RAM modules, but if it helps, my laptop is this one. The Java runtime I used is:&amp;gt;&amp;gt; java -versionjava version &quot;1.6.0_29&quot;Java(TM) SE Runtime Environment (build 1.6.0_29-b11)Java HotSpot(TM) Client VM (build 20.4-b02, mixed mode, sharing)The JDK version I&#39;m using is 1.6.0_29. Both are 32-bit. I implemented the same algorithm that Kjellkod did. The code is available here.Following are my results (the axis at the bottom refers to the number of elements being sorted):Just like Kjellkod, I found the performance of the linked-list vastly inferior to that of a vector. Notice, however, that my code actually ran faster despite the Java implementation. On a semantic level, does my code not mean the same as his? In other words, did I screw up somewhere?[UPDATE 2012-03-19: My code did not run faster by any means... I mistakenly assumed that Kjellkod&#39;s times were in milliseconds as opposed to microseconds... until a few days ago, I wasn&#39;t even aware that measuring elapsed times of a few microseconds was possible on a typical consumer computer! Here&#39;s a really great link: https://blogs.oracle.com/dholmes/entry/inside_the_hotspot_vm_clocks.]Notice that I threw in an ArrayList in there for comparison. Being an array deep down, its performance should be far better than the linked-list. It is. But how does it stack up against the Vector?&amp;nbsp; This is where it gets interesting: if you google search around you will find that the general consensus is the Vector will be slower because it is synchronized whereas the ArrayList is not. Let&#39;s find out.Using the same testbed:&amp;nbsp;Notice that slight overshoot (relative underperformance) of a couple of ms of the Vector in the ~200-700 range. I was able to reproduce that every time and can&#39;t yet figure out what could be the cause. After ~700 elements though, the vector is clearly faster than the arraylist by a few milliseconds, with the gap widening as we add more and more elements. Yet, that initial spike in the vector&#39;s time is unexpected; I predicted the ArrayList to be consistently slower than the Vector because of the additional overhead.ConclusionOverall, there seem to be only a few edge cases where a Linked-List makes sense (read the comments on Kjellrod&#39;s articles). Apart from those though, stay away from the linked-list despite everything the web tells you.Between a Vector and an ArrayList though, there&#39;s pretty much no real reason to favor one over the other in terms of performance alone. Developing applications in Java typically means you&#39;re not interested in wringing out every single millisecond of performance out of your code and hardware, and also it&#39;s unusual to be working with lists of several thousand elements. Ranges from a dozen elements to a few hundred are typically the case and, as you can see above, the difference in performance is only 2 or 3 milliseconds at the most within that range." }, { "title": "Why I stopped caring and learned to hate XPath", "url": "/posts/why-i-stopped-caring-and-learned-to/", "categories": "", "tags": "xpath, boolean expression", "date": "2012-03-06 09:42:00 -0500", "snippet": "Yesterday I came across another one of XPath&#39;s little quirks that cost me up to an hour of painstaking step-by-step troubleshooting to narrow down: comparing the value of a boolean element to one of the boolean functions (either true() or false()), also known as a subset of boolean expressions.I had an element of type boolean buried in an XML document and my initial expression to test its value was like this:/path/to/boolean/element = true()The expression is so simple and innocent-looking that (gasp!) there just couldn&#39;t be anything wrong with it. The path was expressed correctly, I was using the right operator and I wanted to see if the boolean element had a boolean value of true. Except, of course, it just didn&#39;t work as expected and the code just kept taking a wrong turn.I admit that I had previously seen this tested another way, namely like:&amp;nbsp;/path/to/boolean/element = &#39;true&#39;Notice how the comparison is now done against a string and not a boolean type. And it works. I thought it was just the engine or the specification trying to make life easier for programmers, some sort of condescending you&#39;re doing it wrong but we understand what you&#39;re trying to accomplish here so we&#39;ll just run along with it thing going on here. And why not believe that since, after all, at least some features of XPath were catered to non-programmers? So, me being me, I wanted to do it how I believed was the true, correctly typed way of course.And like I said, it doesn&#39;t work.If you go back to the link on boolean expressions you&#39;ll notice this line:If one object to be compared is a node-set and the other is a boolean, then the comparison will be true if and only if the result of performing the comparison on the boolean and on the result of converting the node-set to a boolean using the boolean function is true.&amp;nbsp;This is the scenario I&#39;m talking about. Let&#39;s follow through the link to the boolean function:&amp;nbsp;The boolean function converts its argument to a boolean as follows:a node-set is true if and only if it is non-empty Mother of god... My expression wasn&#39;t testing the value of my boolean element - it was checking to see if the result of my xpath returned any nodes at all! Gasp! Going back to the section on boolean expressions reveals this: If one object to be compared is a node-set and the other is a string, then the comparison will be true if and only if there is a node in the node-set such that the result of performing the comparison on the string-value of the node and the other string is true.So hold on a minute.... work with me here. If I compare my xpath that selects a boolean element against a string then the actual value of said element is considered, but when I compare that same xpath against a boolean type then the actual value of the element is irrelevant?What insane world are we living in? What was the genius behind this design decision?" }, { "title": "Those facepalm moments...", "url": "/posts/those-facepalm-moments/", "categories": "", "tags": "", "date": "2012-03-04 22:31:00 -0500", "snippet": "[...] by now you should know that the likelihood of a given event contributing to your code failure is inversely proportional to your initial index of suspicion of it&#39;s relation.&amp;nbsp;Source: Ethan&#39;s Software Blog." }, { "title": "The illusion of being &#39;busy&#39; while not being productive", "url": "/posts/illusion-of-being-busy-while-not-being/", "categories": "", "tags": "", "date": "2012-02-29 20:19:00 -0500", "snippet": "If you are an office worker, answer me this: when was the last time you felt you were being truly productive and doing something truly important while at work? In other words, when was the last time you were bona fide busy? And for how long?Now here&#39;s another question for you: do others think you&#39;re as busy as you believe you are?I just finished reading Rands&#39; (not his real name) latest article at http://www.randsinrepose.com/archives/2012/02/29/a_precious_hour.html (if you&#39;re not following him yet, you really should do so) and he touched on something I&#39;ve been thinking about and avoiding for the past few months: keeping up with numerous and endless meetings, multitasking, calling in the whole world so you can brainstorm the problem in front of you, running around and calling people for status updates, and other no-so-fun and not-so-productive things, in order to perpetuate the appearance of being busy.&quot;Ohh he&#39;s in a rush and talking really fast - you can see he&#39;s got a lot of things going on just by the look on his face - he&#39;s definitely a busy guy!&quot;While everybody else is having lunch: &quot;Wow, in how many meetings today have I ran into you already? 2? And it&#39;s only noon time!&quot;&quot;Hey, look at him. I always see him at his desk after work hours are over - he must definitely be one of those hard working, productive types.&quot;Bullocks.You would think that just plainly shunning this facade and focusing on actually getting things done would get you places but chances are far against your favor that you will actually travel far. Not in this part of the world at least. What is it about westerners (or if not, the whole world; I don&#39;t know enough about orientals to make any educated guesses) and their obsession with appearances?I think I&#39;ll end up side tracked and ranting even more if I continue this so I&#39;ll just stop. Like Rands, I also have things to do." }, { "title": "Closures", "url": "/posts/closures/", "categories": "", "tags": "", "date": "2011-11-05 20:27:00 -0400", "snippet": "Wikipedia&#39;s article still makes me somewhat dizzy whenever I have another go at it." }, { "title": "Oracle&#39;s JDBC BLOB support sucks....", "url": "/posts/oracles-jdbc-blob-support-sucks/", "categories": "", "tags": "jdbc, oracle", "date": "2011-08-05 18:57:00 -0400", "snippet": "Exception in thread &quot;main&quot; : java.lang.ClassCastException: javax.sql.rowset.serial.SerialBlob cannot be cast to oracle.sql.BLOB...Caused by: java.lang.ClassCastException: javax.sql.rowset.serial.SerialBlob cannot be cast to oracle.sql.BLOB&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at oracle.jdbc.driver.OraclePreparedStatement.setBlob(OraclePreparedStatement.java:6634)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at oracle.jdbc.driver.OraclePreparedStatementWrapper.setBlob(OraclePreparedStatementWrapper.java:126)The &quot;infringing&quot; code is this:&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; SerialBlob sb1 = new SerialBlob(value);&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; stmt.setBlob(5, sb1);&amp;nbsp;&amp;nbsp; //column 5 is a blob, in case you&#39;re wonderingReally, why not code against standards?EDIT 8/5/2011 19:35: Issue worked around like so:&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ByteArrayInputStream bais1 = new ByteArrayInputStream(value);&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; stmt.setBinaryStream(5, bais1, value.length);" }, { "title": "More Vim goodness: The Power of g", "url": "/posts/more-vim-goodness-power-of-g/", "categories": "", "tags": "vim", "date": "2011-07-21 10:03:00 -0400", "snippet": "The Power of gI was in particular need of: 0&quot;ay0:g//y A" }, { "title": "Editting zip files with Vim...", "url": "/posts/editting-zip-files-with-vim/", "categories": "", "tags": "awesome, jar, vim", "date": "2011-06-30 17:42:00 -0400", "snippet": "WoahScreenie:Files like .jar and .xpi, themselves just renamed zip files, can also be opened using Vim(right click -&amp;gt; View Image to view in full size) You can even edit the files with Vim! So long jar xvf/uvf! Seriously, with Vim, the possibilities are endless..." }, { "title": "Vim - End of line characters!", "url": "/posts/vim-end-of-line-characters/", "categories": "", "tags": "", "date": "2011-06-17 21:10:00 -0400", "snippet": "There are some dtds created with XmlSpy that are used by the system at work and for some reason they have mac style line endings (I really hope this isn&#39;t XmlSpy&#39;s doing...). Found the following link hugely helpful: http://vim.wikia.com/wiki/File_formatExcerpt:A common problem is that you open a file and see ^M at the end of many lines. Entering :set ff? will probably show that the file was read as unix: the problem is that some lines actually end with CRLF. To fix this, you need to tell Vim to read the file again using dos file format. When reading as dos, all CRLF line endings, and all LF-only line endings, are removed. Then you need to change the file format for the buffer and save the file. The following procedures will easily handle this situation, but they only work reliably on reasonably recent versions of Vim (7.2.40 or higher). Convert from dos/unix to unix To convert the current file from any mixture of CRLF/LF-only line endings, so all lines end with LF only: :update Save any changes. :e ++ff=dos Edit file again, using dos file format (&#39;fileformats&#39; is ignored).[A 1] :setlocal ff=unix This buffer will use LF-only line endings when written.[A 2] :w Write buffer using unix (LF-only) line endings. In the above, replacing :set ff=unix with :set ff=mac would write the file with mac (CR-only) line endings. Or, if it was a mac file to start with, you would use :e ++ff=mac to read the file correctly, so you could convert the line endings to unix or dos. Convert from dos/unix to dos To convert the current file from any mixture of CRLF/LF-only line endings, so all lines end with CRLF only: :update Save any changes. :e ++ff=dos Edit file again, using dos file format (&#39;fileformats&#39; is ignored).[A 1] :w Write buffer using dos (CRLF) line endings. " }, { "title": "JFreeChart: PiePlot.getSectionPaint()", "url": "/posts/jfreechart-pieplotgetsectionpaint/", "categories": "", "tags": "jfreechart, graph", "date": "2010-10-26 18:07:00 -0400", "snippet": "I am starting to consider replacing the JFreeChart API with some other one in future projects - gotta start looking for it.Anyway, today&#39;s annoyance with this tool happens to be the fact that charts don&#39;t assign colors until they&#39;re drawn.[EDIT] I forgot to add that this small detail is not mentioned in the javadoc. :) [/EDIT]My requirement was simple: assign the same color of each pieplot section to its respective series in an xychart. Sounds extremely simple, right? Well, after having it not work and me scrambling around, adding some ridiculous debugging messages I finally saw that PiePlot.getSectionPaint(Comparable key) was returning null. At that point I wanted to do additional debugging but getSectionKey() is protected for reasons beyond my understanding. So I gave up debugging this myself and turned to google and got slapped in the face with the above.Damn man." }, { "title": "JDBC and Oracle: setNULL", "url": "/posts/jdbc-and-oracle-setnull/", "categories": "", "tags": "jdbc, preparedstatement, oracle, setnull, java", "date": "2010-10-06 10:14:00 -0400", "snippet": "I was surprised (and disappointed) by this behaviour (using Sun JVM 1.4 with Oracle DB 10g):String sql = &quot;select count(distinct seq_no) from tmp_table where time_stamp = to_date(&#39;29/09/2010 07:00:00&#39;, &#39;DD/MM/YYYY HH24:MI:SS&#39;) and err_code = ?&quot;;PreparedStatement pstmt = conn.prepareStatement(sql);pstmt.setNull(1, Types.NULL);result = pstmt.executeQuery();result.next();System.out.println(&quot;Result: &quot; + result.getInt(1));The output is: Result: 0Zero? But there are rows with err_code set to null!!!Substituting Types.NULL for Types.VARCHAR produces the same result.Googled around a bit and according to this the jdbc driver is actually sending a string &#39;null&#39;. Wow. So now I will need to have a second sql/preparedstatement just for this scenario... takes away some of the benefits of preparedstatements... oh well. :/" }, { "title": "VIM - Code Completion", "url": "/posts/vim-code-completion/", "categories": "", "tags": "", "date": "2010-10-02 23:21:00 -0400", "snippet": "It&#39;s been a while!Anyway, this post&#39;s catalyst occurred this morning during my first day of a Java course I&#39;m attending (an attempt at keeping my teeth sharp). As is custom, use of an IDE is not supported and instead attendants are to use a &quot;normal text editor&quot; (we all know Vim is far from &quot;normal text editor&quot; but bare with me here :P).It suddenly struck me: it was time to fiddle around with code completion in Vim.Quickly dug around and found that Vim already had code completion for some common languages (from version 7 onwards)! Java was unfortunately not on the list so I had to look for a third party solution. So far I have come across eclim, VJDE and java complete. I didn&#39;t like eclim&#39;s design (running a server with which vim would interface), and I couldn&#39;t make VJDE work for some reason, but java complete has so far been working wonders:Just one more reason to love Vim! :D" }, { "title": "The Tao of Programming", "url": "/posts/tao-of-programming/", "categories": "", "tags": "", "date": "2009-12-15 09:49:00 -0500", "snippet": "The master programmer moves fromprogram to program without fear.No change in management can harmhim. He will not be fired, evenif the project is cancelled.Whyis this? He is filled with Tao.The novice programmer stares inwonder at the bird, for heunderstands it not. The averageprogrammer dreads the coming ofthe bird, for he fears itsmessage. The master programmercontinues to work at his terminal,for he does not know that thebird has come and gone.Classic!PS: Not to be confused with the blog." }, { "title": "Java String.split() and the | (pipe) character", "url": "/posts/java-stringsplit-and-pipe-character/", "categories": "", "tags": "", "date": "2009-12-02 17:32:00 -0500", "snippet": "I came across this issue today and someone else already blogged about it: http://hoskinator.blogspot.com/2006/11/trouble-using-pipe-with-stringsplit.html. Some of the comments are really good!The pipe character is not mentioned in the Pattern javadoc.Basically, just escape the pipe character with double backslashes like so &quot;\\\\|&quot; and you&#39;re set.This post merely provides redundancy in case the above link goes extinct..." }, { "title": "Vim Tip of the Day", "url": "/posts/vim-tip-of-day/", "categories": "", "tags": "copy search hits, vi, vim", "date": "2009-07-23 15:13:00 -0400", "snippet": "Copying lines containing search hits (source):1.- Clear a register (eg &#39;a&#39;): qaq2.- Append all matching lines to that register: g/pattern/y A3.- Copy contents of register a to clipboard (register &#39;+&#39;): let @+ = @aEnjoy." }, { "title": "Some Windows magic", "url": "/posts/some-windows-magic/", "categories": "", "tags": "windows, batch file, count lines, com, cmd, command line", "date": "2009-07-22 23:05:00 -0400", "snippet": "Script to get number of lines in a file:REM %1 is the file name to be parsed, passed at command line@set count=0for /f %%a in (%1) do (@set /a count+=1)@echo %count%Script that will look at each entry (no spaces) in a file (called seq_no_t3.csv in my example) and search for lines that have an exact match with the string (ie &quot;^string$&quot;) in a list of files (specified by adroms*_replies.txt in my example) and output the string &quot;string,file_name&quot; followed by an extra \\n in an output file (called loc_matches_2.txt in my example). Also, it will constantly echo to the screen the line number on which it is currently working on from input file.@echo off@setlocalset count=0for /f %%i in (&#39;type .\\seq_no_t3.csv&#39;) do (set /a count+=1set count&amp;lt;nul (set /p tmp=%%i) &gt;&amp;gt; loc_matches_2.txt&amp;lt;nul (set /p tmp=,) &amp;gt;&gt; loc_matches_2.txtfor /f %%j in (\\findstr /m /r &quot;^%%i$&quot; adroms*_replies.txt&#39;) do (&amp;lt;nul (set /p tmp=%%j) &amp;gt;&amp;gt; loc_matches_2.txt)@echo. &gt;&gt; loc_matches_2.txt)@endlocal@echo onThis script has several interesting things going on. Apart from the use of the relatively powerful for loop (where in this case the default delimiter of or is being used), the lines starting with will pipe a nul response to set /p so that the variable remains unchanged. Due to the quirky way set works, set /p prints the value to the right of the equals sign, not the value of the variable itself. Therefore, you&#39;re not changing the value of the %tmp% variable yet you are appending %%i/,/%%j to the file with no CRLF! (see source).Pending fixes:1.- Fix issue where blank lines (actually they have space characters) are printed to output file2.- Fix issue where if the input string is matched in several files, a list of files (where delim=CRLF) is appended to the line corresponding to the input string. Hence there will be lines with no comma separation - instead they will have a file name where the last input string was found." }, { "title": "Vim column mode", "url": "/posts/vim-column-mode/", "categories": "", "tags": "vi, column mode, vim", "date": "2009-07-17 18:59:00 -0400", "snippet": "Today I found myself in the need of Vim&#39;s column mode (using regex for what I wanted to do would have been hard).I googled real quick but the first few hits weren&#39;t of much use/very clear and and so after figuring out how it works I think it&#39;s worthy of posting here as a) a reminder for myself :) and b) so others who google this may get to see this - faster, hopefully.Let&#39;s say you have sql statements like in the 1st screenshot. Look closely - they are missing a cast of the date fields with to_date. If you are stuck at 10PM at the office on a friday night, like me, and need to perform this, I really think substitution with regular expressions is the least preferable option at this point.Enter column mode.Move the cursor to the position where you would like to insert text:Hit Ctrl+q. This will make Vim go into &#39;visual block&#39; mode. Select the intersection of columns and rows where you would like to insert your text. In my case, I wanted to insert some text right before the single quote selected:Now hit Shift+I. This will bring the cursor to the first coordinate selected before in mode. Type in the text you want to enter. Don&#39;t fret when you see that you&#39;re only updating that line. Once you finish, press ESC - this is where the magic happens:Isn&#39;t vim the greatest text editor in the world? :)Enjoy!" } ]
